// Thesis content - Auto-generated from PDF extraction
// This file was generated automatically. Manual edits may be overwritten.

export interface ThesisContent {
  hero: {
    title: string;
    subtitle?: string;
    author: string;
    university: string;
    date: string;
    department?: string;
  };
  abstract: {
    title: string;
    content: string;
  };
  introduction: {
    title: string;
    content: string[];
  };
  methodology: {
    title: string;
    content: (string | { type: 'image'; src: string; alt: string; caption?: string })[];
  };
  results: {
    title: string;
    content: string[];
  };
  conclusions: {
    title: string;
    content: string[];
  };
  downloads: {
    title: string;
    thesisPdf: string;
    presentationPptx: string;
    frameworkGitHub?: string;
  };
  contact: {
    title: string;
    name: string;
    email: string;
    university?: string;
    department?: string;
  };
}

export const thesisContent: ThesisContent = {
  hero: {
    title: string;
    subtitle?: string;
    author: string;
    university: string;
    date: string;
    department?: string;
  },
  abstract: {
    title: "Abstract",
    content: "ção que evoluem de heurísticas baseadas em formato \npara uma classificação baseada em perfil, e emprega um \npós-processador do tipo votação para garantir uma extração \nde decisão robusta. A estrutura proposta é avaliada em \ntermos de precisão de encaminhamento, correção da \nresposta de ponta a ponta e perfil energético detalhado \n(CPU e GPU), utilizando um conjunto de dados compósito \nque combina conhecimento geral com forte dependência de \nrecuperação de informação e questões de ciência focadas \nem raciocínio (itens ao estilo ARC-Easy e HotPotQA), num \ncomputador com uma única GPU. Os resultados mostram \nque os prompts baseados em perfil podem melhorar o \nequilíbrio do encaminhamento: as versões mais \ndesenvolvidas atingem uma precisão de resposta superior a \n85% em consultas do tipo ARC, mantendo-se muito mais \neficientes em termos energéticos do que modelos maiores. \nAlém disso, as análises demonstram que as respostas \nincorretas consomem mais energia e que o design da \ninstrução transfere o consumo de energia entre a \nrecuperação de informação (RAG), intensiva em CPU, e o \nraciocínio, intensivo em GPU. Consequentemente, os \nnossos resultados indicam que o controlo arquitetónico e a \nengenharia de prompts podem diminuir a diferença de \ndesempenho entre modelos de pequena e média dimensão, \nenquanto alcançam ganhos de eficiência significativos e \nfornecem um caminho prático para sistemas de \nRecuperação de Informação (IR) e de Pergunta-Resposta \n\n- ii - \n \n \n(QA) de alta qualidade, sob fortes restrições de recursos e \nrequisitos de segurança de dados. \n \n \n \n \n  \n\n- iii - \n \n \n  \n\n- iv - \n \n \nKeywords \n \nAI; RAG; Quantization; HyDE\n\n; Large Language Models; \nInstruction Optimization. \n \n \nabstract \n \nAs the computational and financial costs of state-of-the-art \nlarge language models (LLMs) continue to grow, deploying \nthem becomes harder for resource-constrained \norganizations as improvement methods such as Retrieval-\nAugmented Generation (RAG), Chain-of-Thought (CoT), \nHyDE, and related techniques enhance quality but incur \nvariable overheads. This work presents a dynamic query-\nrouting framework, in which a compact LLM (8B \nparameters) is paired with an adaptive controller that selects \nfrom three routes per query: direct answer, CoT or, RAG. \nTherefore the controller builds on iterative prompt \nrefinement, proceeding through six instruction designs that \nevolve from format-driven heuristics to profile-based \nclassification, and employs a voting-style post-processor to \nensure robust decision extraction. The proposed framework \nis evaluated on routing accuracy, end-to-end answer \ncorrectness, and detailed energy profiling (CPU and GPU) \nusing a composite dataset that combines retrieval-heavy \ngeneral-knowledge and reasoning-focused science \nquestions (ARC-Easy and HotPotQA-style items), on a \nsingle-GPU workstation. Results show that profile-based \nprompts can improve routing balance: mature versions \nreach 85%+ answer accuracy on ARC-style queries while \nremaining much more energy efficient than larger models. \nMoreover, analyses show that incorrect answers consume \nmore energy, and that instruction design shifts the energy \nburden between CPU-heavy retrieval and GPU-heavy \nreasoning. Consequently our results indicate that \narchitectural control and prompt en\n\ngineering can close the \nperformance gap between small and mid-sized models \nwhile achieving significant efficiency gains and providing a \npractical path to high-quality IR and QA systems under \ntight resource constraints and data security requirements. \n \n \n \n  \n\n- v - \n \n \n \n \n\nContents\n1    Abstract2\n2    Resumo3\n3    Acknowledgments11\n4    Introduction15\n4.1Motivation .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .15\n4.2Aims and Research Questions    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .16\n4.3Document Outline .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .17\n5    State-of-the-Art17\n5.1Transformers and Language Models   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .17\n5.2Quantization .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .21\n5.2.1Post-Training Quantization (PTQ)  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .21\n5.2.2Weight-Only Quantization   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .23\n5.2.3Non Uniform Weight Quantization .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .24\n5.2.4Weight + Activation Quantization   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .30\n5.2.5Mixed Precision Quantization   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .30\n5.2.6Quantization-Aware Training (QAT)  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .32\n5.3Retrieval-Augmented Generation (RAG)  .  .  .  .  .  .  .  .  .  .  .  .  . \n\n .  .  .  .  .  .  .  .34\n5.3.1Hypothetical Document Embeddings (HyDE)  .  .  .  .  .  .  .  .  .  .  .  .  .  .36\n5.3.2Cache Augmented Generation   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .39\n5.3.3Hybrid approaches   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .40\n5.4Challenges and Applications of Quantization in RAG  .  .  .  .  .  .  .  .  .  .  .  .  .  .48\n5.5Retrieval Methods to enhance HyDE  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .48\n5.5.1Contriever .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .49\n5.6Self-Knowledge Guided Retrieval Augmentation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .50\n5.6.1Collecting Self-Knowledge .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .51\n5.6.2Eliciting Self-Knowledge of LLMs    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .51\n5.6.3Using Self-Knowledge for Adaptive Retrieval Augmentation.  .  .  .  .54\n5.7Model Comparison   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .54\n5.7.1MMLU   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .54\n5.7.2MMLU-Pro  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .54\n5.7.3GPQA .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .55\n5.7.4MUSR    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .56\n5.7.5BBH    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .\n\n  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .57\n5.7.6IFEval .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .58\n5.7.7ARC    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .58\n4\n\n5.7.8HellaSwag   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .59\n5.7.9ThrutfulQA  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .59\n5.7.10   WinoGrande   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.11   GSM8K .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.12   Math Lvl5    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.13   RAGEval  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.14   HotPotQA    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .63\n6    Methodology64\n6.1Overview of the Query Rewriting Flow    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .64\n6.2Hardware and Software Environment .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .66\n6.3Rewriting Approaches    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .66\n6.3.1Straight LLM  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .66\n6.3.2Chain-of-Thought    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n\n .  .  .  .  .  .  .  .  .  .66\n6.3.3RAG    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .67\n6.3.4Selecting the Approach .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .67\n6.4Dataset Augmentation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5Query-Answer Validation  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.1Automated Preparation  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.2AI-Powered Triage  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.3Human Verification  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.4Dataset formation .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.6Power Data Collection   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.6.1GPU    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.6.2CPU .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.7Evaluation Framework   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .71\n6.7.1Retrieval Performance Metrics  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .72\n6.7.2Straight Model   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .73\n6.7.3Chain-of-Thought Reasoning Performance Me\n\ntrics  .  .  .  .  .  .  .  .  .  .  .73\n6.7.4Automated Evaluation Script .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .73\n6.7.5Efficiency Metrics   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .74\n6.8Optimizing Query Classification through Iterative Prompt Refinement   .  .  .  .  .74\n6.8.1Instruction V1: A Simple Baseline .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .75\n6.8.2Instruction V2: An Aggressive, Safety-First Heuristic .  .  .  .  .  .  .  .  .  .77\n6.8.3Instruction V3: Introducing Balanced Criteria  .  .  .  .  .  .  .  .  .  .  .  .  .  .80\n6.8.4Instruction V4: A Shift to Profile-Based Classification   .  .  .  .  .  .  .  .  .84\n6.8.5Instruction V5: Final Refinement with a Guiding Principle  .  .  .  .  .  .  .88\n6.8.6Instruction V6: A Strategic Pivot to Efficiency    .  .  .  .  .  .  .  .  .  .  .  .  .92\n6.9Analysis of Energy Consumption and Efficiency .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .94\n5\n\n6.10  Detailed Energy Consumption Profiles  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .98\n6.10.1   Overall Energy Trends Across Instruction Versions  .  .  .  .  .  .  .  .  .  .  .98\n6.10.2   CPU vs. GPU: Deconstructing the Energy Cost  .  .  .  .  .  .  .  .  .  .  .  .  .   100\n6.10.3   The Energetic Cost of Correcting Errors  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   101\n6.10.4   Energy Distribution and Consumption Predictability   .  .  .  .  .  .  .  .  .  .   104\n6.10.5   Overall Performance Quadrant:  Synthesizing Accuracy and Efficiency\nfor ARC queries    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .",
  },
  introduction: {
    title: "Introduction",
    content: [
      "Answering\nPMIPointwise Mutual Information\nPPLPerplexity\nPTQPost-Training Quantization\nQATQuantization-Aware Training\nQRAQuestion-Reference-Answer\nRAGRetrieval-Augmented Generation\nRNNRecurrent Neural Network\nRPTQReorder-based Post-training Quantization\nRTNRounding to Nearest-Number\nRTRLReal-Time Recurrent Learning\nSKRSelf-Knowledge Guided Retrieval\nSLMsSmall Language Models\nSMEsSmall to Medium Enterprises\nSMLsSmall Language Models\nSpQRSparse-Quantized Representation\nT-PTLMsTransformer-based Pre-trained Language Models\nThrutfulQATruthfulQA\nvLLMA high-throughput LLM serving engine\nWinoGrandeWinograd Schema Challenge\n14",
      "4    Introduction\nArtificial Intelligence (AI) has revolutionized natural language processing (NLP) through\nthe  advent  of  Large  Language  Models  (LLMs),  which  demonstrate  exceptional  capabilities\nin  understanding  and  generating  human-like  language,  with  widespread  applications  across\ndiverse  industries.   However,  deploying  these  models  in  real-world,  regulated  environments\npresents substantial challenges.",
      "Organizations like banks, hospitals, and government offices are likely to handle sensitive\ninformation that should not exit their premise under strict data privacy laws like GDPR and\nCCPA. Legal restrictions like these inhibit the use of AI models that are often hosted on external\nservers,  where there is limited transparency in data processing operations.",
      "Furthermore,  the\ncomputational demands of LLMs make them prohibitively expensive for small and medium-\nsized enterprises (SMEs), which often lack access to high-performance hardware infrastructure.",
      "ntization and the development of lightweight LLMs,\na major gap still remains in effectively adapting these models to specialized, domain-specific\ntasks under limited computational resources.  Methods like Retrieval-Augmented Generation\n(RAG) and Hypothetical Document Embedding (HyDE) have emerged as strong contenders\nin the sense that they can integrate external knowledge into the models with ease.",
      "However,\nsuccess in such applications largely relies on the quality of query rewriting and retrieval.\nThis thesis focuses on retrieval-based question answering in such contrained domains, lever-\naging query rewriting to improve relevance and reduce computational overhead. This is achieved\nby  at  first  optimizing  Small  Language  Models  (SLMs)  through  advanced  quantization  tech-\nniques, which significantly reduce their computational and memory footprint.",
      "However, this ap-\nproach introduces a critical challenge, that is the degradation in model performance and knowl-\nedge retention.  To counter this effect, the system integrates a powerful Retrieval-Augmented-\nGeneration (RAG) framework.  This framework is not merely just add-on for external knowl-\nedge but it serves as a targeted mechanism to recover most of the performance lost during the\nquantization stage.",
      "By leveraging instructions optimization, and Chain-of-Thought reasoning,\nthe RAG component ensures that the quantized SLM can access and effectively utilize precise,\nrelevant information from external document.\n4.",
      "(LLMs).  Despite their capabili-\nties, significant challenges remain in deploying these models in highly regulated and resource-\nconstrained environments.  Organizations such as banks and public institutions face significant\nbarriers in adopting AI models due to strict data protection laws (eg., GDPR, CCPA) and high\ncomputational  requirements.   These  constraint  limit  their  ability  to  utilize  externally  hosted\nLLMs  or  fine-tune  large  models  for  domain-specific  tasks.",
      "demands of  LLMs make them expensive  to deploy,  requiring  powerful  hardware infrastruc-\nture that is often unaffordable for small to medium enterprises (SMEs).  While advancements\nin quantization techniques and lightweight models have made LLMs more accessible, there is\nstill a gap in optimizing these models for domain-specific tasks without extensive computa-\ntional resources.",
      "Retrieval-Augmented Generation (RAG) and HyDE methods have emerged\nas promising solutions, enabling models to integrate external knowledge efficiently.  However,\ntheir effectiveness depends on the quality of query rewriting and retrieval mechanisms.  This\nthesis seeks to address these challenges by evaluating lightweight, domain-aware strategies for\nquery rewriting within a RAG framework.",
      "By leveraging techniques such as query augmen-\ntation, voting system, Retrieval Augmented Generation and Chain-of-Thought reasoning, the\ngoal is to improve retrieval relevance and performance in regulated and resource-constrained\nsettings.",
      "ing practical insights for deploying AI systems in real-world ap-\nplications.\n4.2    Aims and Research Questions\nThis thesis aims to optimize small language models (SLMs), explore the trade-offs between\nquantized  and  non-quantized  models,  investigate  retrieval  methods  to  enhance  SLM  perfor-\nmance, and outline directions for future research.",
      "SLM optimization will be approached by ex-\nploring recent techniques identified throughout the course of this work, with the goal of making\nthese models more practical and resource-efficient. Analyzing the trade-offs between quantized\nand non-quantized models is an important step, as it will help determine whether future research\nshould focus on quantizing larger models or on further refining smaller ones.",
      "Retrieval methods are a good way to achieve better performance on SMLs on tasks that\nnormally would require training with more specific data-sets.  Selecting an effective retrieval\nstrategy is key to developing a lightweight, high-performing system.\nModel optimization may also include simple strategies such as query injection.  These ap-\nproaches will be evaluated to determine their usefulness and relevance to the overall objectives\nof the thesis.",
      "This research aims to answer the following key questions:\nRQ1:  How can a system using smaller models still compete with larger ones in terms of\nperformance and efficiency?\nRQ2:Can HyDE be applied to a system designed for efficiency?\nRQ3:  What are the trade-offs among answer quality,  inference latency,  and energy con-\nsumption for each rewriting strategy?\nRQ4: Can an efficient approach still achieve high accuracy while remaining useful?",
      "4.3    Document Outline\n1.  Introduction\n•  Sets the stage by explaining the motivation, challenges, and research questions.\n2.  Background and State-of-the-Art\n•  Reviews existing techniques and it’s limitations.\n3.  Methodology\n•  Details the approaches used to optimize SLMs and enhance retrieval.\n4.  Evaluation Framework\n•  Explains how the methods are assessed using different metrics.\n5.  Results and Discussion\n•  Analyzes the outcomes and trade-offs of the proposed methods.\n6.",
      "Conclusion and Future Work\n•  Summarizes the contributions and suggests future work.\n5    State-of-the-Art\n5.1    Transformers and Language Models\nNatural language processing (NLP) has been improving significantly over the last decades\ndue in part to the resurgence of deep neural networks (DNNs) [1].  The first sequential archi-\ntecture, RNN [2], had limitations regarding it’s ability to capture temporal dependencies.",
      "This\nlimitation is related to the vanishing or exploding gradient, which results in the impossibility for\nRNN to retain information over longer sequences. The longer the sequence, the more the gradi-\nents would diminish to near zero or infinity, resulting in less relevant weight updates. LSTM [3]\nand GRU [4] are two sequential models developed to overcome this limitation.",
      "LSTM was the\nfirst to use an algorithm to consider gradient-based learning, which could bridge time intervals\nin excess of 1000 steps. This was achieved using memory cells and gating mechanisms (input,\nforget and output).  This allowed the networks to retain, update, or forget information in the\nmemory cell, avoiding the vanishing/exploding gradient.",
      "U, doesn’t have\na separate memory cell; instead, it directly updates the hidden state using two gates: an update\ngate that combines the forget and input gates of the LSTM model and a reset gate that gives the\nnetwork the ability to control how much information it forgets.\n17",
      "Table  2:  Maximum  path  lengths,  per-layer  complexity  and  minimum  number  of  sequential\noperations for different layer types. [5]\nFigure 1: Transformer model architecture [5]\nTransformer  models  introduced  many  changes  to  try  to  solve  all  of  the  problems  in  the\nprevious  models.   Starting  with  the  Self-attention  Mechanism  which  lowers  the  complexity\nfor short sequences.",
      "This is due to the fact that the computational complexity is O(n\n2\n∗ d)\nwhich is more efficient than Recurrent layers O(n∗ d\n2\n) when the sequence n is smaller than\nthe dimensionality d.  When parallelization is used it requires only O(1) sequential operations\nbut on the recurrent layers they need O(n) sequential operations due to their sequential nature.",
      "Due to the short path length between any two positions in the input and output sequences the\nsignals can travel between all pairs of positions, this proved to be a big improvement for leaning\nlong-range dependencies. Moreover which can be useful to improve the global context since it\ncan attend to all positions in the sequence at once.",
      "adjust based on the input sequence, this can also be modified to make the model focus on just\nthe local context.\nThe ability to interpret attention mechanisms is a significant advantage for understanding\nhow the model works. Papers suc",
      "h as [6] investigate what the model focuses on, which in turn\nprovides insights into its decision-making process.\nMulti-Head Attention this mechanism instead of performing a single attention function with\nd\nmodel\n-dimensional keys, values and queries they linearly project the queries, keys and values\nh times with different learned linear projections to d\nk\n(Queries),d\nk\n(Keys) and d\nv\n(V alues) di-\nmensions.",
      "The queries represent the search The way Multi-Head Attention computes attention\nindependently for each head allows the model to focus on different types of relationships and\nfeatures in parallel capturing more information, this also makes it possible to capture informa-\ntion that with a single head would normally be lost like complex relationships [7].\nMulti-Head Attention extends the standard attention mechanism by applying multiple atten-\ntion functions in parallel.",
      "Instead of performing a single attention operation with d\nmodel\ndimen-\nsional keys, values, and queries, the mechanism linearly projects the queries, keys, and values\nh times using different learned linear transformations into dimensions d\nk\n(Queries),d\nk\n(Keys)\nand d\nv\n(V alues).  These projections allow each head to compute attention independently, en-\nabling the model to focus on different types of relationships and features simultaneously.",
      "This\nparallelization captures a richer set of dependencies and patterns, including complex relation-\nships that might otherwise be lost with a single attention head [7].",
      "s by scaling the dot products\nto prevent their values from becoming too large.\nIn natural languages,  word order plays a crucial role in conveying meaning and context.\nTherefore, it is essential for models to incorporate positional information.  Unlike recurrent or\nconvolutional models, the Transformer does not have an inherent mechanism to capture token\norder.",
      "To address this, positional encoding is introduced to inject information about the relative\nor absolute position of tokens within the input sequence.",
      "To make sure the the weight adapts to\nthe size of the query it uses the wave length of sin and cos that increases exponentially with the\ndimension of the index i, this grants that the positional encoding adjusts to compensate for any\ndimension length:\nPE(pos, 2i) = sin\n\npos\n10000\n2i\nd\nmodel\n\n[5]\nPE(pos, 2i + 1) = cos\n\npos\n10000\n2i\nd\nmodel\n\n[5]\nAs a benefit to how the model treats the positional encoding, it is able to adapt to sequences\nlonger than the ones found in training.",
      "tween them:\nFFN(x) = max(0,xW\n1\n+ b\n1\n)W\n2\n+ b\n2\n[5]\nReLU or rectified linear unit is used to add non-linearity to the network allowing it to learn more\ncomplex functions.   The non linearity comes from the activation function max(0,x),  which\nwhen the input x is positive it passes through without any change,  on the other hand when\nthe input is negative it outputs 0.",
      "r-layer for 2048 this means that w\n1\nwill transform from\n512 to 2048 and then w\n2\nwill transform the 2048 into 512.  ”Another way of describing this is\nas two convolutions with kernel size 1.”[5].\nThe evolution of the Transformer models came with GPT and BERT [8].",
      "GPT and BERT\nwere the first T-PTLMs developed based on transformer decoder and encoder layers respec-\ntively, these models where the basis for the discovery that performance of T-PTLMs could be\nincreased just by increasing the size of the model which triggered the development of mod-\nels like GPT-3 (175B)[9], PANGU- (200B)[10] and even a model with 1.6 trillions of tokens\nnamed Switch-Transformers [11].",
      "Although performance is not strictly linear and depends on\nmany factors, the number of tokens plays a significant role. This was only made possible in part\ndue to the parallelization ability of the Transformer model.  Bert[12] differs from the original\nTransformer model due to its bidirectional architecture contrary to the original model which\nwas unidirectional.  Bert uses its bidirectional architecture (left-to-right, right-to-left) to access\ncontext from both directions simultaneously.",
      "Another  important  feature  introduced  during  pretraining  is  Masked  Language  Modeling\n(MLM), in which 15% of the tokens in the input are randomly masked and the model learns to\npredict them using bidirectional context. Additionally, Next Sentence Prediction (NSP) is used\nto train the model to determine whether two sentences logically follow each other.",
      "odel in conjunction with segment embeddings\nto handle sentence pairs, this allowed BERT to handle both single sentence and sentence pair\ntasks.  GPT differs significantly in its training approach by combining both supervised and un-\nsupervised learning. In the unsupervised phase, the model is trained on a large text corpus (e.g.,\nBooksCorpus) using standard language modeling. This is followed by a fine-tuning phase using\nsupervised learning on specific downstream tasks.",
      "This two-stage process allows the model to\nfirst learn general language patterns and then adapt to more specialized tasks.  GPT also uses\nonly a Transformer decoder architecture, consisting of 12 layers of masked self-attention.  Be-\ncause it relies solely on a decoder, the model can only attend to previous tokens in the sequence,\nmaking it well-suited for auto-regressive language modeling.",
      "This model construction makes it\nsuitable to need minimal architecture changes when adapting to different tasks.",
      "5.2    Quantization\nQuantization is one of the most important techniques used to improve the performance and\nefficiency of large language models (LLMs), which are increasingly applied across a wide range\nof domains from customer service to scientific research.  However, as models grow in size and\ncomputational demands increase, a major challenge arises:  the hardware required to run these\nmodels becomes a limiting factor. High-performance hardware can be extremely expensive and\nenergy-intensive.",
      "s to widespread adoption. To give an example, Microsoft’s Phi-3-mini-4k-instruct model\n[13] requires 512 H100-80G GPUs to be run consecutively for 10 days with each costing around\nC30,000. Although such needs refer to training, which is done only once, running this kind of\nmodel still proves to be a computationally heavy task.\nThe most frequently used method to improve on this challenge is quantization, which re-\nduces the computational and memory requirements of the machine learning model.",
      "It achieves\nthis by converting the model weights and, in some cases, the activations from high-precision\n32FP to a lower precision representation such as INT8. This reduces the memory consumption\nof the model on the GPU and can accelerate computation because integer operations consume\nfewer resources compared to floating-point operations.\nThe two major methods of quantization will be discussed in more detail in subsequent sec-\ntions: Post-Training and Quantization-Aware Training.\n5.2.",
      "1    Post-Training Quantization (PTQ)\nTThis is the most commonly used quantization method because it does not require access\nto the model’s training process.  The quantization is applied after the model has already been\ntrained, making it especially useful when training resources or data are unavailable.  This type\nof quantization has been proven not to be as accurate at lower bit levels and there is a tendency\nof degradation if quantized to lower than 8-bits.",
      "[14] However since this method is less resource\nintensive, it has attracted more attention with a remarkable surge in post-training quantization\nmethods in the recent years.",
      "t efficient,  as it directly quantizes 16-bit values to\n8-bit using row-wise symmetric quantization.   While this method is straightforward,  it typi-\ncally results in only negligible degradation in perplexity. However, this breaks down with 4-bit\nquantization as it witnesses a significant drop in perplexity [15].  To improve the quantization\nperformance for low-bit applications, ZeroQuant-V2 [15] proposed Low-Rank Compensation\n(LoRC) method.",
      "This method approximates the error E between the original weight matrix W\nand the quantized weight matrix\nˆ\nW  using storage-efficient low-rank matrix\n ̃\nE so that\nˆ\nW +\n ̃\nE\nwould be a better approximation of the original weight W [16].",
      "distributions could achieve even lower bit-width quantization.  This is due to the weight distri-\nbution after training being nonuniform so it makes sense for the weight distribution not being\nquantized uniformly.  This is done by allowing the quantization process to allocate more pre-\ncision to the ranges of weights that are more densely populated while leaving larger intervals\nfor less frequent weight ranges.",
      "Building upon these methodologies OPTQ[19] emerged as an\nadvancement in quantization for big LLM’s.   Making it possible to run OPT-175B on just a\nsingle Nvidia A100 GPU or only two of the more cost-effective A6000.  OPTQ also provided\ngreater results in the extreme quantization regime where models were quantized all the way\ndown to 2 bits, or even ternary values.",
      "he weights in a greedy order [21] this\nmeans that the weight picked for the next quantization was picked based on minimum quan-\ntization error, this performs well but compared to arbitrary order quantization only offered a\nnegligible improvement in large, heavily-parameterized layers.",
      "The likely reasons for the lack\nof improvement were that large individual quantization errors were balanced out overall, and\nthat these errors occurred later in the quantization process when fewer weights remained to\nbe quantized, leaving less opportunity for adjustment.  But with the OPTQ approach instead\nof quantizing the weights row-by-row, this method aimed to quantize the weights in all rows\nsimultaneously and in the same order.",
      "This can be shown by how the unquantized weights F\nand the inverse layer hessian (H\n−1\nF\n) depend only on the input activations (X\nF\n) and not on the\nweights themselves, this proves that the quantization of a column affected all rows uniformly.\nColumns  within  blocks  are  quantized  recursively  and  at  each  step,  unquantized  weights  are\nupdated based on the quantized weights.\nFigure 2: RN18 squared error.",
      "[21]\nWith that it also achieved efficiency gains compared to OBQ which achieved O(d\nrow\n· d\n3\ncol\n)\ncomparing  it  to  OPTQ  which  achieves O(max{d\nrow\n· d\n2\ncol\n,d\n3\ncol\n}),  reducing  it  by  a  factor  of\n{d\nrow\n,d\ncol\n}.  For larger models this can be proven to be more efficient into several orders of\nmagnitude. See Table 3 for details on runtime analysis.",
      "The second step involves the use of lazy batch updates, which were introduced to improve\nupon the original Optimal Brain Quantization (OBQ) method.",
      "original approach, weights\nwere iteratively quantized, requiring updates to all elements of a potentially large matrix while\n22",
      "ParameterValue\nd\nrow\n10000\nd\ncol\n100\nRuntime TypeCalculation\nOBQ RuntimeO(10000· 100\n3\n) = O(10, 000· 1, 000, 000) = O(10\n10\n)\nOPTQ Runtime\nMax Term: O(max{10000· 100\n2\n, 100\n3\n})\nCalculations: 10000· 100\n2\n= 10, 000· 10, 000 = 10\n8\n100\n3\n= 10\n6\nResult: O(max{10\n8\n, 10\n6\n}) = O(10\n8\n)\nTable 3: Runtime Analysis\nusing only a few FLOPs per entry[21].   This means that the GPU usage was limited by the\nspeed of the memory bandwidth.",
      "Lazy Batch-Updates addressed this issue by grouping updates\nacross B × B  blocks of the inverse Hessian matrix H\n−1\n,  with B  being typically set to 128\ncolumns at a time. These blocks are processed and after that, they are used to apply updates to\nthe matrix.  Thus avoiding the need for frequent recalculations throughout the matrix, cutting\non memory-bound operations.",
      "The final modification was the Cholesky Reformulation that was used to improve on two\nkey issues, numerical inaccuracies and error accumulation, the numerical inaccuracies occur as\nmodel sizes increase beyond a few billion parameters, these can lead to instability.  This hap-\npens because the matrix H\n−1\nF\nbecomes indefinite during iterative updates causing erratic weight\nupdates, the error accumulation is caused by the compounding numerical errors of the matrix\ninversion.",
      "Though previously they used dampening techniques like adding a small constant λ\n(which was normally always 1% of the average diagonal value) to the diagonal elements of H .",
      "Cholesky decom-\nposition, precomputation of necessary rows, and dampening prevents the H\n−1\nF\nfrom becoming\nindefinite mitigating the accumulation of numerical errors, and makes the algorithm suitable\nfor large models.  A deeper dive into some of these methods are described in the subsequent\nsections.\n5.2.2    Weight-Only Quantization\nThe Weight-Only Quantization focuses on quantizing only the weights of the LLM and not\nthe activations, this reduces the model size and memory transfer time.",
      "However, this doesn’t\nbenefit from hardware-accerlerated low-bit operations.",
      "is rounding to nearest-nember (RTN).[22], this works by quantizing a tensor x into k-bits.\nQ[x] = s× clamp\n\nx\ns\n,l\nmin\n,l\nmax\n\n[22]\nHere the s is the quantization scale, l\nmin\nand l\nmax\nare the low and upper bound clipping, which\nis then rounded to the nearest number⌊·⌋. Usually setting l\nmin\n=−2\nk−1\n+ 1 and l\nmax\n= 2\nk\n− 1\nand set s to be the maximum absolute value in x.  There are two main ways to find the best\nconfiguration in weight only quantization.",
      "The first one is by minimizing the reconstruction\nerror of the weight parameter which is define as\nr(W) :=∥Q[W]− W∥\n2\n[22]\nOn the function above only the weights are accessed therefore it’s a data-free process. However\nrecent studies ([19], [23]) propose the usage of output error as compensation.\ne(W) =\nX\nX∈D\n∥Q[W]X − WX∥\n2\n[22]\nD corresponds to the calibration set sampled form of the original training data, for optimization.",
      "By regularizing the model with its training data, more promising results are achieved com-\npared to the reconstruction-based method.",
      "s of these 2 methods have a two\nbig draw backs, the first one being that there’s a requirement of having the original training data\nof the model since most quantizations are done by others than the creators off the model makes\nit hard to find exactly which data was used to train the model. The second problem is that using\nthe same data again can jeopardize the ability of generalization of the model due to the model\nover-fitting to the calibration set.",
      "For this two reasons it is clearly very important to achieve a\nData-free quantization.\n5.2.3    Non Uniform Weight Quantization\nMost of the typical quantization methods handle the weights differently from this approach.\nThis method quantizes weights differently depending on their importance to the LLM. Not all\nweights  are  of  equal  importance  to  the  performance  of  a  model,  and  so  they  should  not  be\ntreated similarly, which is the case with techniques such as EasyQuant.",
      "EasyQuant[22] proposes the usage of the reconstruction error as the regulation metric since\nthis can be used to optimize the quantized model indirectly improving the generalization ability\nof the model. According to [22] the performance gap of the quantized model (INT4) and the full\nprecision model is due to two main factors.",
      "The first being that normally the quantization range\nis picked as the maximum absolute value of the weight thus inducing a large reconstruction error\nfor low-bits quantization.  The latter refers to the fact that the 0.",
      "model’s performance.Keeping this in m",
      "ind, if we try to define the outliers using the condition:\n|W\ni,j\n− mean(W)|≥ n· var(W)    [22]\nFor  any  weight W ,  where W\nij\nis  the  (i,j)-th  weight  and  (n)  representing  the  threshold  for\nidentifying the outliers, we can classify certain weights as outliers [22]. However, the challenge\nis that simply detecting the outliers and avoiding their quantization is not sufficient to achieve\ngood model performance.",
      "Furthermore,  if the percentage of outliers becomes too large,  the\noverhead introduced by the dequantization kernel increases, which can lead to a reduction in\noverall throughput.\nTable 4: PPL results on Wikitext2 of BLOOM-7B with and without outlier isolation. [22]\nEasyQuant also experimented with an ablation study focusing on three aspects, the outlier\ninfluence, outlier distribution, and the Quantization Range.  The ablation study began by pre-\nserving 10% of the weights in FP16.",
      "This resulted in an 8% increase in perplexity, compared to\nonly a 1% increase achieved with EasyQuant.  These findings suggest that simply isolating the\noutliers was not sufficient to maintain the expected perplexity levels. To check the outlier influ-\nence on EasyQUant, outlier isolation is key however this can only impose an indirect influence\non the model accuracy.",
      "The phenomenon found is the outliers behave like a gating mechanism\nmeaning that without the outlier isolation, the model performance deteriorates significantly with\nsmaller reconstruction error, and with outliers in FP16 the model shows continuous improve-\nment decreasing the perplexity with smaller reconstruction error Table 4.",
      "h influence outliers with big weight magnitude and small weight\nmagnitude have on the model performance, this was done by pruning 1% of the values (accord-\ning to their magnitude) in the weights into 0 and see the perplexity results.\nTable 5: PPL results after pruning 1% weight with different magnitude [22]\nBased on Table 5 [22] shows that the largest magnitude outliers imposed the same influence\non the model performance as the normal values. This suggests that outliers exert a similar direct\n25",
      "influence on model accuracy as regular weights, thereby indicating that isolating outliers has an\nimportant indirect impact on the overall performance of the model.",
      "Table 6: Outlier fraction distribution in different modules in BLOOM-7B under 3-sigma thresh-\nold [22]\nTable 7:  Outlier fraction distribution in different layer index in BLOOM-7B under 3-sigma\nthreshold [22]\nOn  the  outlier  distribution,  it  was  explored  the  distribution  along  different  modules  and\nlayers, and it showed that the fraction of the outliers share different patterns in different modules\nand layers, refer to Tables 6 and 7.\nAnother characteristic found was that the FFN.",
      "2 module showed a significantly higher frac-\ntion of outliers, however there was no pattern along the layer index.\nOn the quantization range, it was observed that the dynamic quantization range of different\noptimization steps and concluded that the range decreased fast in the early stages of training\nmeaning a smaller quantization range facilitating more precise quantization of parameters.",
      "g that the optimal range had already been achieved.  In deep neural networks, not all\nweights have the same influence on the model’s performance some contribute more significantly\nthan others [22]. This implies that relying solely on the magnitude of the weights is insufficient\nto fully capture the impact of each element on the model’s behavior.  A good benchmark to\ndetect parameter sensitivity is the Hessian metric.",
      "This occurs due to the fact of the Hessian\nmatrix being leveraged to assess the salience of parameters in each under-binarized layer.",
      "The H  represents the Hessian matrix of each layer and the w\ni\nwhich represents the original\nvalue of each element. The s\ni\nis then used as a criterion for assessing the weight significance of\nthe element also used as a feature indicator for a structured selection.\nStructural search selection can be implemented using unstructured selection, allowing the\nmodel to cover all salient weights.",
      "However, this approach requires an additional 1-bit bitmap\nindex  [25],  which  increases  the  average  bit-width.   This  proves  to  be  inefficient,  especially\nfor the Hessian outlier weights that are only less than 1% of the total.  According to [24] the\nmajority  of  the  weights  that  are  sensitive  Hessian  values  are  predominantly  concentrated  in\nspecific columns or rows.\nFigure 3:  The Hessian metrics (sensitivity) and magnitude (value) of weights in LLMs.",
      "[24]\nThis pattern is due to the convergence effects inherent in multi-head self-attention mecha-\nnism of the models, thus needing a structured approach to select salient weights, reducing the\nadditional bit-map. The approach described in [24] is to employ a per-channel or per row type of\nbinarization, they determine salience through a per-column segmentation on the whole matrix.\nFigure 4:  Illustration of salient weight binarization.",
      "The B1 binarized from salient weight is\nmade into a residual with the original value and then binarized again to obtain B2.[24]\nThe  main  idea  is  to  rank  the  columns  by  their  salience  in  descending  order  and  use  an\noptimized search algorithm to minimize quantization error. This process determines the optimal\nnumber of columns to include in the salient group.",
      "where W\nb\ncorresponds to the binarized output, α denotes the scaling factor and W\nf\ndenotes the\nweights at full precision (FP16).  This was then used to define the objective of the binarization\nquantization, used in this equation:\narg min\nα,B\n∥W − αB∥\n2\n,[24](1)\nwhere the B is the number of selected columns, α and B can simply be solved as α =\n∥W∥\nℓ\n1\nn×k\nand B = sign(W).",
      "Then the optimization function to select salient columns is defined as:\narg min\nW\nuns\n∥W − (α\nsal\nsign(W\nsal\n)∪ α\nuns\nsign(W\nuns\n))∥\n2\n,[24]\nwhere W\nsal\ndenotes the column-wise combination of the original weight and W\nuns\nis the\nleft non-salient part. W can be determined by W\nsal\n∪W\nuns\nso the only variable parameter is the\nnumber of rows in W\nsal\n.",
      "allenge of preserving\nsalient weights which are limited in quantity, but exhibit significant variance when aggregated.\nIf  these  weights  are  preserved  at  their  original  formats  FP16  or  INT8  it  increased  the  aver-\nage weight bit-width, reducing the compression beneficts of binarization.  However traditional\nmethods of binarization result in a substantial quantization errors.",
      "Contrary to the comprehen-\nsive high-order quantization [26] which also applies quantization to the entire weight matrix,\nthe approach described in [24] uses a residual approximation method. This approach focuses on\nbinarizing only a subset of salient weights minimizing the error through a second-order aprox-\nimation.  This method grants the precision of salient weights while simultaneously decreasing\nbit-width overhead.",
      "As shown in Figure 3 this approach incorporates a recursive computation\nstrategy for weight binarization compensation, applying a subsequent binarization process to\nthe residuals remaining after the initial binary process.",
      "Based on the equation 1 they redesigned\nthe residual approximation optimization specifically for salient weights by implementing:\n\n\n\n\n\nα\n∗\no\n,B\n∗\no\n= arg min\nα\no\n,B\no\n∥W − α\no\nB\no\n∥\n2\n,\nα\n∗\nr\n,B\n∗\nr\n= arg min\nα\nr\n,B\nr\n∥(W − α\n∗\no\nB\n∗\no\n)− α\nr\nB\nr\n∥\n2\n[24]\nthe B\no\nrepresents the original binary tensor, while B\nr\ndenotes the residual binarized matrix\nwith the same size as B\no\n.",
      "the",
      "direct one of 1. The residual binarization error was defined byE .\nE\nrb\n=∥W − α\n∗\no\nB\n∗\no\n− α\n∗\nr\nB\n∗\nr\n∥\n2\n2\n[24]\nThe original binarized quantization error is calculated as∥W − α\n∗\no\nB\n∗\no\n∥\n2\n2\n, and from the second\nsub-equation of equation 5.2.3 it’s determined that loss E\nrb\n≤ ∥W − α\n∗\no\nB\n∗\no\n∥\n2\n.",
      "Thus showing\nthat the method of residual approximation proved to be able to further reduce the binary quanti-\nzation error of salient weights with ultra-low bit-width storage compared to retaining the salient\nweights at their full precision or even INT8.",
      "The performance of a LLaMA model with this super low quantization method is still impres-\nsive, only losing about 45% of the performance of the original model on a suit of benchmarks\nconsisting of PIQA, ARC-e, ARC-c, HellaSwag and WinoGrand, as depicted on Figure 8.",
      "5.2.4    Weight + Activation Quantization\nAlthough weights are already extremely hard to quantize, incorporating the activations into\nthe quantization pipeline adds another degree of complexity, especially for large language mod-\nels. Compared to weights, activations have some unique challenges since they are dynamic, and\ntheir range and statistics are unknown until runtime.",
      "The LLMs also have a unique problem,\nnot broadly seen for other transformer-based models: the systematic outlier activations.  These\noutliers if clipped during quantization can cause significant degradation in performance and\nrequire special attention if model accuracy is to be preserved [16].",
      "the  input  activations  and\nweights of a linear layer in OPT-13B[29]\nFigure 5: Comparison of activations and weights in LLAMA2-7B and OPT-13B models.\nAnother problem that makes it hard to quantize is the significant variations in value range\nacross different channels, which can be troublesome for the quatization algorithm. However, a\nstrong motivation for undertaking this complexity is the efficiency gained by quantizing both\nweights and activations to low-bit data types on specific hardware.",
      "This efficiency is demon-\nstrated by the performance of SmoothQuant, as shown in Figure 8.\n5.2.5    Mixed Precision Quantization\nRPTQ, or Reorder-based Post-Training Quantization, differs from per-tensor quantization\ntechniques in that it does not apply the same quantization parameters uniformly across the entire\ntensor. This distinction is important, as uniform quantization can sometimes lead to suboptimal\nresults.",
      "One key problem consists of the range of quantization being too wide to cover a large value\nrange, as this can cause increased quantization errors in channels with smaller value ranges.",
      "the other hand if the quantization range is too narrow it could lead to truncation of the outliers\nresulting in quantization errors.\nFor example if one channel as a range of -100 to -50 and another 80 to 100 when trying to\ncover their ranges by quantizing them form -100 to 100 this will result in a significant quanti-\nzation error for both channels.\nTo address this researchers have proposed several methods one of them being LLM.",
      "utliers in activations and low precision data types (INT8) for the remaining values. As\nexplained above this improves model performance preventing errors caused by the quantization\nof a wide range of values.  Another method for quantizing the activation SmoothQuant [29]\nsolved the problem by introducing a process that is meant to ”smooth” the input activation by\ndividing it by a per-channel smoothing factor s∈R, C\ni\n.",
      "To keep the mathematical equivalence\nof a linear layer the weights are scaled accordingly in the reversed direction:\nY = (X diag(s)\n−1\n)· (diag(s)W) =\nˆ\nX\nˆ\nW[29]\nThe next point of SmoothQuant is called Migrate Quantization Difficulty and the idea is to con-\ntrol the trade-off between the quantization difficulty of activations and weights by redistributing\ntheir values scale, meaning migrating the difficulty from activation to weights.",
      "The idea works by choosing a per-channel smoothing factor s such that\nˆ\nX = X diag(s)\n−1\nso it’s easier to quantize.  To reduce quantization error, the effective quantization bits for all\nchannels should be increased.   This maximizes the total effective quantization bits when all\nchannels share the same maximum magnitude, making the optimal choice the scale factor s\nj\n=\nmax(|X\nj\n|), j = 1, 2,...,C\ni\nwhere j corresponds to the j-th input channel.",
      "This choice grants\nthat after the division, all the channels will have the same maximum value, which makes it easy\nto quantize.  However, this formula shifts all the quantization difficulty to the weights.  As a\nresult, the quantization errors tend to be larger in the weights, leading to significant accuracy\ndegradation shown in Figure 6.",
      "6: Finding the sweet spot for the migration strength[29]\nHowever there is a possibility off also pushing all of the quantization difficulty from the\n31",
      "weights to the activations by choosing s\nj\n=\n1\nmax(|W\nj\n|)\n.  Similarly the model performance will\ndegrade heavily due to the activation quantization errors this introduces,  therefore there is a\nneed to split all of the quantization difficulty between weights and activations so they are both\neasier to quantize.",
      "SmoothQuant achieves this by introducing a hyper-parameter migration strength, depicted\nin Figure 6, to control how much difficulty will be migrated from activation to weights, this is\ndone using:\ns\nj\n=\nmax(|X\nj\n|)\nα\nmax(|W\nj\n|)\n1−α\n[29]\nFigure 7: Main idea of SmoothQuant when α is 0.5. The smoothing factor s is obtained on cal-\nibration samples and the entire transformation is performed offline. At runtime, the activations\nare smooth without scaling.",
      "[29]\nThis formula ensures that the weights and activations at the corresponding channel share a\nsimilar maximum value, thus sharing the same difficulty[29]. Figure 7 illustrates the smoothing\ntransformation when α = 0.5, this works for models where the activation outliers aren’t very\nsignificant, on models where the outliers are more significant (e.g. GLM-130B[31] which hap-\npens to have∼ 30% outliers), a larger α can be picked to migrate more quantization difficulty\nto the weights(like 0.7).",
      "The performance results for this method are very promising on models like OPT-175B [32]\nshow an efficient quantization at INT8 quantization level since the method can match the FP16\naccuracy on all evaluation datasets.",
      "true for LLaMA [33]. Although the\noutliers in this model tend to be less severe, SmoothQuant still performed well, with an average\nperformance drop of only 0.4%. The PyTorch implementation also proved effective, achieving\na 1.51× speedup and a 1.96× memory reduction for OPT models.\n5.2.6    Quantization-Aware Training (QAT)\nLLM-QAT is an advanced method for Quantization-Aware training (QAT) specifically de-\nsigned for LLMs[14].",
      "This method as been proven to be accurate to quantization levels as low as\n4-bits. This method helps in keeping the origianl output distribution and allows the quantization\nof weights, activations, and the key-value cache.  There are three core components, Symetric\nMinMax Quantization,  Student-Teacher Framework and Data Generation Process.",
      "ric MinMax Quantization is a method used to preserve the outliers in large language models\n(LLMs) and maintain their performance.\nX\ni\nQ\n=\n\nX\ni\nR\nα\n\n,  α =\nmax(|X\nR\n|)\n2\nN−1\n− 1\n,[14]\nIn the function above the X\nQ\nrepresents the quantized values, X\nR\nrepresents the full preci-\nsion and the α is the scaling factor this is a general quantization formula and it can be applied\nto both weights and activations, but the method to quantize differs depending on the target.",
      "In\nthe case of weights, per-channel quantization is used, meaning that at each channel (or filter)\nin the weight tensor it will be quantized independently.  This approach allows the quantization\nprocess to adapt accordingly to the specific range of values in every channel, preserving more\ninformation and reducing the possible quantization error.",
      "ntization is applied.  In this case, the quantization is performed independently\nfor each token, allowing the method to account for the diverse ranges of activation values across\nthe multiple tokens. This distinction ensures that the quantization process is purposely selected\nto the specific characteristics of weights, activations, and KV cache, optimizing the trade-off\nbetween efficiency and accuracy for each case.",
      "Then  LLM-QAT uses  the  student-teacher  model framework  to  ensure  that the  quantized\nmodel retains the performance of the full-precision model.  This works by having the teacher\nmodel the full-precision version guide the student, which is the newly quantized model.  The\nguidance is provided through cross-entropy-based logits distillation.",
      "L\nCE\n=−\n1\nn\nX\nc\nn\nX\ni=1\np\nT\nc\n(X\ni\n) log(p\nS\nc\n(X\ni\n))    [14](2)\nOn Equation 2 the i represents the i-th sample in the batch , c denotes the number of classes\n(vocabulary  size),  and T  and S  are  the  Teacher  and  the  Student  models,  respectively.   The\nnext-token data generation is based on the full-precision model and is proposed as a method to\nsynthesize a distribution similar to the pre-training data.",
      "This data is generated by the teacher\nmodel,  which  begins  with  a  random  start  token  and  iteratively  generates  subsequent  tokens\nuntil  it  reaches  the  end  of  the  sentence  or  the  maximum  sequence  length.",
      "from the full precision SoftMax output distribution.  Lastly, the\ngenerated  data  is  then  used  as  input  for  fine-tuning  the  quantized  model,  where  the  teacher\nmodel’s predictions serve as labels to guide training thus achieving a performance close to the\noriginal model yet quantized to a specified level.\n33",
      "5.3    Retrieval-Augmented Generation (RAG)\nLarge language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [34] this is accomplished without access to any outside data [5] this comes with\nsome pros and cons.  On the pros side, there’s the ability of the model to capture a lot of data\nand compress it, on the other hand, this comes with the downside of not being able to expand\nthe model knowledge or even revise their memory.",
      "Another downside is the hallucinations that\nsometimes are produced by this models. These limitations can be addressed using a method pro-\nposed by [35], known as Retrieval-Augmented Generation (RAG). This method consists of four\nmain components:  a query encoder (q), a retriever (p\nn\n), a document indexer, and a generator\n(p\n0\n).\nFigure 8: RAG implementation overview[35]\nThe first required model is a retriver DPR based on [36].",
      "This retriver works by indexing all\nthe passages in a low-dimensional and continuous space, so that later the top K passages can\nbe retrived efficiently. At run-time, passages that are relevant to the input question are retrieved\nfor the reader module.  The number of passages from which the model can select is extremely\nlarge the paper refers to a corpus of 21 million passages, with the value of K (i.e.",
      ") ranging between 20 and 100.\nDPR represented by p\nn\n(z|x) follows a bi-encoder architecture:\np\nη\n(z | x)∝ exp(d(z)⊤q(X)),  d(z) = BERT\nd\n(z),  q(x) = BERT\nq\n(x)    [35]\nWhere d(z)  is  a  dense  representation  of  a  document  produced  by  a BERT\nBASE\ndocument\nencoder [12] and q(x) a query representation produced by a query encoder, in this case also\nusing BERT\nBASE\n.",
      "The  retrieval  is  done  using  (MIPS)  Maximum  Inner  Product  Search  to\ncalculate the top-k(p\nη\n(· | x)), which represents the list of k  documents z  with highest prior\nprobability p\nn\n(z|x).  MIPS identifies these documents by finding those with the largest inner\nproduct between their dense representations and the query representation.",
      "This process can be\napproximately solved in sub-linear time using an efficient approximation to the nearest neighbor\nsearch, like FAISS or hierarchical navigable small-world graphs.  This is crucial for enabling\nthe scalability of large document collections such as Wikipedia.",
      "ument index.  The Generator is used to combine the input x with the retrieved content z using\nBART, these are simply concatenated.\nThe generator component given by p (y\ni\n| x,z,y\n1:i−1\n) could be modeled using any encoder\ndecoder, however BART-large [37] a pre-trained seq2seq transformer with 400M parameters, is\ncommonly used.\nBART pre trainned using a denoising objective which served as its foundation.",
      "encourage contextual understanding [38].\nThey refer to the BART Generator parameters 0 as the parametric memory and all of the\nretrieved external knowledge as non-parametric knowledge.  The marginalization can be done\nvia two  methods described  in the  paper RAG-Sequence  model [39]  uses the  same retrieved\ndocument to generate the entire output sequence.",
      "Specifically, it treats the retrieved document\nas a single latent variable, which is marginalized to obtain the sequence-to-sequence probability\np(y—x) using a top-K approximation.",
      "p\nRAG-Sequence\n(y|x)≈\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)p\nθ\n(y|x,z)\n=\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)\nN\nY\ni=1\np\nθ\n(y\ni\n|x,z,y\n1:i−1\n)\n[35](3)\nThis marginalization allows the model to combine information from the topk  document,\neffectively converging the information from diverse sources within the same document to gen-\nerate a coherent and contextually accurate output.",
      "The final retrieval step ensures that the most\nrelevant documents are selected based on their dense representations,\nThe second method is RAG-Token model which can be used to draw a different latent doc-\nument for each target token and marginalize accordingly.  This makes it possible for the gen-\nerator to choose the content from several documents when producing the answer.  The top-K\ndocuments are retrieved using a retriever.",
      "The generator then produces a probability distribu-\ntion for the next output token for each retrieved document.",
      "5]\nIn the case for sequence classification tasks RAG-Sequence and RAG-Token can be used by\nconsidering the target class as a target sequence of length one.\n35",
      "AspectRAG-TokenRAG-Sequence\nDocument UsageDifferent  documents  for  each  to-\nken.\nSame document for the entire se-\nquence.\nMarginalizationPer-token over top-K documents.Per-sequence   over   top-K   docu-\nments.\nBeam SearchStandard beam search.Beam search for each document.\nEfficiencyComputationally efficient.Thorough Decoding is expensive\nFlexibilityCombines information from multi-\nple documents dynamically.\nRelies on a single document for the\nentire sequence.",
      "Table 9: Comparison of Decoding Methods\n5.3.1    Hypothetical Document Embeddings (HyDE)\nHyDE [40] is another retrieval method that aims to increase the performance of the model\non zero-shot scenarios,  meaning that they can retrieve relevant documents without requiring\nspecific training.   This model is meant to work with any type of NLP model and is able to\ngeneralize across multiple tasks.",
      "This method differs from RAG [35] in that it uses a generator\nto produce a hypothetical document based on the input query.  This document does not need\nto be factually correct, as it is only used by the retriever (e.g., Contriever), which transforms\nit into a dense embedding vector.  This embedding represents the hypothetical document in a\nhigh-dimensional vector space.",
      "The retriever is then used to search the corpus for real documents that are similar in the\nembedding space,  this similarity is measured using inner product similarity between the hy-\npothetical document embeddings and the real document embeddings.",
      "similar\ndocument is fed into the model which also receives the input query, it then generates the output\nfor the query based on the retrieved document.\nFigure 9: An illustration of the HyDE model.[40]\nThe main issue addressed by [40] is the dependence on a separate query encoder required\n36",
      "by RAG [35] systems.  This is because dense retrievers compute similarity between the query\nand documents using inner product similarity, which necessitates a dedicated query encoder.\nFirstly it uses two encoder enc\nq\nenc\nd\nthat maps the query q and the document d into d di-\nmension vectors v\nq\n,v\nd\n, whose inner product is used as the similarity measurement.",
      "sim(q,d) =⟨enc\nq\n(q), enc\nd\n(d)⟩ =⟨v\nq\n,v\nd\n⟩    [40]\nThis is where zero-shot dense retrieval problems lie, it requires learning two embedding func-\ntions one for the query and the other for the document, these need to align into the same em-\nbedding space where the inner product can capture the document’s relevance. HyDE solves this\nproblem by performing a search in the document-only embedding space that captures the doc-\nument’s similarity.",
      "This method can be easily learned using unsupervised contrastive learning\n[41]. They set the enc\nd\ndirectly as the contrastive encoder enc\nc\non as follows:\nf = enc\nd\n= enc\nc\non    [40](4)\nThis  unsupervised  contrastive  encoder  is  be  shared  by  all  incoming  document  corpus.   The\nfunction 4 is also denoted as f .",
      "they call it InstructLM so it’s easy to represent. It then takes the query q and a textual\ninstruction INST  and follows them to perform the task specified in INST , like so:\nTo build the query vector, they use an instruction-following LLM in this case, text-davinci-\n003 from OpenAI’s GPT-3 series[42]. This model was specifically chosen for its strong gener-\nalization capabilities and is referred to as InstructLM  for ease of representation.",
      "It takes the\nquery q and a textual instruction INST , and follows the instruction to perform the specified\ntask, as shown below:\ng(q, INST) = InstructLM(q, INST)    [40]\nThe g can be used to map queries to the hypothetical documents by sampling from g, the INST\nis set to be ”write a paragraph that answers the question”, the generated document isn’t real and\nmay be factually incorrect due to models hallucinations [42].",
      "However, this is not important\nbecause  the  hypothetical  document  is  used  only  to  capture  the  relevance  pattern.   Then  the\nrelevance modeling is offloaded to an NLG that has the ability to generalize more easily, nat-\nurally,  and more effectively.",
      "query-document relevance.\nE[v\nq\nij\n] = E[f(g(q\nij\n, INST\ni\n))]    [40](5)\nThe f  corresponds to the document encoder, g defines a probability distribution based on the\nchain  rule.   In  their  implementation,  they  assume  that  the  distribution  of v\nq\nij  is  uni-modal,\nimplying that the query is not ambiguous.  To estimate Equation 5, they sample N  docume",
      "nts\nfrom g,\nh\nˆ\nd\n1\n,\nˆ\nd\n2\n,...,\nˆ\nd\nN\ni\n.\nˆv\nq\nij\n=\n1\nN\nX\nˆ\nd\nk\n∼g(q\nij\n,INST\ni\n)\nf(d\nk\n)\n=\n1\nN\nN\nX\nk=1\nf(\nˆ\nd\nk\n)    [40]\nThey also consider the query as a possible hypothesis,\nˆv\nq\nij\n=\n1\nN + 1\n\"\nN\nX\nk=1\nf(\nˆ\nd\nk\n) + f(q\nij\n)\n#\n[40]\nThe inner product is computed between ˆv\nqij\nand the set of all document vectors{f(d)|d∈ D\ni\n},\nthen the most similar documents are retrieved.",
      "In their implementation, the encoder function\nf acts as a lossy compressor, producing dense vectors in which unnecessary details are filtered\nout.  This enables the system to use a generated document even if it lacks factual accuracy to\neffectively search for the correct one.\nAccording to their study,  HyDE remains competitive even when compared to fine-tuned\nmodels.",
      "Another strong result comes from the web research setting, where the performance\nof  HyDE  is  particularly  impressive  even  when  compared  to  methods  that  rely  on  relevance\njudgments. Notably, HyDE achieves these results without requiring such judgments.\nTable  10:  Results  for  web  search  on  DL19/20.   Best  performing  w/o  relevance  and  overall\nsystem(s) are marked bold.  DPR, ANCE and ContrieverFT are in-domain supervised models\nthat are finetuned on MS MARCO training data.",
      "5.3.2    Cache Augmented Generation\nRAG is the most used approach for enhancing language models but as discussed previously\nit has two main drawbacks, specifically the retrieval latency and the potential errors in document\nselection [43].  LLMs have been increasing their context size over the years and the CAG [43]\napproach proposes a method that uses this increased context size to reduce the model",
      "latency\nand potential errors that could occur on RAG systems.\nFigure 10: Comparison RAG on the top and CAG on the botom[43]\nThis  approach  works  by  enabling  retrieval-free  knowledge  integration.   This  is  done  by\npreloading external knowledge sources, such as a collection of documents D ={d\n1\n,d\n2\n,...,d\nn\n}\nand precomputing these documents key-value (KV) cache C\nK\nV ,  this addresses some of the\ncomputational challenges and inefficiencies inherent to real-time retrieval on RAG systems.",
      "This approach is divided into three main steps:\nExternal Knowledge Preloading, represents the adaptation and preprocessing of the collec-\ntion of documents D that are relevant to the target application, this adapts them to fit within the\nmodel’s context window.",
      "This is done by the LLM M , with parameters 0, and processed the\ndocuments D, thus transforming it into a precomputed KV cache:\nC\nK\nV = KV − ENCODE(D)    [43]\nThis KV  cache which contains the inference state of the LLM, is stored on disk or in memory\nfor future use.  This implementation brings some of the computational cost down because the\ncost of processing D is incurred only once even if used in multiple subsequent queries.",
      "then the LLM uses this cached context to generate the responses:\nR = M(Q|C\nK\nV )    [43]\nBy giving the model the external cached knowledge it eliminates the retrieval latency and re-\nduces  the  risk  of  errors  or  omissions  that  comes  with  dynamic  retrieval.   The  prompt P =\nConcat(D,Q) ensures a unified understanding of the user query and the external knowle",
      "dge.\nCache Reset is the part of the system is responsible for maintaining performance across\nmultiple inference sessions. Since the KV  cache grows in an append-only manner sequentially\nstoring new tokens t\n1\n,t, 2,...,t\nk\nthe context may eventually need to be freed.  This is achieved\nby truncating the oldest tokens to prevent the cache from exceeding memory limits.\nC\nreset\nKV\n= Truncate(C\nKV\n,t\n1\n,t\n2\n,...",
      ",t\nk\n)    [43]\nThis ensures that the re-initialization is fast without the need to reload it from disk, ensuring a\nconstant speed and responsiveness.\n5.3.",
      "3    Hybrid approaches\nThere are two main Hybrid approaches focusing on different implementations RAGCache\n[44] and Self-Route [45] though these can’t be compared directly in terms of their implemen-\ntation since RAGCache is a system-level optimization for RAG and Self-Route is an approach\nthat selects the optimal way to give context to the model.",
      "Starting with Self-Route, this method combines the benefits of Retrieval-Augmented Gen-\neration (RAG) notably its proven effectiveness and efficiency in leveraging external knowledge\nwith the capabilities of recent long-context (LC) models, which can directly process and un-\nderstand extended contexts.  Models like Gemini 1.5 Pro [46] is able to achieve near-perfect\nrecall of up to 1M tokens and maintains this recall performance of up to 10M tokens.",
      "If this\ntrend of bigger and bigger context sizes continues this method could be a big improvement over\ntraditional RAG implementations.",
      "ions.  However, RAG\nis significantly more efficient, with its cost estimated to be around 20% of that required by LC\nmodels.\n40",
      "Figure 11: Comparison between performance and costs on multiple models using LC, RAG and\nSelf-Route[45]\nAs seen in Figure 11 the performance is maintained if not improved on some models but the\ncost on most cases is less than half. This is due to the nature of the approach, which combines\nthe strengths of both methods. When RAG can be applied, it offers reduced computational costs\nwhile maintaining most of the performance.",
      "However, despite the performance gap, there is a\nhigh degree of overlap in the predictions made by both methods.\nFigure 12: Distribution of the difference of prediction scores between RAG and LC[45]\nFigure 12 shows the differences between RAG prediction scores S\nRAG\nand LC prediction\nscores S\nLC\nthese are not just similar, in 63% of queries the model predictions are exactly identi-\ncal, and for 70% of queries, the score difference is less than 10%.",
      "This is also true for incorrect\nanswers when looking at the red color which corresponds to an accuracy of 0 it can be seen that\nRAG and LC make similar errors as well.\nSelf-Route  uses  the  LLM  itself  to  route  queries  based  on  self-reflection,  under  the  idea\nthat LLMs are well-calibrated in predicting whether a query is answerable given the provided\ncontext, this is done using a two-step approach RAG-and-Route and long-context prediction.",
      "RAG-and-Route works by providing the original query along with the retrieved text chunks\nto the LLM, prompting it to assess whether the query is answerable based on the given context.\nIf the mo",
      "del determines that the query can be answered,  it proceeds to generate a response.\nHowever, if it deems the query unanswerable, it is instructed to return the phrase ”unanswer-\nable” as per the prompt:  ”Write ’unanswerable’ if the query cannot be answered based on the\nprovided text.” In such cases, a fallback approach is then triggered.",
      "This approach consists of giving the LLM the full LC which consists of all documents able\nto fit the context , although this is not explained in the paper, it gives some hints that this must be\nthe case. This as seen on Figure11 giving a better result although with a higher cost. This trade-\noff is most cost-efficient when k = 5, meaning the number of retrieved documents is five.",
      "This\nis because, as k increases, the cost of RAG also increases, but so does the number of queries\nthat can be successfully routed.  Ultimately, the efficiency depends on the specific task being\nevaluated.  For instance, in extractive question answering tasks where multi-hop reasoning is\nnot required a lower k (e.g., k = 1) may result in lower computational cost.  Conversely, tasks\nthat require deeper reasoning may benefit from a higher k.",
      "Therefore, the optimal value of k is\ndependent on both the nature of the task and the level of performance required.\nFigure  13:  Trade-off  curves  between  (a)  model  performance  and  (b)  token  percentage  as  a\nfunction of k.[45]\nThe other hybrid approach RAGCache works by caching the key-value tensors of retrieved\ndocuments across multiple requests, this is done to try and minimize redundant computation for\nefficiency gains.",
      "Figure 14: RAGCache Overview[44]\nCache Structure and",
      "Replacement Policy operate differently from traditional cache systems\nthat cache individual objects.  Instead, this method caches the key-value tensor of the retrieved\ndocuments, which are sensitive to the order in which they are referenced. For instance consider\ntwo document sequences [D\n1\n,D\n3\n] with key-value tensors KV  and [D\n2\n,D\n3\n] with KV\n′\nrespec-\ntively.  Although KV [1] and KV\n′\n[1] both contain D\n3\n, their value differs.",
      "This occurs because\nthe key-value tensor of a given token being generated based on the preceding tokens, thus un-\nderscoring the order-dependence of key-value tensors. To aid retrieval speed while maintaining\ndocument order, RAGCache structures the document’s key-value tensors with a knowledge tree\n15.  The knowledge tree assigns each document to a node that refers to the memory addresses\nof the document’s key-value tensors.",
      "Similarly to vLLM [47], RAGCache also stores key-value\ntensors in non-contiguous memory blocks, allowing the KV cache to be reused.  The root of\nthe tree, S, corresponds to the shared system prompt.  A path from the root to any node rep-\nresents a unique sequence of documents, thus allowing RAGCache to handle multiple requests\nsimultaneously by leveraging overlapping paths.\nRAGCache retrieves tensors by performing prefix matching along these paths.",
      "If a subse-\nquent document in a sequence cannot be found among the child nodes, the traversal is termi-\nnated, and the longest identified document sequence is returned. This method ensures efficiency\nwith a time complexity of O(h), where h is the height of the tree.",
      "PGDSF) replacement policy is the name for the\n43",
      "node placement optimizer.  The knowledge tree decides each node’s placement within a hier-\narchical  cache.   For  example,  nodes  that  are  constantly  accessed  are  ideally  stored  in  GPU\nmemory, while those that are accessed less often are stored in slower host memory or are freed\ncompletely.",
      "The node placement optimization occurs when RAGCache uses a PGDSF replace-\nment policy that evaluates each node based on the access frequency, size, and access cost, due\nto its limited storing capacity the priority is defined by:\nPriority = Clock +\nFrequency× Cost\nSize\n[44](6)\nNodes that have a lower priority get freed first while the opposite is also true.",
      "The Clock\ntracks node access frequency, but to adapt to the cache hierarchy, there are two separate logical\nclocks: one for the GPU and another for the host memory. The Clock starts at zero and updates\nevery eviction, when a document is retrieved its clock is set and its priority gets adjusted this\nimposes that nodes with older clocks meaning less recent use, receive lower priorities.",
      "Clock = max\nn∈E\nPriority(n)    [44]\nFrequency in Equation 6 represents the total retrieval count for a document within a time\nframe, this count is reset upon system start or cache clearance. Priority is directly linked to\nthe frequency so the more frequently a document is accessed the higher the priority. Size is the\nnumber of tokens in the given document post-tokenization, thus directly linked to the memory\nrequired for its key-value tensors.",
      "PU performance as well as document\nsize and the sequence of preceding documents.\nFigure 16: Cost estimation PGDSF[44]\nPrefix awareness for RAG is achieved by the PGDSF method through two primary com-\nponents, cost estimation and node placement.  Accurately determining the computational cost\nfor RAG operations is challenging due to the complex dynamics of LLM generation.",
      "Figure\n16 illustrates this challenge by showing the cost differences for an identical request, denoted as\n[S,D\n1\n,D\n2\n,Q], under different caching conditions.",
      "ily on the cache prefix.  For instance,  if the prefix [S,D\n1\n] is already cached,  the subsequent\ncomputational cost includes the generation of key-value tensors for both D\n1\nand Q.  Making\nit extremely difficult to isolate the cost attributed solely to D\n2\n.",
      "To address this issue, PGDSF\nreplaces the Cost/Size term in Formula 6 with the following Equation:\nCost Size =\n1\nm\nm\nX\ni=1\nCost\ni\nNewSize\ni\n[44](7)\nIn Equation 7, m represents the number of requests that access a document not currently\nin cache.  The term Cost\ni\n/NewSize\ni\ndenotes the compute time per non-cached token for the\ni-th request.",
      "This approach effectively amortizes the computational cost across all non-cached\ntokens, thereby incorporating the document’s size into the priority calculation. The cost, Cost\ni\n, is determined through an offline profiling process where RAGCache measures the LLM prefill\ntime for multiple combinations of cached and non cached token lengths.",
      "en request at runtime. Each document\nretrieval event triggers an update to the corresponding node’s frequency, its cost estimation, and\nthe clock mechanism within the knowledge tree.  Furthermore, if a retrieved document is not\nalready in the cache, a new node is created for it in the tree.\nThe  management  of  the  node  placement  on  the  GPU,  host,  or  free  is  performed  by  the\nPGDSF as seen on Figure 15.",
      "The nodes in GPU serve as parent nodes to those in host memory,\nestablishing a hierarchical structure.  RAGCache also manages node eviction across these two\nsegments for efficiency, which is especially true when GPU memory is full. When this occurs,\nRAGCache swaps the lowest-priority node in the leaf nodes to the host memory; this process\nalso occurs on the host memory when it is full,  though in that case,  it is an eviction.",
      "This\nstrategy takes into account the Knowledge tree hierarchical partitioning, which is one key point\nto align with memory sensitivity and prefix sensitivity in LLM generation.  Due to the node\nneeding its parent node for key-value tensor calculation, the required placement of the parent\nnode is prioritized this is so rapid retrieval can be achieved.",
      "Because of PCIe limitations when connecting the GPU with the host memory in comparison\nwith GPU HBM, RAGCache adopts a swap-only-once strategy depicted in Figure 15, where\nyou can see that the key-value tensors of a node are swapped out to the host memory only for\nthe first eviction.  The host memory is responsible for keeping the key-value tensors until the\nnode is fully evicted from the entire cache.",
      "che directly frees the node node without copying any data Due to the size of the host\nmemory being two orders of magnitude larger than the GPU memory, keeping one copy of the\nkey-value tensors in the host memory is acceptable.\n45",
      "Figure 17: Cache aware Reordering[44]\nCache hit rate is vital for RAG Cache’s cache efficiency, but when paired with the unpre-\ndictability of the arrival pattern in user requests, this results in substantial cache trashing.\nRequests that refer to the same document may not be issued on the same time frame thus af-\nfecting the cache efficiency. For example given the requests{Q\ni\n,i%2 == 0} and{Q\ni\n,i%2 ==\n1} that target the documents D\n1\nand D\n2\nrespectively.",
      "When the cache capacity is only one\ndocument, the sequence{Q\n1\n,Q\n2\n,Q\n3\n} causes frequent swapping of the key-value cache of D\n1\nand D\n2\n, making it a zero cache hit rate.  But if a bit of attention is paid to rearrange requests\nto{Q\n1\n,Q\n3\n,Q\n5\n,Q\n2\n,Q\n4\n,Q\n6\n,Q\n6\n,...} this achieves a cache hit rate of 66% thus optimizing cache\nutilization. This shows how strategic request ordering can mitigate cache volatility and improve\ncache efficiency.",
      "To introduce the cache-aware reordering algorithm, two scenarios were con-\nsidered to show the key insights, the recomputation cost was assumed to be proportional to the\nrecomputation length. The first scenario is shown on Figure 17, (a) where it considers requests\nwith identical recomputation demands but varying cached context lengths with a limit of four\ncached documents.",
      "rocessing effectively uses Q\n1\n’s cache\nwhile discarding Q\n2\n’s, resulting in a computational cost of 2 + 1 + 2 = 5.  On the other hand,\nif the order was given as Q\n2\n,Q\n1\nthis would result in a usage of Q\n2\n’s cache but discarding Q\n1\n’s,\nwhich would increase computation to six due to 2+2+2 = 6. This is why cache-aware reorder-\ning advocates to prioritize requests with larger cached context thus improving cache efficiency\nas this brings larger benefits.",
      "In the second scenario (b), the aim was to examine requests with\nsimilar cached context lengths but varying recomputation demands, with a cache capacity of\nfive documents. On a sequence{Q\n1\n,Q\n2\n}, the system must clear Q\n′\n2\ns cache to allocate space for\nQ\n′\n1\ns computation, given only one available memory slot. This makes it necessary to recompute\nQ\n2\nentirely, which results in a cost of 2 + 2 + 1 = 5.",
      "On the other hand the sequence{Q\n2\n,Q\n1\n}\nallows for direct computation of Q\n2\n, due to adequate cache availability. It also reduces the total\ncomputation cost to 2 + 1 = 3, thus the reason why cache-aware reordering is beneficial when\nit prioritizes requests with shorter recomputation segments, this way results in a minimization\nof the adverse side effects on cache efficiency.",
      "Equation 8, directly prioritizes the requests that will probably lead to enhanced cache ef-\nficiency.   This is directly linked to the increase in the cache hit rate an",
      "d the decreased total\ncomputation time of RAGCache.  Model performance and resource usage are also improved\nthanks to this implementation. To avoid possible starvation when requests don’t align with the\ncached documents RAGCache sets a window for each request to ensure that all requests are\nprocessed in a timely manner.",
      "On an LLM enhanced with RAG, the key performance bottleneck is usually the LLM gen-\neration, however, if the vector database grows to a larger scale or a higher accuracy is needed in\nthe retrieval, this may cause the retrieval step to incur a substantial latency.",
      "To control the impact of retrieval latency, RAGCache employs dynamic speculative pipelin-\ning to overlap knowledge retrieval and LLM inference, and one thing that can occur is that the\nvector search may produce results earlier in the retrieval step, which can be used by the LLM\nfor speculative generation ahead of time.  This works by a vector search maintaining a queue\nof top-k candidate documents, which are ranked based on their similarity to the request.",
      "Dur-\ning the retrieval process the top-k documents are being constantly updated this is done so that\ndocuments that still being discovered, might come with greater similarity and so they get in-\nserted into the top-k. What could also occur is that the final documents may emerge early in the\nretrieval step [48].  Based on that RAGCache introduced a speculative pipelining strategy that\nsplits a request’s retrieval process into several stages.",
      "will start a new speculative generation and terminate the\nprevious one, if that doesn’t happen then the LLM engine just continues with the generation.\nWhen the top-k documents are finalized and there are no more changes to the top-k these are\nsent by RAGCache to the LLM engine and if they match with the ones previously received the\nengine simply returns the latest speculative generation.  Otherwise, the LLM performs a new\ngeneration with the new top-k documents.",
      "Figure 18: Speculative Pipelining[44]\nFigure 18 shows how RAGCache splits the retrieval process into four stages.   The top-2\ndocuments in candidate queue are [D\n1\n,D\n3\n], [D\n1\n,D\n2\n], [D\n1\n,D\n2\n] and [D\n1\n,D\n2\n] in the four stages.\nAfter the first stage is concluded, RAGCache sends [D\n1\n,D\n3\n] to the the LLM engine for specu-\nlative generation.",
      "the case it terminates the previous speculative generation and starts a new one with the correct\ndocuments. In stage three, the LLM engine receives the exact same documents as for stage two,\nso it continues with the previously started speculative generation. In the last stage, RAGCache\nsends the final top-2 documents to the engine which are still the exact same as the ones in stage\ntwo so there is no change to the speculative generation which is directly returned by the LLM\nengine as the result.",
      "The speculative pipelining allows RAGCache to overlap the retrieval and generation steps,\nand this greatly improves the end-to-end latency of RAG systems.",
      "is can introduce a\nlot of extra steps in the computation of the engine response.  This can be seen above Figure\n18, some speculative generations are incorrect and need to be recalculated.  This can lead to\nperformance degradation under high system loads,  but to solve this RAGCache dynamically\nenables speculative pipelining based on the system load. As an example, they assumed that the\nvector search and the LLM both serve only one request at a time.",
      "This vector search produces\ncandidate retrieval results at the end of each stage with a fixed time interval d.  Since the batch\nsize was set to only one, they could terminate any incorrect speculative generation requests.\nFigure 19: Optimal speculative pipelining strategy [44]\nRAGCache assumes that the LLM engine can schedule requests in the queue in any order,\nbut  it  processes  speculative  generation  requests  for  a  single  request  sequentially.",
      "Figure  19\nillustrates the optimal speculative pipelining strategy under this setting.\n5.4    Challenges and Applications of Quantization in RAG\n5.5    Retrieval Methods to enhance HyDE\nDue to the approach taken by RAG and HyDE on how they retrieve documents, these meth-\nods could be adapted or specifically chosen based on their ability in certain tasks.",
      "Retrievers\nare the basis for content that was retrieved from an external document corpus to enhance the\nLLM output, as well as provide grounds for the generated information on accurate documents.\nA more in-depth research will be done about some of these retrievers.",
      "5.5.1    Contriever\nThe original implementation of HyDE uses the Contriever model as its retriever.  This ap-\nproach w",
      "orks on the basis of contrastive learning, which is based on the fact that every document\nis, in some way, unique. According to [49], this is the only available information in the absence\nof manual supervision. A contrastive loss is used to learn by discriminating between documents.\nThis loss compares either a positive loss when they are the same document or negative when\nit’s from different documents.",
      "The formula responsible for this is:\nL(q,k\n+\n) =−\nexp (s(q,k\n+\n)/τ)\nexp\n\ns(q,k\n+\n)\nτ\n\n+\nP\nK\ni=1\nexp\n\ns(q,k\ni\n)\nτ\n\n′\n[49](9)\nIn Equation 9, q corresponds to the given query, which has an associated positive document\nk+, and a pool of negative documents(k\ni\n)\ni=1..k\n, τ  is the temperature parameter used to adjust\nthe sensitivity of the Contriver.  This function’s construction encourages positive pairs to have\nhigh scores and negative pairs to have low scores.",
      "One crucial piece of this method is how to\nbuild positive pairs from a single input. This could be done in two main ways: the Inverse Cloze\nTask or Independent cropping.\nThe  usage  of  the  Inverse  Cloze  Task  is  a  data  augmentation  strategy  that  generates  two\nmutually exclusive views of a document.  This approach was first described in [50].",
      "The first\nview is obtained by randomly sampling a span of tokens from a segment of text, and the second\nview is obtained by using the complement of the span.  This is done by in a given sequence of\ntext (w\n1\n,...,w\nn\n), ICT samples a span (w\na\n,...,w\nb\n), where 1 ≤ a ≤ b ≤ n, and then uses the\ntokens of the span as the query and the complement (w\n1\n,...,w\na−1\n,w\nb+1\n,...,w\nn\n) as the key.",
      "l for matching the query with the document directly.\nThis method is commonly used on images where multiple views are generated independently\nby cropping the input. Since this implementation is used for text it is done by sampling a span\nof tokens.  Because of the importance of the positive pairs this strategy samples independently\ntwo spans from a document to form the needed pair.",
      "Contrary to the inverse Cloze task in the\ncropping stage both views of the example correspond to the same contiguous subsequence of the\noriginal data. Another difference is that between cropping and ICT is that independent random\ncropping is symmetric meaning both of the queries and documents follow the same distribution.\nThis also causes overlap between the two views of the data, this being one of the reasons that\nencourages the network to learn exact matches between query and document.",
      "This works very\nsimilar to how lexical matching methods function, BM25 being a great example of this. So you\ncould either fix the length of the span for the query and key or sample them both.\nA big part of contrastive learning is how the system handles negative pairs this includes\nsampling a large set of negatives. This was tested by [49] using two methods, in-batch negative\nsampling and MoCo.",
      "batch.  For example, each item in the batch is transformed twice to generate the positive pairs,\nand the negatives are generated by using the other examples views from the batch, they called\nthese ”in-batch negatives”.  In this specific case, the gradient is back-propagated through the\nrepres",
      "entations  of  both  the  queries  and  the  keys.   The  main  downside  to  this  method  is  the\nrequirement for extremely large batch sizes to work well.\nThe other approach Negative pairs across batches tries to solve the problem by storing the\nrepresentations from previous batches in a queue and using these as negative examples in the\nloss calculation.  This makes it possible to have a smaller batch size but may slightly change\nthe loss by making it asymmetric between the queries.",
      "This being the view generated from\nthe elements of the current batch, and the keys, which are the elements already stored in the\nqueue.  This occurs as the gradient is only back-propagated through the queries,  leaving the\nrepresentation of the keys fixed. This is caused by the features being already stored in the queue\nfrom prior batches coming from past interactions with the network. A problem occurs when the\nnetwork is rapidly evolving during training can cause the performance to drop.",
      "Instead, they used the approach called MoCo [51] which generates representation keys from\na second network that is updated more slowly, the two networks are as follows one is responsible\nfor the keys, parametrized by 0\nk\n, and another network for the query, parametrized by 0\nq\n.  The\nparameter for the query network gets updated using back-propagation and stochastic gradient\ndescent.   This works similarly to when in-batch negatives are used.",
      "On the other hand,  the\nkey network also called Momentum encoder is only updated from the parameters of the query\nnetwork using an exponential moving average.\n5.",
      "ses the few-shot prompts to make the LLM judge if it knows the answer or\nnot.  In the case that the answer isn’t known, SKR [52] proceeds to the retrieval using RAG to\nimprove on the model response.\nTheir method works by three main ways: Collecting Self-Knowledge, Eliciting Self-knowledge,\nand Using Self-Knowledge. These represent the main pipeline for SKR, as shown in Figure 20.\nFigure 20: The SKR Pipeline and its component interactions.[52]\n50",
      "5.6.1    Collecting Self-Knowledge\nGiven a dataset D with question-answer pairs{q\nj\n,a\nj\n}\n|D|\nj=1\n, the model M is used to generate\nthe answers for each entry q\ni\n:\nˆa(M,q\ni\n) =M(q\n1\n◦ a\n1\n,...,q\nd\n◦ a\nd\n,q\ni\n)    [52](10)\nWhere ◦ denotes the concatenation and {q\nj\n◦ a\nj\n}\nd\nj=1\nare d demonstrations.   Equation 10\nrepresents the generated answer with ˆa(M,q\ni\n), this also represents the internal knowledge to the\nquestion q\ni\nin M .",
      "The other approach is to possibly find passages from external resources that\nmay be related with said question q\ni\n, these passages can then be used as additional information\nprovided on the model input. This is done per query, using a pretrained retriever represented by\nR to find the related information from the corpus C:\np\ni\n={p\ni1\n,p\ni2\n,...,p\nik\n} =R(q\ni\n,C),[52](11)\nAccording to Equation 11 the top-k retrieved passages for the question q\ni\nare represented\nby p\ni\n= {p\ni1\n,p\ni2\n,...,p\nik\n}.",
      "A dense passage retriever is used [36] for R, and C  consists of\npassage chunks from Wikipedia.  Then they use M  again to generate the answer with retrieval\naugmentation:\nˆa\nR\n(M,q\ni\n) =M(q\n1\n◦ p\n1\n◦ a\n1\n,...,q\nd\n◦ p\nd\n◦ a\nd\n,q\ni\n◦ p\ni\n).",
      "s ˆa(M,q\ni\n), ˆa\nR\n(M,q\ni\n) 12, and the ground-truth answer a\ni\n, they can cat-\negorize each question into a positive subset D\n+\nand a negative sub-set D\n−\nusing the differences\nbetween the results:\nq\ni\n∈\n\n\n\nD\n+\n,    if E[ˆa(M,q\ni\n)]≥ E[ˆa\nR\n(M,q\ni\n)];\nD\n−\n,    otherwise,\n[52]\nIn Equation 5.6.1 E is an evaluation metric like accuracy and exact match score, but they\nget discarded if the question q\ni\n, answer ˆa(M,q\ni\n) and ˆa\nR\n(M,q\ni\n) are incorrect.",
      "They then split\nthe training set into a subset D\n+\n= {q\n+\ni\n,...,q\n+\nm\n} which include questions that M  can directly\ngive correct answers to without external knowledge R and the other subset D\n−\n={q\n−\n1\n,...,q\n−\nn\n}\nwhere the R is needed for more accurate results.\n5.6.2    Eliciting Self-Knowledge of LLMs\nThe four different strategies proposed to detect the self knowledge of the target questions\nare direct prompting, in-context learning, training classifier, and nearest neighbor search.",
      "These\nwork by on the first two using the LLM itself and the latter two using smaller nodes purposely\nbuilt.",
      "Direct Prompting given a question q\nt\n, a straight-forward approach to detect wether LLMs\nare capable of solving it is to ask them directly:\nFigure 21: Direct Prompting [52]\nOn this method the prompt is used in conjunction with ’Do you need additional information\nto answer this question?’ to detect self-knowledge based on the response provided by the LLM.\nThis approach results in direct prompting the model and it may work.",
      "Although this doesn’t\nuse any of the collected training questions shown previously.  To improve on that 3 different\nstrategies where created.",
      "estions where selected from D\n+\nand D\n−\nas demonstrations to\nshow the self-knowledge of the question q\nt\n:\nFigure 22: In-Context Learning [52]\n52",
      "In here answer templates where used, ”No, I don’t need...” or ”Yes, I need...” in demonstra-\ntions based on wether the answer comes from the positive set D\n+\nor the negative one D\n−\n.\nThis direct prompting and in-context learning methods can induce self-knowledge of LLMs\nto some extent.  But they come with three main drawbacks.  First one being that both methods\nrequire designing prompts and calling the LLMs for each new question, making it cumbersome.",
      "Second, in-context learning could be also unstable due to contextual bias and sensitivity which\nis difficult to address in closed source LLMs. Third, use of all questions cannot be guaranteed,\ndue to the maximum tokens input of the LLMs. To avoid the above issues smaller models were\nused to help elicit self-knowledge.",
      "A classifier was trained using D\n+\nand D\n−\n, as a two-way classification problem using the\nsamples to train a BERT\nBase\nclassifier [12]:\nˆy\ni\n= softmax(Wh\ncls\n(q\ni\n) + b),(13)\nWhere q\ni\n∈ D\n+\n∪D\n−\nis a training question, h\ncls\n(q\ni\n) is the sentence-level representation\nfrom BERT\nBase\n, W  and b are parameters used by the classification head.  The parameters can\nbe optimized to improve the cross-entropy loss between the predicted label distribution ˆy\ni\nand\nthe ground-truth label of q\ni\n.",
      "Latter the training model can also be used to infer the label of new\nquestion q\nt\ndescribed in equation 13.",
      "based on the label of the questions through\nk-nearest-neighbor (kNN) search using a pre-trained fixed encoder as showed by Figure 23.\nThe kNN [53] is an algorithm widely used for a range of NLP tasks. This idea comes from\nthe similarity between the semantically embedded space of two questions if these are closely\nrelated then the knowledge needed for the model to answer would also be similar.",
      "Figure 23: k-nearest-neighbor to understand model knowledge [52]\nEach question was encoded into embeddings, and computed the semantic similarity through\ncosine distance sim(q\nt\n,q\ni\n) = (\ne(q\nt\n)·e(q\ni\n)\n||e(q\nt\n)||·||e(q\ni\n)||\n), where q\ni\n∈ {q\n+\n1\n,...,q\n+\nm\n,q\n−\n1\n,...,q\n−\nn\n}, e(·) being\nthe representation of a sentence encoder.  Then the search for the top-k nearest neighbors can\nbe done based on the results that include the l positive ones and k− l negative ones.",
      "be used to label the question q\nt\nas positive if\nl\nk−l\n≥\nm\nn\nor negative if\nl\nk−l\n<\nm\nn\nm and n are the\nnumber of questions from D\n+\nand D\n−\nrespectively.\n5.6.3    Using Self-Knowledge for Adaptive Retrieval Augmentation\nThe self knowledge acquired previously from the LLMs responses or the predicted labels\nreflect the necessity or not of retrieving external knowledge, for each question q\nt\naccordingly.\nSo that retrieval can be done or not depending on these results.\n5.",
      "7    Model Comparison\nThe model comparison will be used to help pick the correct model based on the require-\nments, these could be based on VRAM, performance or open-sourceness. A well-known leader-\nboard for LLM performance is llm.extratum.io [54].",
      "acteristics  like  VRAM  usage,  quantization  level,  model  size,  and  many  more.\nLlm.extratum.io also sorts based on performance that is measured on key evaluation data sets\nand metrics like GPQA, MUSR, BBH, IFEval, ARC, HellaSwag, MMLU, ThrutfulQA, Wino-\nGrande, GSM8K, MATH Lvl5 and MMLU Pro. These are some of the best methods to quantize\na model’s performance on multiple aspects and will be discussed further in the next sections.\n5.7.",
      "1    MMLU\nThe  MMLU  [55]  or  Massive  Multitask  Language  Understanding  is  an  LLM  benchmark\nthat consists of a dataset designed to be a comprehensive test of the model’s ability to respond\ncorrectly on a diverse range of tasks and topics.  It includes 57 different subjects that include\nknowledge across many disciplines like humanities, STEM, social sciences, and professional\nfields.   The  questions  in  the  benchmark  are  multiple-choice  of  four  options  there  are  over\n15.",
      "000 questions ranging from simple elementary math questions all the way to professional\nmedicine making it a very diverse benchmark. Some of the more important features include the\nstandardized evaluation metrics, calibrated difficulty levels, comprehensive coverage of human\nknowledge, and professional-level expertise requirements.  All of these points transform this\nbenchmark into a very important benchmark in the field due to its variety and complexity.",
      "This\ntest has however become easy for the most recent models like LLaMA 3 70B which achieved\n80.06 out of 100 [54]. To counteract the new advancements an improved version of the MMLU\nwas developed, MMLU-Pro[56].\n5.7.",
      "challenging multi-task language understanding bench-\nmark. This was achieved by increasing the complexity of the options expanding from 4 options\nto 10 thus reducing the probability of guessing the correct answer by chance this also made the\n54",
      "benchmark more challenging and more discriminative. This was done using GPT4-Turbo to in-\ntroduce six additional choices. These are created with the intuition of being plausible distractors\nthat need discerning reasoning to pick the correct answer.  The questions were also improved\nadding to the quality of the benchmark by eliminating trivial and noisy questions from the orig-\ninal MMLU.",
      "Which contained some that were found to be too easy by using a list of small\nLLMs when more than four were able to answer the question this question was then removed.\nThe benchmark was also improved by increasing the portion of more challenging questions by\nadding a bigger share of college-level exam problems.  All of these changes were then verified\nby two rounds of expert reviews to reduce the dataset noise.",
      "This proved to make the benchmark\nmore robust making it less sensitive to prompt variation changing from 4%− 5% to just 2%.\nThis came with the benefit of generating a more stable performance across different prompt\nstyles showing a greater consistency in model evaluation.  These improvements achieved a big\nimprovement in the discrimination between results of different models that previously scored\nsimilarly, the prior gap between GPT-4o and GPT-4-Turbo was 1% with MMLU-Pro it’s 9%.\n5.7.",
      "LLM’s ability to respond to 448 multiple-choice questions.  Which were created\nby domain experts in biology, physics, and chemistry.  These questions were tested on experts\npursuing PhDs in the corresponding domain and still only reached at best 74% when discounting\nthe clear mistakes the experts had identified in retrospect.  One good point to mention is that\nthis was also tested on what the paper describes as highly skilled non-expert validators where\nthe accuracy was only 34%.",
      "This was done with an average of 30 minutes per question and\naccess to the web to research the topic.  This dataset questions were first written by a domain\nexpert, this was then answered by another domain expert who would give constructive feedback\nto improve the clarity of the question and would also suggest revisions if needed.",
      "After said\nrevision by the writer of the question, it is sent to another different domain expert and three\nnon-expert validators who are experts in other domains to access the quality of the query and\nvarious options of answer.\nThis method made sure that the questions are proven and tested by at least two different\ndomain experts and three other area experts.  GPQA is divided into 3 subsets Extended, Main\nset, and Diamond.",
      "The extended subset contains all of the validated questions with 546 different\nquestions.\nGPQA is the name of the main not containing non-objective questions, these being those that\nboth domain expert validators got wrong yet the three non-expert validators got right.",
      "but agreed that they made a clear mistake after being\nshown the solution. The strictest set is the Diamond where only 198 questions are present, these\nare the highest quality and thus only include questions where both experts answered correctly\n55",
      "and the majority of non-experts answered incorrectly.  Though this also includes the questions\nwhere the second domain expert validator got the answer wrong yet explains his mistake once\nshown the correct one similarly to the previous set, but in this case, the first domain expert must\nanswer correctly.\nThis  benchmark  proved  very  efficient  at  achieving  the  proposed  results  since  even  with\ninternet access GPT-4 only achieved 39.4% which was an increase of just 0.",
      "4% from the original\nscore without internet access.  At that point in time when this benchmark was launched GPT-4\nwas the state of the art with older models like GPT-3.5 only achieving 28% check Table 11 for\nmore tests.\nTable 11: Accuracy on each set [57]\n5.7.4    MUSR\nThis benchmark was developed to evaluate LLMs on multistep soft reasoning tasks specific\nto the natural language narrative [58].  This is done by three main domains murder mysteries,\nobject placements, and team allocation.",
      "All of these are synthetic meaning, these were produced\nby an LLM. The creators choose to use synthetic stories due to two main reasons scalability and\nthe ability to regenerate the story due to possible data leaks.  The scalability is important since\nmore capable LLMs are being created every year,the dataset could be adapted as needed to\nbecome more complex and add longer narratives thus introducing more difficult access for the\nLLMs.",
      "The ability to regenerate the story is also very important since a data leak could mean\nthat the LLMs could be trained with the data of the benchmark which would take away the\nzero-shoot aspect of this benchmark.\n56",
      "Table 12: LLMs using CoT+, Humans scores on the multiple domains[58]\nThis data set was constructed using an LLM that is prompted to generate the gold facts\nrequired to deduce the correct answer.  Subsequently, these facts form the basis of a recursive\nquerying process, where the LLM is used to establish the reasoning that connects them, therefor\nconstructing a reasoning tree.  This tree components get then used one by one to generate the\nfinal narrative.",
      "This method generates a narrative that is a hard for machines yet solvable by\nhumans  refer  to  Table  12.   This  is  true  even  when  using  multiple  prompting  strategies  and\nneurosymbolic approaches like chain of thought plus.  The limiting factor discovered by the\nMUSR creators is the limitation that LLMs encounter when generating the deep reasoning trees\nthis greatly limits the narrative complexity.\n5.7.",
      "5    BBH\nBBH or Big-Bench-Hard is the improvement of the original Big-Bench which is a bench-\nmark  consisting  of  204  tasks  from  405  authors  across  132  institutions.   This  extensive  and\ndiverse authorship is a significant strength of the benchmark, as it serves to lower any potential\nfor institutional or individual biases that might come from more limited set of contributors.",
      "on-\ning, biology, physics, social bias, and software development, among others. The tasks selected\nfrom these domains were specifically chosen because they were considered to be beyond the\ncapabilities of state-of-the-art models at the time of its creation.\nBBH was formed, by curating a specific subset of tasks from the original Big Bench collec-\ntion. The selection process was designed to isolate the most dificult challenges for contemporary\nmodels.",
      "The resulting benchmark consists of merely 23 tasks, that were chosen exclusively due\nto the performance being lower for State-of-Art models than those achieved by human raters.\nFrom this group of tasks that proved difficult for machines,  only the most demanding were\nselected to form the final BBH set.\nThe  new  benchmark  prompting  also  differed  in  prompting  since  the  new  approach  uses\nChain-of-Thought prompting.",
      "some as much as 28.5%.  The categories of this dataset are also relevant since they don’t just\nfocus on algorithmic tasks, as they also have natural language understanding, world knowledge,\nand multilingual.  The last one being great addition since it transforms this dataset into a mul-\ntilingual one.  Though the linguistics part doesn’t affect the score by much due to its size in\nrelation with the whole dataset.\n5.7.",
      "6    IFEval\nIFEval [59] or instruction-following evaluation is used to project the ability of LLMs to\nadhere to verifiable instructions.",
      "ctable content, combination, case change,\nstart with / end with, and punctuation.  The instruction also comes with a description to exem-\nplify what the model is required to do.\nThis metric is designed to evaluate the model’s ability to adhere to the instructions provided\nby the proposed system, as illustrated in Figure 27. The method also accounts for errors in the\nmodel’s text formatting relative to the given instruction.",
      "This is accomplished using a flexible\naccuracy metric that tolerates superficial differences, such as formatting variations, provided\nthe core intent of the instruction is fulfilled.\nTo provide a comprehensive assessment, instruction-following is evaluated at two granular-\nities.  These being the per-prompt basis and the per-instruction basis.",
      "This dual-level analysis\nreveals whether the model can maintain adherence to a sequence of instructions or if it only\nfollows the initial ones successfully.\n5.7.7    ARC\nAI2 Reasoning Challenge [60] consists of a dataset and evaluation framework created with\nthe intuition of assessing and advancing the reasoning capabilities of AI systems. This system is\nspecially designed for assessing the models capabilities at answering challenging grade-school-\nlevel science questions.",
      "ARC contains two different sets, the challenge consists of 2590 queries and the easy con-\ntaining 5197.  The hard set is composed with queries that both retrieval-based solutions and\nword co-occurrence fail to solve, on the other hand the easy set is composed of questions that\ndon’t require a lot of reasoning to be answered.",
      "ion on the\nchallenge-set the LLM require deeper reasoning, due to the fact that these can’t be answered\nusing surface-level cues or simple retrieval methods.\nThe paper also talks about how even models that used IR or Pointwise Mutual Information\n(PMI) failed to outperform random guessing on the Challenge set.\n58",
      "5.7.8    HellaSwag\nHellaSwag [61] is a test set created to evaluate LLMs ability in commonsense natural lan-\nguage inference (NLI). The task is to choose the most reasonable continuation of a context,\nfrom four possibilities.\nThe test set is made to be simple for human participants with an accuracy of 95.6%, yet\nsimultaneously difficult for state-of-the-art models, with accuracy below 50%.",
      "The dataset is an extension of the original SWAG dataset but with the inclusion of Adver-\nsarial Filtering (AF). This is done to increase the difficulty of the task.  AF works by contin-\nuously selecting wrong answers generated by adversarial machines,  thereby keeping a chal-\nlenging dataset even for state-of-the-art models like BERT. The novelty lies in generating a\n”Goldilocks zone” of text difficulty, where the wrong answers are nonsensical to humans but\nare often misclassified by models.",
      "HellaSwag consists of 70,000 examples that are gathered from ActivityNet video captions\nand WikiHow text, thus contributing to the diversity of contexts in addition to the length and\ndifficulty of the examples. The dataset also includes zero-shot test classes, in which models are\ntested on unseen domains to estimate their ability to generalize.",
      "pecific bias. Results indicate that even the top models, i.e., BERT\nLarge\n, fail\nto generalize but instead depend on shallow lexical patterns rather than actual commonsense\nreasoning.\nThis  benchmark  indicates  the  weaknesses  of  existing  language  models  in  reasoning  and\nunderstanding the world, demonstrating that natural language processing progress demands the\ndevelopment of benchmarks that evolve together with progress in model capabilities.\n5.7.",
      "9    ThrutfulQA\nThrutfulQA [62] is a benchmark designed to evaluate the truthfulness of LLMs when an-\nswering questions. This benchmark is constructed with 817 questions across 38 different cate-\ngories, such as health, law, finance, and conspiracies. These questions were specially designed\nto test whether models generate imitative falsehoods, these are answers that mimic common\nhuman misconceptions or misinformation found in the training data, including falsehoods.",
      "For\nexample, some models might say that cracking knuckles causes arthritis, even though this is\na false statement.  This benchmark also shows a big gap in human to LLM performance, with\nhumans achieving 94% compared to just 58% of the UnifiedQA LLM. This method can be used\nto see if techniques like fine-tuning to prioritize truthfulness over imitation are achieving the\nwanted results.",
      "ThrutfulQA is a great tool to stop the spread of misinformation and reduce the\ndeception caused by the usage of LLMs by users that think these models are truthful.",
      "5.7.10    WinoGrande\nWinoGrande is an expanded version of the Winograd Schema Challenge, which originally\nconsisted of 273 expert-crafted pronoun resolution problems.  These",
      "problems are trivial for\nhumans but challenging for machine learning algorithms, as these require commonsense rea-\nsoning rather than reliance on statistical patterns or word association.  But with the state-of-art\nmodels achieving near-perfect scores there was a need to improve on the original method so\nWinoGrande was created.\nThis method introduces 44000 problems inspired by the previous method but this time de-\nsigned to be more challenging and also scalable.",
      "These new problems were created using crowd-\nsourcing and after-validated to ensure their trivialness to humans while still being difficult for\nstate-of-the-art models.\nThis data set shows a big gap in performance from LLMs to SLMs.\n5.7.11    GSM8K\nGSM8K [63] is a dataset specially crafted to evaluate the LLM’s ability to perform multi-\nstep mathematical reasoning.   This data set consists of 8.5K high-quality grade school math\nword  problems,  this  data  set  is  split  with  7.",
      "5k  on  the  data  set  and  1k  for  the  testing.   The\nproblems are linguistically diverse and need 2 to 8 steps to solve, these are focused on basic\narithmetic operations like addition, subtraction, multiplication, and division.  The solutions to\nthese are provided in natural language to encourage the model’s interpretability and reasoning.\nThis benchmark is used to test a model’s performance on informal reasoning and problem-\nsolving capabilities.\n5.7.",
      "12    Math Lvl5\nThis data set MATH [64] consists of various levels of math problems with five being the\nmost challenging tier. This test is designed to test advanced reasoning and heuristic applications.",
      "f mathematical concepts, creative problem-solving\nstrategies, and the ability to aggregate various techniques to find a solution.  The performance\nof LLM models like LLaMA 3.1 8B achieves only 5.36% [54], while International Mathemat-\nical Olympiad gold medalists achieve a near-perfect score.  This highlights the significant gap\nin performance between current AI models and expert-level human reasoning.",
      "The problems\nfound in this data set require logical chaining, abstraction, and error-free computation, areas\nwhere LLM’s performance tends to be lackluster.\n5.7.",
      "references.  All of these are generated by a schema-based pipeline to maintain accuracy.  This\napproach allows them to implement metrics that differ from the standard and are more aligned\nwith factual accuracy, there are three metrics for this, Completeness, Hallucination, and Irrele-\nvance.",
      "The process to generate all the needed files is as follows: S −→ C −→ D −→ (Q,A)−→ R −→\nKeypoints\nThis sequence shows all the steps taken by this approach starting with the schema summary\nS that leads to the configuration generation C, followed by the document generation D.",
      "With\nthe  document  formed  the  question  answer  pairs  are  formed (Q,A)  and  the  references  need\nto come that answer identified R and then the keypoints are extracted, representing the most\ncritical information in the answers.",
      "io-specific\ntext generation, these key elements encapsulate the aspects of essential factual knowledge from\nthe  input  documents.   This  schema  acts  as  a  backbone  to  ensure  that  the  content  is  diverse\nand reliable while maintaining a standard across various scenarios.   The schema defines the\nstructural framework of key elements for domain-specific documents without containing actual\ndata.",
      "As an example in medicine,  it can outline categories for symptoms and treatments,  in\nfinance it could establish classifications for metrics, sectors, and organizations.  One concrete\nexample of a schema generation starts with the initial generation by the LLM, the model gets\nfeed with carefully chosen seed documents, these are real legal documents that represent the\nkind of knowledge and structure that the schema is to take.",
      "After the schema is created a series\nof iterative refinements are taken by a human using it’s intuition and contextual understanding\nto fix any nuances the model had generated. Due to the fact that this process occurs more than\nonce, it ensures that a balance between comprehensiveness, accuracy, and generalization, thus\nsupporting content generation across diverse sub-scenarios.",
      "Generating a document that is rich with factual information and isn’t contradictory, is cru-\ncial to creating high quality datasets, ensuring that the generated content can be evaluated ac-\ncurately and used effectively in downstream tasks.  To generate documents with that quality,\nfirst the configurations C are generated, these derive from the previously established schema S.",
      "r text generation, thus maintaining\nconsistency across the document.  To generate these configurations a hybrid approach is taken\nthat combines rule-based methods with LLMs to assign values to the schema elements.  These\nrule-based methods like selecting values randomly from predefined scenario-specific options,\nensure that high accuracy and factual consistency is maintained for a more structured data. This\nand the more complex or diverse content, balances consistency and creativity.",
      "After the con-\nfiguration is ready a GPT-4o is used to convert the factual information from the C into a more\nstructured narrative format that is more aligned with a specific scenario. For example, in medi-\ncal records the generated document can include categories that add a more complex background\nto the document these can be a patient information, medical history, or a treatment plan.",
      "same is done with other topics but with categories that better align with them.\nFigure 24:  RAGEval System:  1 summarizing a schema containing specific knowledge from\nseed documents.  2 filling in factual information based on this schema to generate diverse con-\nfigurations.   3  generating  documents  according  to  the  configurations.   4  creating  evaluation\ndata composed of questions, answers, and references derived from the configurations and doc-\numents.",
      "[65]\nQuestion-Reference-Answer (QRA) to generate these RAGEval uses the documents D and\nconfigurations C  these are used to establish a robust evaluation framework ready to be used\non information retrieval and reasoning capable applications.",
      "estions as well as the initial answers, this forces the generated\ncontent to be aligned with the schema elements.  To address different types of questions like,\nmulti-hop reasoning,  summarization,  and multi-document questions,  each one is specifically\ndesigned to evaluate specific facets of language understanding. To ensure diversity and control-\nlability of these questions 7 main question types were designed.",
      "The model gets provided with\ndetailed instructions and examples for the question type needed to be generated, the model then\noutputs the question Q as well as the initial answer A. Using the Q and A the relevant informa-\ntion fragments get extracted R from the documents D. This is done using an extraction prompt,\nthus ensuring that the generated answer is grounded in the source material this improves the\nreliability and traceability.",
      "To reduce the misalignment between A and R, the answers get it-\neratively refined thus also improving the coherence and accuracy. If references contain content\nmissing from the answers they supplement them accordingly.  To reduce the hallucinations a\nlook is taken at the answers to find any unsupported content that either gets corrected with rel-\nevant references or removed.",
      "Finally the keypoints get generated from the answers A for each\nquestion Q to highlight the critical information in the responses. Normally each answer A gets\nbroken down into 3-5 keypoints, that encompass all essential factual detail, as well as relevant\ninferences, and conclusions.\nDragonBall dataset which means Diverse RAG Omni-Benchmark for All scenarios.",
      "and encompasses a range of texts\n62",
      "and RAG questions across 3 main domains finance,  law,  and medical.  This dataset consists\nof both Chinese and English texts, that serve as a comprehensive resource for multi-language\nscenario-specific research. Due to its big size of 6.711 questions it can be used on just English\nor Chinese assessment.\n5.7.14    HotPotQA\nHotPotQA [66] is a dataset with 113k question-answer pairs that contain 4 main features\nthat tell it apart.",
      "Number one being the question requires finding and reasoning over multiple\ndocuments to achieve a correct answer. Number two is the variety these questions present, these\nare diverse and aren’t constrained to any pre-existing knowledge bases or schemas.  Number\nthree this dataset provides sentence-level supporting facts that are needed for reasoning.",
      "Lastly\nnumber four it also introduces factoid comparison questions that test wether QA systems can,\nnot only extract relevant facts, but also compare them.\nThe data used for this dataset creation was gathered by crowd-workers on Amazon Mechan-\nical Turk.  The gathered data had it’s origin on English Wikipedia, the process followed five\nmajor steps.",
      "Starting with finding a paragraph on a specific Wikipedia page, next is the navi-\ngation to a hyperlynk found in that paragraph, this will later be a related article. The following\ntask is the formation of a question about the two articles, this formed query can’t be answered\nwith just the information of one and needs the two of them to be complete.   The following\nstep is the generation of a answer to the previous question this answer gets information from\nboth pages.",
      "tep is to identify the supporting facts, this is a crucial explainability step\nwhere the worker must select the specific sentences from the two articles that are needed to\nreason through and arrive to the final answer. The specific sentences found are the ground-truth\nsupporting facts. These steps form the generation pipeline of this method.\nThis method being specially develop with reasoning in mind, comes with two main types of\nreasoning Bridge-Entity, and Comparison Reasoning.",
      "Bridge-Entity  Reasoning  is  most  prevalent  question  in  HotPotQA  and  involves  a  bridge\nentity, meaning the two necessary documents are linked by a common person, place, or thing.\nTo  answer  the  question,  the  model  must  first  use  one  document  to  identify  this  bridge  and\nthen use the second document to find the final piece of information.",
      "As for the Comparison\nReasoning this requires the model to extract information and form various facts,  these facts\nneed then to be compared between themselves to find the final solution. This can be something\nlike ”when was the last public record of Astra systems publicized”.   This would require the\nmodel to find all the public records about Astra systems, then it would need to compare dates\nof all the documents, and only then would the final answer be evident.",
      "This dataset also comes with a evaluation framework specially designed to work with the\ndifferences found from more common datasets. The evaluation is measured in answer accuracy\nthat is the standard evaluation of wether the final answer is correct, this is done using a Exact\nMatch (EM) and F1 score.",
    ],
  },
  methodology: {
    title: "Methodology",
    content: [],
  },
  results: {
    title: "Results",
    content: [],
  },
  conclusions: {
    title: "Conclusions",
    content: [
      "mited computational resources, strict data privacy regu-\nlations, and tight budgets. The prohibitive cost and operational complexities of state-of-the-art\nLarge Language Models pose a significant barrier for small to medium enterprises and regulated\ninstitutions.   In  response,  this  research  proposed  and  evaluated  a  novel  framework  (Answer\nRQ1) centered on a compact, 8 billion parameter Small Language Model.",
      "The core innova-\ntion of this work is a dynamic, adaptive query routing system that intelligently triages incoming\nqueries, selecting the most efficient and effective path be it a direct answer, a reasoning-intensive\nchain of thought, or knowledge augmentation using retrieval-augmented generation.",
      "The results of this study show that architectural control and sophisticated prompt engineer-\ning can substantially bridge the performance gap between small, quantized models and their\nlarger, more resource-intensive counterparts.  Through iterative refinement of the controller’s\ninstruction design, particularly with profile-based prompts, (Answer RQ3) the system achieved\nan  accuracy  exceeding 85%  on  reasoning-focused  science  questions.",
      "Critically,  this  perfor-\nmance was achieved with significantly greater energy efficiency than would be possible using\nlarger models. The detailed energy profiling revealed a direct correlation between incorrect an-\nswers and higher energy consumption, and also showed how different instructions strategically\nshift the computational load between CPU-intensive retrieval and GPU-intensive generation.\nThus confirming that a ”smarter, not bigger” approach is a viable path forward.",
      "cations of this research are both practical and strategic.   It provides a tangible\nblueprint for developing high-quality, cost-effective, and secure question-answering system that\ncan operate on-premise on a single GPU workstation.  This work (Answer RQ4) democratizes\naccess to advanced AI capabilities, enabling organizations without massive computational in-\nfrastructure to leverage the power of language models.",
      "Furthermore, it also contributes to the\ngrowing field of sustainable AI by demonstrating that performance and efficiency are not mu-\ntually exclusive.\n10    Future work\nThe framework developed in this thesis successfully demonstrates that an intelligently con-\ntrolled Small Language Model can achieve high performance in resource-constrained environ-\nments.",
      "This research also opens up several compelling points for future investigation that could\nfurther enhance the robustness, efficiency, and applicability of this approach.\nFirst a critical area for future research is the system’s resilience to irrelevant or misleading\ninformation  within  its  knowledge  base.   The  current  experiments  utilized  a  corpus  that  was\nsometimes relevant to the ARC dataset.",
      "the ARC one, this way when retrieval occurred for ARC it wouldn’t improve the answer of the\nsystem. Though this is part of the advantage of a system like this, as it will always aim for the\nbest possible result.\nSecond, a novel and promising research direction based on the extracted results. A promis-\ning direction",
      "would be to explore the relationship between energy consumption and model hal-\nlucination. During this work, a correlation was observed between incorrect answers and higher\nenergy usage.  This suggests the possibility of identifying a computational signature for hal-\nlucination.  This could involve analyzing power draw and processing patterns to determine if\nnon-factual or fabricated responses can be detected in real-time based on their energy profile.",
      "If a reliable correlation is established, this could lead to the development of a mechanism that\nflags potential hallucinations as they are being generated, allowing the system to intervene and\nreroute the query for correction, thereby improving the model’s trustworthiness.\nFinally, the adaptive routing principles pioneered here for SMLs could be extended to ad-\ndress known inefficiencies in much larger models.",
      "State-of-the-art LLMs, despite their power,\ncan sometimes enter unproductive reasoning loops, repeatedly processing the same logic with-\nout reaching a conclusion, which wastes significant computational resources.  A future imple-\nmentation could adapt the controller to monitor the reasoning paths of an LLM. By detecting\nmajor semantic repetition or a lack of progress, the system could intervene to break the loop,\nperhaps by injecting new information via RAG or rewriting the prompt.",
      "This would not only\nprevent wasted computation and energy while also improving the reliability of LLMs in com-\nplex, multi-step reasoning tasks positioning this framework as a valuable tool for optimizing\nboth small and large language models.",
      "11    Images\nFigure 54: Average GPU Energy Consump",
      "tion by Domain.\n111",
      "Figure 55: Avg. Energy of Analysis Instructions that were also INCORRECT when Baseline Failed.\n112",
      "Figure 56: Correctness Comparison between system versus a 14B Model.\n113",
      "Figure 57: Average Energy Consumption by Domain.\n114",
      "Figure 58: Average CPU Energy Consumption by Domain.\n115",
      "Figure 59: Avg. Energy of Analysis Models that were CORRECT when Baseline Failed.\n116",
      "Figure 60: Science: Avg. Energy when Straight Model is Incorrect & System is Correct.\n117",
      "Figure 61: Average CPU Energy Consumption by Method.\n118",
      "Figure 62: Efficiency Score: Energy Cost of a Correct Answer for the system versus the 14B Model at the science domain.\n119",
      "Figure 63: Average GPU Energy Consumption by Method.\n120",
      "Figure 64: Average Energy Consumption by Method.\n121",
      "Figure 65: Science Domain: Avg. Energy when Baseline is Incorrect & System is also Incorrect.\n122",
      "Figure 66: Total Energy Consumption by Answer.\n123",
      "Figure 67: Average Energy Consumption in Science Domain by Correctness.\n124",
      "Figure 68: Average Energy for CORRECT Answers in Science Domain (with Counts).\n125",
      "Figure 69: Average Energy for CORRECT Answers in Science Domain.\n126",
      "Figure 70: Average Energy for INCORRECT Answers in Science Domain (with Counts).\n127",
      "References\n[1]    Wenpeng Yin et al. Comparative Study of CNN and RNN for Natural Language Process-\ning. arXiv:1702.01923 [cs]. Feb. 2017. DOI: 10.48550/arXiv.1702.01923. URL:\nhttp://arxiv.org/abs/1702.01923 (visited on 01/09/2025).\n[2]    Jeffrey  L.  Elman.  Finding  Structure  in  Time.  1990.  DOI: https : / / doi . org /\n10 . 1207 / s15516709cog1402 \\ _1.  eprint: https : / / onlinelibrary .\nwiley.com/doi",
      "/pdf/10.1207/s15516709cog1402_1.  URL: https://\nonlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1.\n[3]    Neural Computation. “Long short-term memory”. In: Neural Comput 9 (2016), pp. 1735–\n1780.  URL: https : / / interactiveaudiolab . github . io / teaching /\ncasa/HorchreiterSchmidhuber_LSTM.pdf (visited on 01/09/2025).\n[4]    Kyunghyun Cho et al. On the Properties of Neural Machine Translation: Encoder-Decoder\nApproaches. arXiv:1409.1259 [cs]. Oct. 2014. DOI: 10.48550/arXiv.1409.1259.",
      "URL: http://arxiv.org/abs/1409.1259 (visited on 01/09/2025).\n[5]    Ashish  Vaswani  et  al.  “Attention  is  All  you  Need”.  In:  Advances  in  Neural  Informa-\ntion  Processing  Systems.  Vol.  30.  Curran  Associates,  Inc.,  2017.  URL: https : / /\nproceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html (visited on 01/08/2025).\n[6]    Sofia  Serrano  and Noah  A. Smith.  Is  Attention Interpretable?  arXiv:1906.03731 [cs].\nJune 2019.  DOI: 10.48550/arXiv.",
      "1906.03731.  URL: http://arxiv.org/\nabs/1906.03731 (visited on 01/09/2025).\n[7]    [2006.16362] Multi-Head Attention: Collaborate Instead of Concatenate. URL: https:\n//arxiv.org/abs/2006.16362 (visited on 01/10/2025).\n[8]    Katikapalli  Subramanyam  Kalyan,  Ajit  Rajasekharan,  and  Sivanesan  Sangeetha.  AM-\nMUS : A Survey of Transformer-based Pretrained Models in Natural Language Process-\ning.  arXiv:2108.05542  [cs].  Aug.  2021.  DOI: 10.48550/arXiv.2108.05542.\nURL: http://arxiv.",
      "org/abs/2108.05542 (visited on 01/11/2025).\n[9]    Tom B. Brown et al. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs].\nJuly 2020.  DOI: 10.48550/arXiv.2005.",
      "5.  URL: http://arxiv.org/\nabs/2005.14165 (visited on 01/11/2025).\n[10]    Wei Zeng et al. PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language\nModels with Auto-parallel Computation. arXiv:2104.12369 [cs]. Apr. 2021.  DOI: 10.\n48550/arXiv.2104.12369.  URL: http://arxiv.org/abs/2104.12369\n(visited on 01/11/2025).\n128",
      "[11]    Dmitry Lepikhin  et al.  GShard:  Scaling Giant Models  with Conditional Computation\nand Automatic Sharding. arXiv:2006.16668 [cs]. June 2020. DOI: 10.48550/arXiv.\n2006 . 16668.  URL: http : / / arxiv . org / abs / 2006 . 16668  (visited  on\n01/11/2025).\n[12]    Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs]. May 2019.  DOI: 10.48550/arXiv.1810.\n04805. URL: http://arxiv.org/abs/1810.",
      "04805 (visited on 01/08/2025).\n[13]    Microsoft. microsoft/Phi-3-mini-4k-instruct at main. Sept. 2024. URL: https://huggingface.\nco / microsoft / Phi - 3 - mini - 4k - instruct / tree / main  (visited  on\n01/13/2025).\n[14]    Yanshu Wang et al. Art and Science of Quantizing Large-Scale Models: A Comprehen-\nsive Overview. arXiv:2409.11650 [cs]. Sept. 2024.  DOI: 10.48550/arXiv.2409.\n11650. URL: http://arxiv.org/abs/2409.11650 (visited on 01/08/2025).\n[15]    Zhewei Yao et al.",
      "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from\nComprehensive Study to Low Rank Compensation. arXiv:2303.08302 [cs]. May 2023.\nDOI: 10.48550/arXiv.2303.08302.  URL: http://arxiv.org/abs/\n2303.08302 (visited on 01/08/2025).\n[16]    Wenxiao Wang et al. Model Compression and Efficient Inference for Large Language\nModels: A Survey. arXiv:2402.09748 [cs]. Feb.",
      "4. DOI: 10.48550/arXiv.2402.\n09748. URL: http://arxiv.org/abs/2402.09748 (visited on 01/08/2025).\n[17]    Gunho  Park  et  al.  LUT-GEMM:  Quantized  Matrix  Multiplication  based  on  LUTs  for\nEfficient Inference in Large-Scale Generative Language Models. arXiv:2206.09557 [cs].\nApr. 2024.  DOI: 10.48550/arXiv.2206.09557.  URL: http://arxiv.org/\nabs/2206.09557 (visited on 01/08/2025).\n[18]    Sehoon Kim et al. SqueezeLLM: Dense-and-Sparse Quantization. arXiv:2306.07629 [cs].\nJune 2024.",
      "DOI: 10.48550/arXiv.2306.07629.  URL: http://arxiv.org/\nabs/2306.07629 (visited on 01/08/2025).\n[19]    Elias  Frantar  et  al.  GPTQ:  Accurate  Post-Training  Quantization  for  Generative  Pre-\ntrained Transformers. arXiv:2210.17323 [cs]. Mar. 2023.  DOI: 10.48550/arXiv.\n2210 . 17323.  URL: http : / / arxiv . org / abs / 2210 . 17323  (visited  on\n01/08/2025).\n[20]    Elias Frantar et al. “OPTQ: Accurate Quantization for Generative Pre-trained Transform-\ners”. en.",
      "In:  URL: https://openreview.net/forum?id=tcbBPnfwxS (vis-\nited on 01/24/2025).",
      "[21]    Elias Frantar and Dan Alistarh. “Optimal Brain Compression: A Framework for Accurate\nPost-Training Quantization and Pruning”. en. In: (). URL: https://proceedings.\nneurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-\nAbstract-Conference.html (visited on 01/08/2025).\n[22]    Hanlin Tang et al. EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs.\narXiv:2403.02775 [cs]. Mar. 2024.  DOI: 10.48550/arXiv.2403.02775.  URL:\nhttp://arxiv.org/abs/2403.",
      "02775 (visited on 01/08/2025).\n[23]    Zhewei Yao et al.",
      "r\nLarge-Scale Transformers”. en. In: ().  URL: https://proceedings.neurips.\ncc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-\nAbstract-Conference.html (visited on 01/08/2025).\n[24]    Wei Huang et al. BiLLM: Pushing the Limit of Post-Training Quantization for LLMs.\narXiv:2402.04291 [cs]. May 2024.  DOI: 10.48550/arXiv.2402.04291.  URL:\nhttp://arxiv.org/abs/2402.04291 (visited on 01/08/2025).\n[25]    Chee-Yong Chan and Yannis E. Ioannidis. Bitmap index design and evaluation.",
      "Seattle,\nWashington, USA, 1998. DOI: 10.1145/276304.276336. URL: https://doi.\norg/10.1145/276304.276336.\n[26]    Zefan Li et al. “ICCV 2017 Open Access Repository”. In: URL: https://openaccess.\nthecvf.com/content_iccv_2017/html/Li_Performance_Guaranteed_\nNetwork_ICCV_2017_paper.html (visited on 01/08/2025).\n[27]    Wei Huang et al. An empirical study of LLaMA3 quantization: from LLMs to MLLMs.\nDec. 2024. DOI: 10.1007/s44267-024-00070-x. URL: https://doi.org/\n10.1007/s44267-024-00070-x.",
      "[28]    Saleh Ashkboos et al. QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. arXiv:2404.00456\n[cs]. Oct. 2024.  DOI: 10.48550/arXiv.2404.00456.  URL: http://arxiv.\norg/abs/2404.00456 (visited on 01/08/2025).\n[29]    Guangxuan Xiao et al. SmoothQuant: Accurate and Efficient Post-Training Quantization\nfor Large Language Models. Ed. by Andreas Krause et al. July 2023.  URL: https:\n//proceedings.mlr.press/v202/xiao23c.html.\n[30]    Tim Dettmers et al. LLM.",
      "int8(): 8-bit Matrix Multiplication for Transformers at Scale.\narXiv:2208.07339 [cs]. Nov. 2022.  DOI: 10.48550/arXiv.2208.07339.  URL:\nhttp://arxiv.org/abs/2208.07339 (visited on 01/14/2025).",
      "t al. GLM-130B: An Open Bilingual Pre-trained Model. arXiv:2210.02414\n[cs]. Oct. 2023.  DOI: 10.48550/arXiv.2210.02414.  URL: http://arxiv.\norg/abs/2210.02414 (visited on 01/08/2025).\n130",
      "[32]    Susan Zhang et al. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068\n[cs]. June 2022.  DOI: 10.48550/arXiv.2205.01068.  URL: http://arxiv.\norg/abs/2205.01068 (visited on 01/08/2025).\n[33]    Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971\n[cs]. Feb. 2023.  DOI: 10.48550/arXiv.2302.13971.  URL: http://arxiv.\norg/abs/2302.13971 (visited on 01/08/2025).\n[34]    Fabio Petroni et al. Language Models as Knowledge Bases? arXiv:1909.",
      "01066 [cs]. Sept.\n2019. DOI: 10.48550/arXiv.1909.01066. URL: http://arxiv.org/abs/\n1909.01066 (visited on 01/08/2025).\n[35]    Patrick  Lewis  et  al.  “Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[36]    Vladimir Karpukhin et al.",
      "Dense Passage Retrieval for Open-Domain Question Answer-\ning.  arXiv:2004.04906  [cs].  Sept.  2020.  DOI: 10.48550/arXiv.2004.04906.\nURL: http://arxiv.org/abs/2004.04906 (visited on 01/08/2025).\n[37]    Mike Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Lan-\nguage Generation, Translation, and Comprehension. arXiv:1910.13461 [cs] version: 1.\nOct. 2019.  DOI: 10.48550/arXiv.1910.13461.  URL: http://arxiv.org/\nabs/1910.",
      "5).\n[38]    Fabing Duan, Franc ̧ois Chapeau-Blondeau, and Derek Abbott. “Optimized injection of\nnoise in activation functions to improve generalization of neural networks”. In: Chaos,\nSolitons & Fractals 178  (Jan.  2024), p.  114363.  ISSN:  0960-0779.  DOI: 10.1016/\nj . chaos . 2023 . 114363.  URL: https : / / www . sciencedirect . com /\nscience/article/pii/S0960077923012651 (visited on 01/08/2025).\n[39]    Patrick  Lewis  et  al.",
      "“Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[40]    Luyu Gao et al. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv:2212.10496\n[cs]. Dec. 2022.  DOI: 10.48550/arXiv.2212.10496.  URL: http://arxiv.\norg/abs/2212.",
      "10496 (visited on 01/08/2025).",
      "[41]    Luyu Gao and Jamie Callan. Unsupervised Corpus Aware Language Model Pre-training\nfor  Dense  Passage  Retrieval.  arXiv:2108.05540  [cs].  Aug.  2021.  DOI: 10.48550/\narXiv.2108.05540.  URL: http://arxiv.org/abs/2108.05540 (visited\non 01/08/2025).\n[42]    Long Ouyang et al. “Training language models to follow instructions with human feed-\nback”. en. In: (). URL: https://proceedings.neurips.cc/paper_files/\npaper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-\nConference.",
      "html (visited on 01/08/2025).\n[43]    Brian J. Chan et al. Don’t Do RAG: When Cache-Augmented Generation is All You Need\nfor Knowledge Tasks. arXiv:2412.15605 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.\n2412 .",
      "15605.  URL: http : / / arxiv . org / abs / 2412 . 15605  (visited  on\n01/15/2025).\n[44]    Chao Jin et al. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Gen-\neration. arXiv:2404.12457 [cs]. Apr. 2024.  DOI: 10.48550/arXiv.2404.12457.\nURL: http://arxiv.org/abs/2404.12457 (visited on 01/15/2025).\n[45]    Zhuowan Li et al. “Retrieval Augmented Generation or Long-Context LLMs? A Com-\nprehensive Study and Hybrid Approach”.",
      "In: Proceedings of the 2024 Conference on Em-\npirical Methods in Natural Language Processing: Industry Track. Ed. by Franck Dernon-\ncourt, Daniel Preot ̧iuc-Pietro, and Anastasia Shimorina. Miami, Florida, US: Association\nfor Computational Linguistics, Nov. 2024, pp. 881–893. DOI: 10.18653/v1/2024.\nemnlp-industry.66.  URL: https://aclanthology.org/2024.emnlp-\nindustry.66/ (visited on 01/15/2025).\n[46]    Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
      "arXiv:2403.05530 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.2403.05530.  URL:\nhttp://arxiv.org/abs/2403.05530.\n[47]    Woosuk Kwon et al. “Efficient Memory Management for Large Language Model Serving\nwith  PagedAttention”.  In:  Proceedings  of  the  29th  Symposium  on  Operating  Systems\nPrinciples. SOSP ’23. New York, NY, USA: Association for Computing Machinery, Oct.\n2023, pp. 611–626. ISBN: 979-8-4007-0229-7. DOI: 10.1145/3600006.3613165.\nURL: https://dl.acm.org/doi/10.1145/3600006.",
      "3613165 (visited on\n01/16/2025).\n[48]    Conglong Li et al. “Improving Approximate Nearest Neighbor Search through Learned\nAdaptive Early Termination”. In: Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data.",
      "MOD ’20. New York, NY, USA: Association\nfor Computing Machinery, May 2020, pp. 2539–2554.  ISBN: 978-1-4503-6735-6.  DOI:\n10.1145/3318464.3380600. URL: https://dl.acm.org/doi/10.1145/\n3318464.3380600 (visited on 01/19/2025).\n132",
      "[49]    Gautier Izacard et al. Unsupervised Dense Information Retrieval with Contrastive Learn-\ning. arXiv:2112.09118 [cs]. Aug. 2022. DOI: 10.48550/arXiv.2112.09118. URL:\nhttp://arxiv.org/abs/2112.09118 (visited on 01/20/2025).\n[50]    Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent Retrieval for Weakly Su-\npervised  Open  Domain  Question  Answering.  arXiv:1906.00300  [cs].  June  2019.  DOI:\n10.48550/arXiv.1906.00300.  URL: http://arxiv.org/abs/1906.\n00300 (visited on 01/20/2025).",
      "[51]    Kaiming He et al. Momentum Contrast for Unsupervised Visual Representation Learn-\ning. arXiv:1911.05722 [cs]. Mar. 2020. DOI: 10.48550/arXiv.1911.05722. URL:\nhttp://arxiv.org/abs/1911.05722 (visited on 01/21/2025).\n[52]    Yile Wang et al. “Self-Knowledge Guided Retrieval Augmentation for Large Language\nModels”. In: Findings of the Association for Computational Linguistics: EMNLP 2023.\nEd. by Houda Bouamor, Juan Pino, and Kalika Bali.",
      "Singapore: Association for Com-\nputational Linguistics, Dec. 2023, pp. 10303–10315.  DOI: 10.18653/v1/2023.\nfindings-emnlp.691. URL: https://aclanthology.org/2023.findings-\nemnlp.691/ (visited on 05/05/2025).\n[53]    Evelyn Fix and J. L. Hodges. “Discriminatory Analysis. Nonparametric Discrimination:\nConsistency Properties”. In: International Statistical Review / Revue Internationale de\nStatistique 57.3 (1989), pp. 238–247. ISSN: 03067734, 17515823. URL: http://www.\njstor.",
      "able/1403797 (visited on 06/17/2025).\n[54]    LLM Explorer. LLM Explorer: A Curated Large Language Model Directory. LLM List.\n41870 Open-Source Language Models. en.  URL: https://llm.extractum.io\n(visited on 01/24/2025).\n[55]    Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. arXiv:2009.03300\n[cs]. Jan. 2021.  DOI: 10.48550/arXiv.2009.03300.  URL: http://arxiv.\norg/abs/2009.03300 (visited on 01/08/2025).\n[56]    Yubo Wang et al.",
      "MMLU-Pro: A More Robust and Challenging Multi-Task Language Un-\nderstanding Benchmark. arXiv:2406.01574 [cs]. Nov. 2024. DOI: 10.48550/arXiv.\n2406 . 01574.  URL: http : / / arxiv . org / abs / 2406 . 01574  (visited  on\n01/08/2025).\n[57]    David Rein et al. GPQA: A Graduate-Level Google-Proof Q&A Benchmark. arXiv:2311.12022\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.12022.  URL: http://arxiv.\norg/abs/2311.12022 (visited on 01/08/2025).\n[58]    Zayne Sprague et al.",
      "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\nReasoning.  arXiv:2310.16049  [cs].  Mar.  2024.  DOI: 10 . 48550 / arXiv . 2310 .\n16049. URL: http://arxiv.org/abs/2310.16049 (visited on 01/08/2025).",
      "[59]    Jeffrey Zhou et al. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.07911.  URL: http://arxiv.\norg/abs/2311.07911 (visited on 01/08/2025).\n[60]    Peter Clark et al. Think you have Solved Question Answering? Try ARC, the AI2 Reason-\ning Challenge. arXiv:1803.05457 [cs]. Mar. 2018.  DOI: 10.48550/arXiv.1803.\n05457. URL: http://arxiv.org/abs/1803.05457 (visited on 01/24/2025).\n[61]    Rowan Zellers et al.",
      "Really Finish Your Sentence? arXiv:1905.07830\n[cs]. May 2019.  DOI: 10.48550/arXiv.1905.07830.  URL: http://arxiv.\norg/abs/1905.07830 (visited on 01/08/2025).\n[62]    Stephanie  Lin,  Jacob  Hilton,  and  Owain  Evans.  TruthfulQA:  Measuring  How  Models\nMimic Human Falsehoods. arXiv:2109.07958 [cs]. May 2022. DOI: 10.48550/arXiv.\n2109 . 07958.  URL: http : / / arxiv . org / abs / 2109 . 07958  (visited  on\n01/08/2025).\n[63]    Karl Cobbe et al. Training Verifiers to Solve Math Word Problems.",
      "arXiv:2110.14168\n[cs]. Nov. 2021.  DOI: 10.48550/arXiv.2110.14168.  URL: http://arxiv.\norg/abs/2110.14168 (visited on 01/08/2025).\n[64]    Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset.\narXiv:2103.03874 [cs]. Nov. 2021.  DOI: 10.48550/arXiv.2103.03874.  URL:\nhttp://arxiv.org/abs/2103.03874 (visited on 01/08/2025).\n[65]    Kunlun  Zhu  et  al.  RAGEval:  Scenario  Specific  RAG  Evaluation  Dataset  Generation\nFramework.  arXiv:2408.01262  [cs].  Mar.  2025.",
      "DOI: 10.48550/arXiv.2408.\n01262. URL: http://arxiv.org/abs/2408.01262 (visited on 03/28/2025).\n[66]    Zhilin Yang et al. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question An-\nswering. arXiv:1809.09600 [cs]. Sept. 2018. DOI: 10.48550/arXiv.1809.09600.\nURL: http://arxiv.org/abs/1809.09600 (visited on 07/29/2025).\n[67]    Rolf Jagerman et al. Query Expansion by Prompting Large Language Models. arXiv:2305.03653\n[cs]. May 2023.  DOI: 10.48550/arXiv.2305.03653.  URL: http://arxiv.",
      "org/abs/2305.03653 (visited on 01/22/2025).\n[68]    Lihu Chen and Ga\n ̈\nel Varoquaux. What is the Role of Small Models in the LLM Era: A\nSurvey. arXiv:2409.06857 [cs].",
      "Dec. 2024.  DOI: 10.48550/arXiv.2409.06857.\nURL: http://arxiv.org/abs/2409.06857 (visited on 01/08/2025).\n[69]    DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning. en. arXiv:2501.12948 [cs]. Jan. 2025.  DOI: 10.48550/arXiv.\n2501 . 12948.  URL: http : / / arxiv . org / abs / 2501 . 12948  (visited  on\n06/20/2025).\n134",
      "[70]    Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can\nSolve  Them.  arXiv:2210.09261  [cs].  Oct.  2022.  DOI: 10.48550/arXiv.2210.\n09261. URL: http://arxiv.org/abs/2210.09261 (visited on 01/08/2025).\n[71]    Xinyi Wu et al. On the Emergence of Position Bias in Transformers. arXiv:2502.01951\n[cs]. June 2025.  DOI: 10.48550/arXiv.2502.01951.  URL: http://arxiv.\norg/abs/2502.01951 (visited on 06/20/2025).\n[72]    docs.nvidia.com/deploy/nvidia-smi/index.html.",
      "URL: https://docs.nvidia.com/\ndeploy/nvidia-smi/index.html (visited on 07/02/2025).\n[73]    NVML Device Queries. en-us. cppModule.  URL: https://docs.nvidia.com/\ndeploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_\n1g7ef7dff0ff14238d08a19ad7fb23fc87 (visited on 07/02/2025).\n[74]    Alireza Salemi and Hamed Zamani. “Evaluating Retrieval Quality in Retrieval-Augmented\nGeneration”.",
      "In: Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearch  and  Development  in  Information  Retrieval.  SIGIR  ’24.  New  York,  NY,  USA:\nAssociation for Computing Machinery, July 2024, pp. 2395–2400.  ISBN: 979-8-4007-\n0431-4.  DOI: 10.1145/3626772.3657957.  URL: https://dl.acm.org/\ndoi/10.1145/3626772.3657957 (visited on 01/23/2025).",
    ],
  },
  downloads: {
    title: string;
    thesisPdf: string;
    presentationPptx: string;
    frameworkGitHub?: string;
  },
  contact: {
    title: string;
    name: string;
    email: string;
    university?: string;
    department?: string;
  },
};
