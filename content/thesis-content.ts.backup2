// Thesis content - Auto-generated from PDF extraction
// This file was generated automatically. Manual edits may be overwritten.

export interface ThesisContent {
  hero: {
    title: string;
    subtitle?: string;
    author: string;
    university: string;
    date: string;
    department?: string;
  };
  abstract: {
    title: string;
    content: string;
  };
  introduction: {
    title: string;
    content: (string | { type: 'image'; src: string; alt: string; caption?: string })[];
  };
  methodology: {
    title: string;
    content: (string | { type: 'image'; src: string; alt: string; caption?: string })[];
  };
  results: {
    title: string;
    content: string[];
  };
  conclusions: {
    title: string;
    content: (string | { type: 'image'; src: string; alt: string; caption?: string })[];
  };
  downloads: {
    title: string;
    thesisPdf: string;
    presentationPptx: string;
    frameworkGitHub?: string;
  };
  contact: {
    title: string;
    name: string;
    email: string;
    university?: string;
    department?: string;
  };
}

export const thesisContent: ThesisContent = {
  hero: {
    title: string;
    subtitle?: string;
    author: string;
    university: string;
    date: string;
    department?: string;
  };
  abstract: {
    title: string;
    content: string;
  };
  introduction: {
    title: "Introduction",
    content: [
  "Answering\nPMIPointwise Mutual Information\nPPLPerplexity\nPTQPost-Training Quantization\nQATQuantization-Aware Training\nQRAQuestion-Reference-Answer\nRAGRetrieval-Augmented Generation\nRNNRecurrent Neural Network\nRPTQReorder-based Post-training Quantization\nRTNRounding to Nearest-Number\nRTRLReal-Time Recurrent Learning\nSKRSelf-Knowledge Guided Retrieval\nSLMsSmall Language Models\nSMEsSmall to Medium Enterprises\nSMLsSmall Language Models\nSpQRSparse-Quantized Representation\nT-PTLMsTransformer-based Pre-trained Language Models\nThrutfulQATruthfulQA\nvLLMA high-throughput LLM serving engine\nWinoGrandeWinograd Schema Challenge",
  "4    Introduction\nArtificial Intelligence (AI) has revolutionized natural language processing (NLP) through\nthe  advent  of  Large  Language  Models  (LLMs),  which  demonstrate  exceptional  capabilities\nin  understanding  and  generating  human-like  language,  with  widespread  applications  across\ndiverse  industries.   However,  deploying  these  models  in  real-world,  regulated  environments\npresents substantial challenges.\nOrganizations like banks, hospitals, and government offices are likely to handle sensitive\ninformation that should not exit their premise under strict data privacy laws like GDPR and\nCCPA. Legal restrictions like these inhibit the use of AI models that are often hosted on external\nservers,  where there is limited transparency in data processing operations.   Furthermore,  the\ncomputational demands of LLMs make them prohibitively expensive for small and medium-\nsized enterprises (SMEs), which often lack access to high-performance hardware infrastructure.\nDespite advancements such as model qua",
  "ntization and the development of lightweight LLMs,\na major gap still remains in effectively adapting these models to specialized, domain-specific\ntasks under limited computational resources.  Methods like Retrieval-Augmented Generation\n(RAG) and Hypothetical Document Embedding (HyDE) have emerged as strong contenders\nin the sense that they can integrate external knowledge into the models with ease.  However,\nsuccess in such applications largely relies on the quality of query rewriting and retrieval.\nThis thesis focuses on retrieval-based question answering in such contrained domains, lever-\naging query rewriting to improve relevance and reduce computational overhead. This is achieved\nby  at  first  optimizing  Small  Language  Models  (SLMs)  through  advanced  quantization  tech-\nniques, which significantly reduce their computational and memory footprint. However, this ap-\nproach introduces a critical challenge, that is the degradation in model performance and knowl-\nedge retention.  To counter this effect, the system integrates a powerful Retrieval-Augmented-\nGeneration (RAG) framework.  This framework is not merely just add-on for external knowl-\nedge but it serves as a targeted mechanism to recover most of the performance lost during the\nquantization stage.  By leveraging instructions optimization, and Chain-of-Thought reasoning,\nthe RAG component ensures that the quantized SLM can access and effectively utilize precise,\nrelevant information from external document.\n4.1    Motivation\nArtificial Intelligence (AI) has made remarkable advancements in natural language process-\ning (NLP) through the development of large language models",
  "(LLMs).  Despite their capabili-\nties, significant challenges remain in deploying these models in highly regulated and resource-\nconstrained environments.  Organizations such as banks and public institutions face significant\nbarriers in adopting AI models due to strict data protection laws (eg., GDPR, CCPA) and high\ncomputational  requirements.   These  constraint  limit  their  ability  to  utilize  externally  hosted\nLLMs  or  fine-tune  large  models  for  domain-specific  tasks.   Additionally,  the  computational",
  "demands of  LLMs make them expensive  to deploy,  requiring  powerful  hardware infrastruc-\nture that is often unaffordable for small to medium enterprises (SMEs).  While advancements\nin quantization techniques and lightweight models have made LLMs more accessible, there is\nstill a gap in optimizing these models for domain-specific tasks without extensive computa-\ntional resources.  Retrieval-Augmented Generation (RAG) and HyDE methods have emerged\nas promising solutions, enabling models to integrate external knowledge efficiently.  However,\ntheir effectiveness depends on the quality of query rewriting and retrieval mechanisms.  This\nthesis seeks to address these challenges by evaluating lightweight, domain-aware strategies for\nquery rewriting within a RAG framework.  By leveraging techniques such as query augmen-\ntation, voting system, Retrieval Augmented Generation and Chain-of-Thought reasoning, the\ngoal is to improve retrieval relevance and performance in regulated and resource-constrained\nsettings.  This work will also explore the trade-offs between large quantized models and small\nnon-quantized models, provid",
  "ing practical insights for deploying AI systems in real-world ap-\nplications.\n4.2    Aims and Research Questions\nThis thesis aims to optimize small language models (SLMs), explore the trade-offs between\nquantized  and  non-quantized  models,  investigate  retrieval  methods  to  enhance  SLM  perfor-\nmance, and outline directions for future research. SLM optimization will be approached by ex-\nploring recent techniques identified throughout the course of this work, with the goal of making\nthese models more practical and resource-efficient. Analyzing the trade-offs between quantized\nand non-quantized models is an important step, as it will help determine whether future research\nshould focus on quantizing larger models or on further refining smaller ones.\nRetrieval methods are a good way to achieve better performance on SMLs on tasks that\nnormally would require training with more specific data-sets.  Selecting an effective retrieval\nstrategy is key to developing a lightweight, high-performing system.\nModel optimization may also include simple strategies such as query injection.  These ap-\nproaches will be evaluated to determine their usefulness and relevance to the overall objectives\nof the thesis.\nThis research aims to answer the following key questions:\nRQ1:  How can a system using smaller models still compete with larger ones in terms of\nperformance and efficiency?\nRQ2:Can HyDE be applied to a system designed for efficiency?\nRQ3:  What are the trade-offs among answer quality,  inference latency,  and energy con-\nsumption for each rewriting strategy?\nRQ4: Can an efficient approach still achieve high accuracy while remaining useful?",
  "4.3    Document Outline\n1.  Introduction\n•  Sets the stage by explaining the motivation, challenges, and research questions.\n2.  Background and State-of-the-Art\n•  Reviews existing techniques and it’s limitations.\n3.  Methodology\n•  Details the approaches used to optimize SLMs and enhance retrieval.\n4.  Evaluation Framework\n•  Explains how the methods are assessed using different metrics.\n5.  Results and Discussion\n•  Analyzes the outcomes and trade-offs of the proposed methods.\n6.  Conclusion and Future Work\n•  Summarizes the contributions and suggests future work.\n5    State-of-the-Art\n5.1    Transformers and Language Models\nNatural language processing (NLP) has been improving significantly over the last decades\ndue in part to the resurgence of deep neural networks (DNNs) [1].  The first sequential archi-\ntecture, RNN [2], had limitations regarding it’s ability to capture temporal dependencies.  This\nlimitation is related to the vanishing or exploding gradient, which results in the impossibility for\nRNN to retain information over longer sequences. The longer the sequence, the more the gradi-\nents would diminish to near zero or infinity, resulting in less relevant weight updates. LSTM [3]\nand GRU [4] are two sequential models developed to overcome this limitation. LSTM was the\nfirst to use an algorithm to consider gradient-based learning, which could bridge time intervals\nin excess of 1000 steps. This was achieved using memory cells and gating mechanisms (input,\nforget and output).  This allowed the networks to retain, update, or forget information in the\nmemory cell, avoiding the vanishing/exploding gradient. On the other hand GR",
  "U, doesn’t have\na separate memory cell; instead, it directly updates the hidden state using two gates: an update\ngate that combines the forget and input gates of the LSTM model and a reset gate that gives the\nnetwork the ability to control how much information it forgets.",
  "Table  2:  Maximum  path  lengths,  per-layer  complexity  and  minimum  number  of  sequential\noperations for different layer types. [5]\nFigure 1: Transformer model architecture [5]\nTransformer  models  introduced  many  changes  to  try  to  solve  all  of  the  problems  in  the\nprevious  models.   Starting  with  the  Self-attention  Mechanism  which  lowers  the  complexity\nfor short sequences.   This is due to the fact that the computational complexity is O(n\n∗ d)\nwhich is more efficient than Recurrent layers O(n∗ d\n) when the sequence n is smaller than\nthe dimensionality d.  When parallelization is used it requires only O(1) sequential operations\nbut on the recurrent layers they need O(n) sequential operations due to their sequential nature.\nDue to the short path length between any two positions in the input and output sequences the\nsignals can travel between all pairs of positions, this proved to be a big improvement for leaning\nlong-range dependencies. Moreover which can be useful to improve the global context since it\ncan attend to all positions in the sequence at once. Transformers self-attention can dynamically",
  "adjust based on the input sequence, this can also be modified to make the model focus on just\nthe local context.\nThe ability to interpret attention mechanisms is a significant advantage for understanding\nhow the model works. Papers suc",
  "h as [6] investigate what the model focuses on, which in turn\nprovides insights into its decision-making process.\nMulti-Head Attention this mechanism instead of performing a single attention function with\nd\nmodel\n-dimensional keys, values and queries they linearly project the queries, keys and values\nh times with different learned linear projections to d\nk\n(Queries),d\nk\n(Keys) and d\nv\n(V alues) di-\nmensions. The queries represent the search The way Multi-Head Attention computes attention\nindependently for each head allows the model to focus on different types of relationships and\nfeatures in parallel capturing more information, this also makes it possible to capture informa-\ntion that with a single head would normally be lost like complex relationships [7].\nMulti-Head Attention extends the standard attention mechanism by applying multiple atten-\ntion functions in parallel. Instead of performing a single attention operation with d\nmodel\ndimen-\nsional keys, values, and queries, the mechanism linearly projects the queries, keys, and values\nh times using different learned linear transformations into dimensions d\nk\n(Queries),d\nk\n(Keys)\nand d\nv\n(V alues).  These projections allow each head to compute attention independently, en-\nabling the model to focus on different types of relationships and features simultaneously.  This\nparallelization captures a richer set of dependencies and patterns, including complex relation-\nships that might otherwise be lost with a single attention head [7].\nBecause each head has smaller subspaces instead of one high-dimensional space this reduces\nthe computational complexity and improves the optimization proces",
  "s by scaling the dot products\nto prevent their values from becoming too large.\nIn natural languages,  word order plays a crucial role in conveying meaning and context.\nTherefore, it is essential for models to incorporate positional information.  Unlike recurrent or\nconvolutional models, the Transformer does not have an inherent mechanism to capture token\norder. To address this, positional encoding is introduced to inject information about the relative\nor absolute position of tokens within the input sequence. To make sure the the weight adapts to\nthe size of the query it uses the wave length of sin and cos that increases exponentially with the\ndimension of the index i, this grants that the positional encoding adjusts to compensate for any\ndimension length:\nPE(pos, 2i) = sin\n\u0012\npos\n2i\nd\nmodel\n\u0013\n[5]\nPE(pos, 2i + 1) = cos\n\u0012\npos\n2i\nd\nmodel\n\u0013\n[5]\nAs a benefit to how the model treats the positional encoding, it is able to adapt to sequences\nlonger than the ones found in training.\nFeed-Forward network consists of two linear transformations with a ReLU activation be-",
  "tween them:\nFFN(x) = max(0,xW\n+ b\n)W\n+ b\n[5]\nReLU or rectified linear unit is used to add non-linearity to the network allowing it to learn more\ncomplex functions.   The non linearity comes from the activation function max(0,x),  which\nwhen the input x is positive it passes through without any change,  on the other hand when\nthe input is negative it outputs 0.  With linear transformations being the same across different\npositions, they use different parameters from layer to layer, for example if the input and output\ndimensionality is set to 512 and the inne",
  "r-layer for 2048 this means that w\nwill transform from\n512 to 2048 and then w\nwill transform the 2048 into 512.  ”Another way of describing this is\nas two convolutions with kernel size 1.”[5].\nThe evolution of the Transformer models came with GPT and BERT [8].  GPT and BERT\nwere the first T-PTLMs developed based on transformer decoder and encoder layers respec-\ntively, these models where the basis for the discovery that performance of T-PTLMs could be\nincreased just by increasing the size of the model which triggered the development of mod-\nels like GPT-3 (175B)[9], PANGU- (200B)[10] and even a model with 1.6 trillions of tokens\nnamed Switch-Transformers [11].  Although performance is not strictly linear and depends on\nmany factors, the number of tokens plays a significant role. This was only made possible in part\ndue to the parallelization ability of the Transformer model.  Bert[12] differs from the original\nTransformer model due to its bidirectional architecture contrary to the original model which\nwas unidirectional.  Bert uses its bidirectional architecture (left-to-right, right-to-left) to access\ncontext from both directions simultaneously.\nAnother  important  feature  introduced  during  pretraining  is  Masked  Language  Modeling\n(MLM), in which 15% of the tokens in the input are randomly masked and the model learns to\npredict them using bidirectional context. Additionally, Next Sentence Prediction (NSP) is used\nto train the model to determine whether two sentences logically follow each other.\nAnother improvement to the model was the input representation,  BERT adds special to-\nkens [CLS] and [SEP], these are used by the m",
  "odel in conjunction with segment embeddings\nto handle sentence pairs, this allowed BERT to handle both single sentence and sentence pair\ntasks.  GPT differs significantly in its training approach by combining both supervised and un-\nsupervised learning. In the unsupervised phase, the model is trained on a large text corpus (e.g.,\nBooksCorpus) using standard language modeling. This is followed by a fine-tuning phase using\nsupervised learning on specific downstream tasks.  This two-stage process allows the model to\nfirst learn general language patterns and then adapt to more specialized tasks.  GPT also uses\nonly a Transformer decoder architecture, consisting of 12 layers of masked self-attention.  Be-\ncause it relies solely on a decoder, the model can only attend to previous tokens in the sequence,\nmaking it well-suited for auto-regressive language modeling. This model construction makes it\nsuitable to need minimal architecture changes when adapting to different tasks.",
  "5.2    Quantization\nQuantization is one of the most important techniques used to improve the performance and\nefficiency of large language models (LLMs), which are increasingly applied across a wide range\nof domains from customer service to scientific research.  However, as models grow in size and\ncomputational demands increase, a major challenge arises:  the hardware required to run these\nmodels becomes a limiting factor. High-performance hardware can be extremely expensive and\nenergy-intensive.   While recent advancements in hardware have enabled the development of\nmore powerful AI models, the cost and accessibility of such infrastructure remain significant\nbarrier",
  "s to widespread adoption. To give an example, Microsoft’s Phi-3-mini-4k-instruct model\n[13] requires 512 H100-80G GPUs to be run consecutively for 10 days with each costing around\nC30,000. Although such needs refer to training, which is done only once, running this kind of\nmodel still proves to be a computationally heavy task.\nThe most frequently used method to improve on this challenge is quantization, which re-\nduces the computational and memory requirements of the machine learning model. It achieves\nthis by converting the model weights and, in some cases, the activations from high-precision\n32FP to a lower precision representation such as INT8. This reduces the memory consumption\nof the model on the GPU and can accelerate computation because integer operations consume\nfewer resources compared to floating-point operations.\nThe two major methods of quantization will be discussed in more detail in subsequent sec-\ntions: Post-Training and Quantization-Aware Training.\n5.2.1    Post-Training Quantization (PTQ)\nTThis is the most commonly used quantization method because it does not require access\nto the model’s training process.  The quantization is applied after the model has already been\ntrained, making it especially useful when training resources or data are unavailable.  This type\nof quantization has been proven not to be as accurate at lower bit levels and there is a tendency\nof degradation if quantized to lower than 8-bits.[14] However since this method is less resource\nintensive, it has attracted more attention with a remarkable surge in post-training quantization\nmethods in the recent years.\nThe simplest approach is also the leas",
  "t efficient,  as it directly quantizes 16-bit values to\n8-bit using row-wise symmetric quantization.   While this method is straightforward,  it typi-\ncally results in only negligible degradation in perplexity. However, this breaks down with 4-bit\nquantization as it witnesses a significant drop in perplexity [15].  To improve the quantization\nperformance for low-bit applications, ZeroQuant-V2 [15] proposed Low-Rank Compensation\n(LoRC) method. This method approximates the error E between the original weight matrix W\nand the quantized weight matrix\nˆ\nW  using storage-efficient low-rank matrix\n ̃\nE so that\nˆ\nW +\n ̃\nE\nwould be a better approximation of the original weight W [16].\nLater research by LUT-GEMM[17] and SqueezeLLM[18] showed that non-uniform weight",
  "distributions could achieve even lower bit-width quantization.  This is due to the weight distri-\nbution after training being nonuniform so it makes sense for the weight distribution not being\nquantized uniformly.  This is done by allowing the quantization process to allocate more pre-\ncision to the ranges of weights that are more densely populated while leaving larger intervals\nfor less frequent weight ranges.  Building upon these methodologies OPTQ[19] emerged as an\nadvancement in quantization for big LLM’s.   Making it possible to run OPT-175B on just a\nsingle Nvidia A100 GPU or only two of the more cost-effective A6000.  OPTQ also provided\ngreater results in the extreme quantization regime where models were quantized all the way\ndown to 2 bits, or even ternary values. The OPTQ [20] algorithm improved on Arbitrary Order\nInsight, prior to this method the norm was to quantize t",
  "he weights in a greedy order [21] this\nmeans that the weight picked for the next quantization was picked based on minimum quan-\ntization error, this performs well but compared to arbitrary order quantization only offered a\nnegligible improvement in large, heavily-parameterized layers.  The likely reasons for the lack\nof improvement were that large individual quantization errors were balanced out overall, and\nthat these errors occurred later in the quantization process when fewer weights remained to\nbe quantized, leaving less opportunity for adjustment.  But with the OPTQ approach instead\nof quantizing the weights row-by-row, this method aimed to quantize the weights in all rows\nsimultaneously and in the same order.  This can be shown by how the unquantized weights F\nand the inverse layer hessian (H\n−1\nF\n) depend only on the input activations (X\nF\n) and not on the\nweights themselves, this proves that the quantization of a column affected all rows uniformly.\nColumns  within  blocks  are  quantized  recursively  and  at  each  step,  unquantized  weights  are\nupdated based on the quantized weights.\nFigure 2: RN18 squared error. [21]\nWith that it also achieved efficiency gains compared to OBQ which achieved O(d\nrow\n· d\ncol\n)\ncomparing  it  to  OPTQ  which  achieves O(max{d\nrow\n· d\ncol\n,d\ncol\n}),  reducing  it  by  a  factor  of\n{d\nrow\n,d\ncol\n}.  For larger models this can be proven to be more efficient into several orders of\nmagnitude. See Table 3 for details on runtime analysis.\nThe second step involves the use of lazy batch updates, which were introduced to improve\nupon the original Optimal Brain Quantization (OBQ) method. In the",
  "original approach, weights\nwere iteratively quantized, requiring updates to all elements of a potentially large matrix while",
  "ParameterValue\nd\nrow\nd\ncol\nRuntime TypeCalculation\nOBQ RuntimeO(10000· 100\n) = O(10, 000· 1, 000, 000) = O(10\n)\nOPTQ Runtime",
  ", 10\n}) = O(10\n)\nTable 3: Runtime Analysis\nusing only a few FLOPs per entry[21].   This means that the GPU usage was limited by the\nspeed of the memory bandwidth. Lazy Batch-Updates addressed this issue by grouping updates\nacross B × B  blocks of the inverse Hessian matrix H\n−1\n,  with B  being typically set to 128\ncolumns at a time. These blocks are processed and after that, they are used to apply updates to\nthe matrix.  Thus avoiding the need for frequent recalculations throughout the matrix, cutting\non memory-bound operations.\nThe final modification was the Cholesky Reformulation that was used to improve on two\nkey issues, numerical inaccuracies and error accumulation, the numerical inaccuracies occur as\nmodel sizes increase beyond a few billion parameters, these can lead to instability.  This hap-\npens because the matrix H\n−1\nF\nbecomes indefinite during iterative updates causing erratic weight\nupdates, the error accumulation is caused by the compounding numerical errors of the matrix\ninversion.  Though previously they used dampening techniques like adding a small constant λ\n(which was normally always 1% of the average diagonal value) to the diagonal elements of H .\nHowever, this proved to be inefficient on larger models, but by combining",
  "Cholesky decom-\nposition, precomputation of necessary rows, and dampening prevents the H\n−1\nF\nfrom becoming\nindefinite mitigating the accumulation of numerical errors, and makes the algorithm suitable\nfor large models.  A deeper dive into some of these methods are described in the subsequent\nsections.\n5.2.2    Weight-Only Quantization\nThe Weight-Only Quantization focuses on quantizing only the weights of the LLM and not\nthe activations, this reduces the model size and memory transfer time.  However, this doesn’t\nbenefit from hardware-accerlerated low-bit operations.[16] The most used quantization method",
  "is rounding to nearest-nember (RTN).[22], this works by quantizing a tensor x into k-bits.\nQ[x] = s× clamp\n\u0010\nx\ns\n,l\nmin\n,l\nmax\n\u0011\n[22]\nHere the s is the quantization scale, l\nmin\nand l\nmax\nare the low and upper bound clipping, which\nis then rounded to the nearest number⌊·⌋. Usually setting l\nmin\n=−2\nk−1\n+ 1 and l\nmax\n= 2\nk\n− 1\nand set s to be the maximum absolute value in x.  There are two main ways to find the best\nconfiguration in weight only quantization.  The first one is by minimizing the reconstruction\nerror of the weight parameter which is define as\nr(W) :=∥Q[W]− W∥\n[22]\nOn the function above only the weights are accessed therefore it’s a data-free process. However\nrecent studies ([19], [23]) propose the usage of output error as compensation.\ne(W) =\nX\nX∈D\n∥Q[W]X − WX∥\n[22]\nD corresponds to the calibration set sampled form of the original training data, for optimization.\nBy regularizing the model with its training data, more promising results are achieved com-\npared to the reconstruction-based method. The data requirement",
  "s of these 2 methods have a two\nbig draw backs, the first one being that there’s a requirement of having the original training data\nof the model since most quantizations are done by others than the creators off the model makes\nit hard to find exactly which data was used to train the model. The second problem is that using\nthe same data again can jeopardize the ability of generalization of the model due to the model\nover-fitting to the calibration set.  For this two reasons it is clearly very important to achieve a\nData-free quantization.\n5.2.3    Non Uniform Weight Quantization\nMost of the typical quantization methods handle the weights differently from this approach.\nThis method quantizes weights differently depending on their importance to the LLM. Not all\nweights  are  of  equal  importance  to  the  performance  of  a  model,  and  so  they  should  not  be\ntreated similarly, which is the case with techniques such as EasyQuant.\nEasyQuant[22] proposes the usage of the reconstruction error as the regulation metric since\nthis can be used to optimize the quantized model indirectly improving the generalization ability\nof the model. According to [22] the performance gap of the quantized model (INT4) and the full\nprecision model is due to two main factors. The first being that normally the quantization range\nis picked as the maximum absolute value of the weight thus inducing a large reconstruction error\nfor low-bits quantization.  The latter refers to the fact that the 0.1% of weights corresponds to\noutliers, although representing a small percentage of the total, have a significant impact on the",
  "ind, if we try to define the outliers using the condition:\n|W\ni,j\n− mean(W)|≥ n· var(W)    [22]\nFor  any  weight W ,  where W\nij\nis  the  (i,j)-th  weight  and  (n)  representing  the  threshold  for\nidentifying the outliers, we can classify certain weights as outliers [22]. However, the challenge\nis that simply detecting the outliers and avoiding their quantization is not sufficient to achieve\ngood model performance.   Furthermore,  if the percentage of outliers becomes too large,  the\noverhead introduced by the dequantization kernel increases, which can lead to a reduction in\noverall throughput.\nTable 4: PPL results on Wikitext2 of BLOOM-7B with and without outlier isolation. [22]\nEasyQuant also experimented with an ablation study focusing on three aspects, the outlier\ninfluence, outlier distribution, and the Quantization Range.  The ablation study began by pre-\nserving 10% of the weights in FP16. This resulted in an 8% increase in perplexity, compared to\nonly a 1% increase achieved with EasyQuant.  These findings suggest that simply isolating the\noutliers was not sufficient to maintain the expected perplexity levels. To check the outlier influ-\nence on EasyQUant, outlier isolation is key however this can only impose an indirect influence\non the model accuracy. The phenomenon found is the outliers behave like a gating mechanism\nmeaning that without the outlier isolation, the model performance deteriorates significantly with\nsmaller reconstruction error, and with outliers in FP16 the model shows continuous improve-\nment decreasing the perplexity with smaller reconstruction error Table 4.  Another study was\ndone to understand how muc",
  "h influence outliers with big weight magnitude and small weight\nmagnitude have on the model performance, this was done by pruning 1% of the values (accord-\ning to their magnitude) in the weights into 0 and see the perplexity results.\nTable 5: PPL results after pruning 1% weight with different magnitude [22]\nBased on Table 5 [22] shows that the largest magnitude outliers imposed the same influence\non the model performance as the normal values. This suggests that outliers exert a similar direct",
  "influence on model accuracy as regular weights, thereby indicating that isolating outliers has an\nimportant indirect impact on the overall performance of the model.\nTable 6: Outlier fraction distribution in different modules in BLOOM-7B under 3-sigma thresh-\nold [22]\nTable 7:  Outlier fraction distribution in different layer index in BLOOM-7B under 3-sigma\nthreshold [22]\nOn  the  outlier  distribution,  it  was  explored  the  distribution  along  different  modules  and\nlayers, and it showed that the fraction of the outliers share different patterns in different modules\nand layers, refer to Tables 6 and 7.\nAnother characteristic found was that the FFN.2 module showed a significantly higher frac-\ntion of outliers, however there was no pattern along the layer index.\nOn the quantization range, it was observed that the dynamic quantization range of different\noptimization steps and concluded that the range decreased fast in the early stages of training\nmeaning a smaller quantization range facilitating more precise quantization of parameters. The\nstudy also revealed that after a certain number of steps, the quantization range became stable\nmeanin",
  "g that the optimal range had already been achieved.  In deep neural networks, not all\nweights have the same influence on the model’s performance some contribute more significantly\nthan others [22]. This implies that relying solely on the magnitude of the weights is insufficient\nto fully capture the impact of each element on the model’s behavior.  A good benchmark to\ndetect parameter sensitivity is the Hessian metric.  This occurs due to the fact of the Hessian\nmatrix being leveraged to assess the salience of parameters in each under-binarized layer.  The\noptimized computation process to derive weight sensitivity is given by:\ns\ni\n=\nw\ni\n[H\n−1\n]\nii\n[24]",
  "The H  represents the Hessian matrix of each layer and the w\ni\nwhich represents the original\nvalue of each element. The s\ni\nis then used as a criterion for assessing the weight significance of\nthe element also used as a feature indicator for a structured selection.\nStructural search selection can be implemented using unstructured selection, allowing the\nmodel to cover all salient weights.  However, this approach requires an additional 1-bit bitmap\nindex  [25],  which  increases  the  average  bit-width.   This  proves  to  be  inefficient,  especially\nfor the Hessian outlier weights that are only less than 1% of the total.  According to [24] the\nmajority  of  the  weights  that  are  sensitive  Hessian  values  are  predominantly  concentrated  in\nspecific columns or rows.\nFigure 3:  The Hessian metrics (sensitivity) and magnitude (value) of weights in LLMs.  The\nweights of different layers in LLMs are characterized by bell-shaped distribution, accompanied\nby a few salient values",
  ".[24]\nThis pattern is due to the convergence effects inherent in multi-head self-attention mecha-\nnism of the models, thus needing a structured approach to select salient weights, reducing the\nadditional bit-map. The approach described in [24] is to employ a per-channel or per row type of\nbinarization, they determine salience through a per-column segmentation on the whole matrix.\nFigure 4:  Illustration of salient weight binarization.  The B1 binarized from salient weight is\nmade into a residual with the original value and then binarized again to obtain B2.[24]\nThe  main  idea  is  to  rank  the  columns  by  their  salience  in  descending  order  and  use  an\noptimized search algorithm to minimize quantization error. This process determines the optimal\nnumber of columns to include in the salient group. Based on this formula:\nW\nb\n= α· sign(W\nf\n),[24]",
  "where W\nb\ncorresponds to the binarized output, α denotes the scaling factor and W\nf\ndenotes the\nweights at full precision (FP16).  This was then used to define the objective of the binarization\nquantization, used in this equation:\narg min\nα,B\n∥W − αB∥\n,[24](1)\nwhere the B is the number of selected columns, α and B can simply be solved as α =\n∥W∥\nℓ\nn×k\nand B = sign(W). Then the optimization function to select salient columns is defined as:\narg min\nW\nuns\n∥W − (α\nsal\nsign(W\nsal\n)∪ α\nuns\nsign(W\nuns\n))∥\n,[24]\nwhere W\nsal\ndenotes the column-wise combination of the original weight and W\nuns\nis the\nleft non-salient part. W can be determined by W\nsal\n∪W\nuns\nso the only variable parameter is the\nnumber of rows in W\nsal\n.\nBinary Residual approximation is a technique use to address the ch",
  "allenge of preserving\nsalient weights which are limited in quantity, but exhibit significant variance when aggregated.\nIf  these  weights  are  preserved  at  their  original  formats  FP16  or  INT8  it  increased  the  aver-\nage weight bit-width, reducing the compression beneficts of binarization.  However traditional\nmethods of binarization result in a substantial quantization errors. Contrary to the comprehen-\nsive high-order quantization [26] which also applies quantization to the entire weight matrix,\nthe approach described in [24] uses a residual approximation method. This approach focuses on\nbinarizing only a subset of salient weights minimizing the error through a second-order aprox-\nimation.  This method grants the precision of salient weights while simultaneously decreasing\nbit-width overhead.  As shown in Figure 3 this approach incorporates a recursive computation\nstrategy for weight binarization compensation, applying a subsequent binarization process to\nthe residuals remaining after the initial binary process. Based on the equation 1 they redesigned\nthe residual approximation optimization specifically for salient weights by implementing:\n\n\n\n\n\nα\n∗\no\n,B\n∗\no\n= arg min\nα\no\n,B\no\n∥W − α\no\nB\no\n∥\n,\nα\n∗\nr\n,B\n∗\nr\n= arg min\nα\nr\n,B\nr\n∥(W − α\n∗\no\nB\n∗\no\n)− α\nr\nB\nr\n∥\n[24]\nthe B\no\nrepresents the original binary tensor, while B\nr\ndenotes the residual binarized matrix\nwith the same size as B\no\n.  Efficiently solving it for the two binarized optimization objectives\nresults on this approximation:\nW ≈ α\n∗\no\nB\n∗\no\n+ α\n∗\nr\nB\n∗\nr\n[24]\nTo prove that the residual approach of the equation above has a lower quantization error than",
  "direct one of 1. The residual binarization error was defined byE .\nE\nrb\n=∥W − α\n∗\no\nB\n∗\no\n− α\n∗\nr\nB\n∗\nr\n∥",
  "[24]\nThe original binarized quantization error is calculated as∥W − α\n∗\no\nB\n∗\no\n∥",
  ", and from the second\nsub-equation of equation 5.2.3 it’s determined that loss E\nrb\n≤ ∥W − α\n∗\no\nB\n∗\no\n∥\n.  Thus showing\nthat the method of residual approximation proved to be able to further reduce the binary quanti-\nzation error of salient weights with ultra-low bit-width storage compared to retaining the salient\nweights at their full precision or even INT8.\nThe performance of a LLaMA model with this super low quantization method is still impres-\nsive, only losing about 45% of the performance of the original model on a suit of benchmarks\nconsisting of PIQA, ARC-e, ARC-c, HellaSwag and WinoGrand, as depicted on Figure 8.\nTable 8: Quantized LLaMA3-8B performance[27]",
  "5.2.4    Weight + Activation Quantization\nAlthough weights are already extremely hard to quantize, incorporating the activations into\nthe quantization pipeline adds another degree of complexity, especially for large language mod-\nels. Compared to weights, activations have some unique challenges since they are dynamic, and\ntheir range and statistics are unknown until runtime.  The LLMs also have a unique problem,\nnot broadly seen for other transformer-based models: the systematic outlier activations.  These\noutliers if clipped during quantization can cause significant degradation in performance and\nrequire special attention if model accuracy is to be preserved [16].\n(a) The distributions of activations at the input to the\nFFN block in LLAMA2-7B model[28]\n(b)  Magnitude  of",
  "the  input  activations  and\nweights of a linear layer in OPT-13B[29]\nFigure 5: Comparison of activations and weights in LLAMA2-7B and OPT-13B models.\nAnother problem that makes it hard to quantize is the significant variations in value range\nacross different channels, which can be troublesome for the quatization algorithm. However, a\nstrong motivation for undertaking this complexity is the efficiency gained by quantizing both\nweights and activations to low-bit data types on specific hardware.  This efficiency is demon-\nstrated by the performance of SmoothQuant, as shown in Figure 8.\n5.2.5    Mixed Precision Quantization\nRPTQ, or Reorder-based Post-Training Quantization, differs from per-tensor quantization\ntechniques in that it does not apply the same quantization parameters uniformly across the entire\ntensor. This distinction is important, as uniform quantization can sometimes lead to suboptimal\nresults.\nOne key problem consists of the range of quantization being too wide to cover a large value\nrange, as this can cause increased quantization errors in channels with smaller value ranges. On",
  "the other hand if the quantization range is too narrow it could lead to truncation of the outliers\nresulting in quantization errors.\nFor example if one channel as a range of -100 to -50 and another 80 to 100 when trying to\ncover their ranges by quantizing them form -100 to 100 this will result in a significant quanti-\nzation error for both channels.\nTo address this researchers have proposed several methods one of them being LLM.int8 [30]\nwhich utilizes mixed-precision quantization by using high-precision data types (FP16) to quan-\ntize the o",
  "utliers in activations and low precision data types (INT8) for the remaining values. As\nexplained above this improves model performance preventing errors caused by the quantization\nof a wide range of values.  Another method for quantizing the activation SmoothQuant [29]\nsolved the problem by introducing a process that is meant to ”smooth” the input activation by\ndividing it by a per-channel smoothing factor s∈R, C\ni\n. To keep the mathematical equivalence\nof a linear layer the weights are scaled accordingly in the reversed direction:\nY = (X diag(s)\n−1\n)· (diag(s)W) =\nˆ\nX\nˆ\nW[29]\nThe next point of SmoothQuant is called Migrate Quantization Difficulty and the idea is to con-\ntrol the trade-off between the quantization difficulty of activations and weights by redistributing\ntheir values scale, meaning migrating the difficulty from activation to weights.\nThe idea works by choosing a per-channel smoothing factor s such that\nˆ\nX = X diag(s)\n−1\nso it’s easier to quantize.  To reduce quantization error, the effective quantization bits for all\nchannels should be increased.   This maximizes the total effective quantization bits when all\nchannels share the same maximum magnitude, making the optimal choice the scale factor s\nj\n=\nmax(|X\nj\n|), j = 1, 2,...,C\ni\nwhere j corresponds to the j-th input channel. This choice grants\nthat after the division, all the channels will have the same maximum value, which makes it easy\nto quantize.  However, this formula shifts all the quantization difficulty to the weights.  As a\nresult, the quantization errors tend to be larger in the weights, leading to significant accuracy\ndegradation shown in Figure 6.\nFigure",
  "6: Finding the sweet spot for the migration strength[29]\nHowever there is a possibility off also pushing all of the quantization difficulty from the",
  "weights to the activations by choosing s\nj\n=\nmax(|W\nj\n|)\n.  Similarly the model performance will\ndegrade heavily due to the activation quantization errors this introduces,  therefore there is a\nneed to split all of the quantization difficulty between weights and activations so they are both\neasier to quantize.\nSmoothQuant achieves this by introducing a hyper-parameter migration strength, depicted\nin Figure 6, to control how much difficulty will be migrated from activation to weights, this is\ndone using:\ns\nj\n=\nmax(|X\nj\n|)\nα\nmax(|W\nj\n|)\n1−α\n[29]\nFigure 7: Main idea of SmoothQuant when α is 0.5. The smoothing factor s is obtained on cal-\nibration samples and the entire transformation is performed offline. At runtime, the activations\nare smooth without scaling.[29]\nThis formula ensures that the weights and activations at the corresponding channel share a\nsimilar maximum value, thus sharing the same difficulty[29]. Figure 7 illustrates the smoothing\ntransformation when α = 0.5, this works for models where the activation outliers aren’t very\nsignificant, on models where the outliers are more significant (e.g. GLM-130B[31] which hap-\npens to have∼ 30% outliers), a larger α can be picked to migrate more quantization difficulty\nto the weights(like 0.7).\nThe performance results for this method are very promising on models like OPT-175B [32]\nshow an efficient quantization at INT8 quantization level since the method can match the FP16\naccuracy on all evaluation datasets. This also proved to be",
  "true for LLaMA [33]. Although the\noutliers in this model tend to be less severe, SmoothQuant still performed well, with an average\nperformance drop of only 0.4%. The PyTorch implementation also proved effective, achieving\na 1.51× speedup and a 1.96× memory reduction for OPT models.\n5.2.6    Quantization-Aware Training (QAT)\nLLM-QAT is an advanced method for Quantization-Aware training (QAT) specifically de-\nsigned for LLMs[14]. This method as been proven to be accurate to quantization levels as low as\n4-bits. This method helps in keeping the origianl output distribution and allows the quantization\nof weights, activations, and the key-value cache.  There are three core components, Symetric\nMinMax Quantization,  Student-Teacher Framework and Data Generation Process.   Symmet-",
  "ric MinMax Quantization is a method used to preserve the outliers in large language models\n(LLMs) and maintain their performance.\nX\ni\nQ\n=\n\u0014\nX\ni\nR\nα\n\u0015\n,  α =\nmax(|X\nR\n|)\nN−1\n− 1\n,[14]\nIn the function above the X\nQ\nrepresents the quantized values, X\nR\nrepresents the full preci-\nsion and the α is the scaling factor this is a general quantization formula and it can be applied\nto both weights and activations, but the method to quantize differs depending on the target.  In\nthe case of weights, per-channel quantization is used, meaning that at each channel (or filter)\nin the weight tensor it will be quantized independently.  This approach allows the quantization\nprocess to adapt accordingly to the specific range of values in every channel, preserving more\ninformation and reducing the possible quantization error. As for activations and the KV cache,\na per-token qua",
  "ntization is applied.  In this case, the quantization is performed independently\nfor each token, allowing the method to account for the diverse ranges of activation values across\nthe multiple tokens. This distinction ensures that the quantization process is purposely selected\nto the specific characteristics of weights, activations, and KV cache, optimizing the trade-off\nbetween efficiency and accuracy for each case.\nThen  LLM-QAT uses  the  student-teacher  model framework  to  ensure  that the  quantized\nmodel retains the performance of the full-precision model.  This works by having the teacher\nmodel the full-precision version guide the student, which is the newly quantized model.  The\nguidance is provided through cross-entropy-based logits distillation.\nL\nCE\n=−\nn\nX\nc\nn\nX\ni=1\np\nT\nc\n(X\ni\n) log(p\nS\nc\n(X\ni\n))    [14](2)\nOn Equation 2 the i represents the i-th sample in the batch , c denotes the number of classes\n(vocabulary  size),  and T  and S  are  the  Teacher  and  the  Student  models,  respectively.   The\nnext-token data generation is based on the full-precision model and is proposed as a method to\nsynthesize a distribution similar to the pre-training data.  This data is generated by the teacher\nmodel,  which  begins  with  a  random  start  token  and  iteratively  generates  subsequent  tokens\nuntil  it  reaches  the  end  of  the  sentence  or  the  maximum  sequence  length.   The  LLM-QAT\nintroduced a hybrid approach to ensure the generated data is diverse and accurate, on the hybrid\napproach  only  the  first  few  tokens  are  deterministically  selected  with  the  top-1  strategy  the\nrest are stochastically sampled",
  "from the full precision SoftMax output distribution.  Lastly, the\ngenerated  data  is  then  used  as  input  for  fine-tuning  the  quantized  model,  where  the  teacher\nmodel’s predictions serve as labels to guide training thus achieving a performance close to the\noriginal model yet quantized to a specified level.",
  "5.3    Retrieval-Augmented Generation (RAG)\nLarge language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [34] this is accomplished without access to any outside data [5] this comes with\nsome pros and cons.  On the pros side, there’s the ability of the model to capture a lot of data\nand compress it, on the other hand, this comes with the downside of not being able to expand\nthe model knowledge or even revise their memory. Another downside is the hallucinations that\nsometimes are produced by this models. These limitations can be addressed using a method pro-\nposed by [35], known as Retrieval-Augmented Generation (RAG). This method consists of four\nmain components:  a query encoder (q), a retriever (p\nn\n), a document indexer, and a generator\n(p\n).\nFigure 8: RAG implementation overview[35]\nThe first required model is a retriver DPR based on [36]. This retriver works by indexing all\nthe passages in a low-dimensional and continuous space, so that later the top K passages can\nbe retrived efficiently. At run-time, passages that are relevant to the input question are retrieved\nfor the reader module.  The number of passages from which the model can select is extremely\nlarge the paper refers to a corpus of 21 million passages, with the value of K (i.e., the number\nof retrieved passages",
  ") ranging between 20 and 100.\nDPR represented by p\nn\n(z|x) follows a bi-encoder architecture:\np\nη\n(z | x)∝ exp(d(z)⊤q(X)),  d(z) = BERT\nd\n(z),  q(x) = BERT\nq\n(x)    [35]\nWhere d(z)  is  a  dense  representation  of  a  document  produced  by  a BERT\nBASE\ndocument\nencoder [12] and q(x) a query representation produced by a query encoder, in this case also\nusing BERT\nBASE\n.   The  retrieval  is  done  using  (MIPS)  Maximum  Inner  Product  Search  to\ncalculate the top-k(p\nη\n(· | x)), which represents the list of k  documents z  with highest prior\nprobability p\nn\n(z|x).  MIPS identifies these documents by finding those with the largest inner\nproduct between their dense representations and the query representation.  This process can be\napproximately solved in sub-linear time using an efficient approximation to the nearest neighbor\nsearch, like FAISS or hierarchical navigable small-world graphs.  This is crucial for enabling\nthe scalability of large document collections such as Wikipedia.\nA pre-trained bi-encoder from DPR is used to initialize the retriever and to build the doc-",
  "ument index.  The Generator is used to combine the input x with the retrieved content z using\nBART, these are simply concatenated.\nThe generator component given by p (y\ni\n| x,z,y\n1:i−1\n) could be modeled using any encoder\ndecoder, however BART-large [37] a pre-trained seq2seq transformer with 400M parameters, is\ncommonly used.\nBART pre trainned using a denoising objective which served as its foundation.   For this\nspecific  use  case,  they  also  added  a  variety  of  different  noising  functions  in  the  training  to\nprevent BART from overfitting and also",
  "encourage contextual understanding [38].\nThey refer to the BART Generator parameters 0 as the parametric memory and all of the\nretrieved external knowledge as non-parametric knowledge.  The marginalization can be done\nvia two  methods described  in the  paper RAG-Sequence  model [39]  uses the  same retrieved\ndocument to generate the entire output sequence.  Specifically, it treats the retrieved document\nas a single latent variable, which is marginalized to obtain the sequence-to-sequence probability\np(y—x) using a top-K approximation.\np\nRAG-Sequence\n(y|x)≈\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)p\nθ\n(y|x,z)\n=\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)\nN\nY\ni=1\np\nθ\n(y\ni\n|x,z,y\n1:i−1\n)\n[35](3)\nThis marginalization allows the model to combine information from the topk  document,\neffectively converging the information from diverse sources within the same document to gen-\nerate a coherent and contextually accurate output. The final retrieval step ensures that the most\nrelevant documents are selected based on their dense representations,\nThe second method is RAG-Token model which can be used to draw a different latent doc-\nument for each target token and marginalize accordingly.  This makes it possible for the gen-\nerator to choose the content from several documents when producing the answer.  The top-K\ndocuments are retrieved using a retriever.  The generator then produces a probability distribu-\ntion for the next output token for each retrieved document. This process is repeated iteratively,\nmarginalizing over the documents at each step, to generate the subsequent output tokens:\np\nRAG-Token\n(y|x)≈\nN\nY\ni=1\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)p\nθ\n(y\ni\n|x,z\ni\n,y\n1:i−1\n)    [3",
  "5]\nIn the case for sequence classification tasks RAG-Sequence and RAG-Token can be used by\nconsidering the target class as a target sequence of length one.",
  "AspectRAG-TokenRAG-Sequence\nDocument UsageDifferent  documents  for  each  to-\nken.\nSame document for the entire se-\nquence.\nMarginalizationPer-token over top-K documents.Per-sequence   over   top-K   docu-\nments.\nBeam SearchStandard beam search.Beam search for each document.\nEfficiencyComputationally efficient.Thorough Decoding is expensive\nFlexibilityCombines information from multi-\nple documents dynamically.\nRelies on a single document for the\nentire sequence.\nTable 9: Comparison of Decoding Methods\n5.3.1    Hypothetical Document Embeddings (HyDE)\nHyDE [40] is another retrieval method that aims to increase the performance of the model\non zero-shot scenarios,  meaning that they can retrieve relevant documents without requiring\nspecific training.   This model is meant to work with any type of NLP model and is able to\ngeneralize across multiple tasks. This method differs from RAG [35] in that it uses a generator\nto produce a hypothetical document based on the input query.  This document does not need\nto be factually correct, as it is only used by the retriever (e.g., Contriever), which transforms\nit into a dense embedding vector.  This embedding represents the hypothetical document in a\nhigh-dimensional vector space.\nThe retriever is then used to search the corpus for real documents that are similar in the\nembedding space,  this similarity is measured using inner product similarity between the hy-\npothetical document embeddings and the real document embeddings.  Then the most",
  "similar\ndocument is fed into the model which also receives the input query, it then generates the output\nfor the query based on the retrieved document.\nFigure 9: An illustration of the HyDE model.[40]\nThe main issue addressed by [40] is the dependence on a separate query encoder required",
  "by RAG [35] systems.  This is because dense retrievers compute similarity between the query\nand documents using inner product similarity, which necessitates a dedicated query encoder.\nFirstly it uses two encoder enc\nq\nenc\nd\nthat maps the query q and the document d into d di-\nmension vectors v\nq\n,v\nd\n, whose inner product is used as the similarity measurement.\nsim(q,d) =⟨enc\nq\n(q), enc\nd\n(d)⟩ =⟨v\nq\n,v\nd\n⟩    [40]\nThis is where zero-shot dense retrieval problems lie, it requires learning two embedding func-\ntions one for the query and the other for the document, these need to align into the same em-\nbedding space where the inner product can capture the document’s relevance. HyDE solves this\nproblem by performing a search in the document-only embedding space that captures the doc-\nument’s similarity.  This method can be easily learned using unsupervised contrastive learning\n[41]. They set the enc\nd\ndirectly as the contrastive encoder enc\nc\non as follows:\nf = enc\nd\n= enc\nc\non    [40](4)\nThis  unsupervised  contrastive  encoder  is  be  shared  by  all  incoming  document  corpus.   The\nfunction 4 is also denoted as f .\nv\nd\n= f(d) ∀d∈ D\n∪ D\n∪···∪ D\nL\n[40]\nTo build the query vector they use an instruction following LLM, in this case,  text-davinci-\n003 from OpenAi’s GPT-3 series [42],  this was specifically picked due to its generalization\nability,",
  "they call it InstructLM so it’s easy to represent. It then takes the query q and a textual\ninstruction INST  and follows them to perform the task specified in INST , like so:\nTo build the query vector, they use an instruction-following LLM in this case, text-davinci-\n003 from OpenAI’s GPT-3 series[42]. This model was specifically chosen for its strong gener-\nalization capabilities and is referred to as InstructLM  for ease of representation.  It takes the\nquery q and a textual instruction INST , and follows the instruction to perform the specified\ntask, as shown below:\ng(q, INST) = InstructLM(q, INST)    [40]\nThe g can be used to map queries to the hypothetical documents by sampling from g, the INST\nis set to be ”write a paragraph that answers the question”, the generated document isn’t real and\nmay be factually incorrect due to models hallucinations [42].  However, this is not important\nbecause  the  hypothetical  document  is  used  only  to  capture  the  relevance  pattern.   Then  the\nrelevance modeling is offloaded to an NLG that has the ability to generalize more easily, nat-\nurally,  and more effectively.   Another great thing about generating the examples is that this\nalso replaces explicit modeling of relevance scores making it so there’s no need to compute the",
  "query-document relevance.\nE[v\nq\nij\n] = E[f(g(q\nij\n, INST\ni\n))]    [40](5)\nThe f  corresponds to the document encoder, g defines a probability distribution based on the\nchain  rule.   In  their  implementation,  they  assume  that  the  distribution  of v\nq\nij  is  uni-modal,\nimplying that the query is not ambiguous.  To estimate Equation 5, they sample N  docume",
  "nts\nfrom g,\nh\nˆ\nd\n,\nˆ\nd\n,...,\nˆ\nd\nN\ni\n.\nˆv\nq\nij\n=\nN\nX\nˆ\nd\nk\n∼g(q\nij\n,INST\ni\n)\nf(d\nk\n)\n=\nN\nN\nX\nk=1\nf(\nˆ\nd\nk\n)    [40]\nThey also consider the query as a possible hypothesis,\nˆv\nq\nij\n=\nN + 1\n\"\nN\nX\nk=1\nf(\nˆ\nd\nk\n) + f(q\nij\n)\n#\n[40]\nThe inner product is computed between ˆv\nqij\nand the set of all document vectors{f(d)|d∈ D\ni\n},\nthen the most similar documents are retrieved.  In their implementation, the encoder function\nf acts as a lossy compressor, producing dense vectors in which unnecessary details are filtered\nout.  This enables the system to use a generated document even if it lacks factual accuracy to\neffectively search for the correct one.\nAccording to their study,  HyDE remains competitive even when compared to fine-tuned\nmodels.  Another strong result comes from the web research setting, where the performance\nof  HyDE  is  particularly  impressive  even  when  compared  to  methods  that  rely  on  relevance\njudgments. Notably, HyDE achieves these results without requiring such judgments.\nTable  10:  Results  for  web  search  on  DL19/20.   Best  performing  w/o  relevance  and  overall\nsystem(s) are marked bold.  DPR, ANCE and ContrieverFT are in-domain supervised models\nthat are finetuned on MS MARCO training data. [40]",
  "5.3.2    Cache Augmented Generation\nRAG is the most used approach for enhancing language models but as discussed previously\nit has two main drawbacks, specifically the retrieval latency and the potential errors in document\nselection [43].  LLMs have been increasing their context size over the years and the CAG [43]\napproach proposes a method that uses this increased context size to reduce the model",
  "latency\nand potential errors that could occur on RAG systems.\nFigure 10: Comparison RAG on the top and CAG on the botom[43]\nThis  approach  works  by  enabling  retrieval-free  knowledge  integration.   This  is  done  by\npreloading external knowledge sources, such as a collection of documents D ={d\n,d\n,...,d\nn\n}\nand precomputing these documents key-value (KV) cache C\nK\nV ,  this addresses some of the\ncomputational challenges and inefficiencies inherent to real-time retrieval on RAG systems.\nThis approach is divided into three main steps:\nExternal Knowledge Preloading, represents the adaptation and preprocessing of the collec-\ntion of documents D that are relevant to the target application, this adapts them to fit within the\nmodel’s context window.  This is done by the LLM M , with parameters 0, and processed the\ndocuments D, thus transforming it into a precomputed KV cache:\nC\nK\nV = KV − ENCODE(D)    [43]\nThis KV  cache which contains the inference state of the LLM, is stored on disk or in memory\nfor future use.  This implementation brings some of the computational cost down because the\ncost of processing D is incurred only once even if used in multiple subsequent queries.\nDuring this stage the precomputed KV  cache C\nK\nV  is loaded alongside the user’s query Q,",
  "then the LLM uses this cached context to generate the responses:\nR = M(Q|C\nK\nV )    [43]\nBy giving the model the external cached knowledge it eliminates the retrieval latency and re-\nduces  the  risk  of  errors  or  omissions  that  comes  with  dynamic  retrieval.   The  prompt P =\nConcat(D,Q) ensures a unified understanding of the user query and the external knowle",
  "dge.\nCache Reset is the part of the system is responsible for maintaining performance across\nmultiple inference sessions. Since the KV  cache grows in an append-only manner sequentially\nstoring new tokens t\n,t, 2,...,t\nk\nthe context may eventually need to be freed.  This is achieved\nby truncating the oldest tokens to prevent the cache from exceeding memory limits.\nC\nreset\nKV\n= Truncate(C\nKV\n,t\n,t\n,...,t\nk\n)    [43]\nThis ensures that the re-initialization is fast without the need to reload it from disk, ensuring a\nconstant speed and responsiveness.\n5.3.3    Hybrid approaches\nThere are two main Hybrid approaches focusing on different implementations RAGCache\n[44] and Self-Route [45] though these can’t be compared directly in terms of their implemen-\ntation since RAGCache is a system-level optimization for RAG and Self-Route is an approach\nthat selects the optimal way to give context to the model.\nStarting with Self-Route, this method combines the benefits of Retrieval-Augmented Gen-\neration (RAG) notably its proven effectiveness and efficiency in leveraging external knowledge\nwith the capabilities of recent long-context (LC) models, which can directly process and un-\nderstand extended contexts.  Models like Gemini 1.5 Pro [46] is able to achieve near-perfect\nrecall of up to 1M tokens and maintains this recall performance of up to 10M tokens.  If this\ntrend of bigger and bigger context sizes continues this method could be a big improvement over\ntraditional RAG implementations.\nAlthough one problem with using long-context (LC) models is the increased computational\ncost, their performance can sometimes exceed that of RAG implementat",
  "ions.  However, RAG\nis significantly more efficient, with its cost estimated to be around 20% of that required by LC\nmodels.",
  "Figure 11: Comparison between performance and costs on multiple models using LC, RAG and\nSelf-Route[45]\nAs seen in Figure 11 the performance is maintained if not improved on some models but the\ncost on most cases is less than half. This is due to the nature of the approach, which combines\nthe strengths of both methods. When RAG can be applied, it offers reduced computational costs\nwhile maintaining most of the performance.  However, despite the performance gap, there is a\nhigh degree of overlap in the predictions made by both methods.\nFigure 12: Distribution of the difference of prediction scores between RAG and LC[45]\nFigure 12 shows the differences between RAG prediction scores S\nRAG\nand LC prediction\nscores S\nLC\nthese are not just similar, in 63% of queries the model predictions are exactly identi-\ncal, and for 70% of queries, the score difference is less than 10%. This is also true for incorrect\nanswers when looking at the red color which corresponds to an accuracy of 0 it can be seen that\nRAG and LC make similar errors as well.\nSelf-Route  uses  the  LLM  itself  to  route  queries  based  on  self-reflection,  under  the  idea\nthat LLMs are well-calibrated in predicting whether a query is answerable given the provided\ncontext, this is done using a two-step approach RAG-and-Route and long-context prediction.",
  "RAG-and-Route works by providing the original query along with the retrieved text chunks\nto the LLM, prompting it to assess whether the query is answerable based on the given context.\nIf the mo",
  "del determines that the query can be answered,  it proceeds to generate a response.\nHowever, if it deems the query unanswerable, it is instructed to return the phrase ”unanswer-\nable” as per the prompt:  ”Write ’unanswerable’ if the query cannot be answered based on the\nprovided text.” In such cases, a fallback approach is then triggered.\nThis approach consists of giving the LLM the full LC which consists of all documents able\nto fit the context , although this is not explained in the paper, it gives some hints that this must be\nthe case. This as seen on Figure11 giving a better result although with a higher cost. This trade-\noff is most cost-efficient when k = 5, meaning the number of retrieved documents is five. This\nis because, as k increases, the cost of RAG also increases, but so does the number of queries\nthat can be successfully routed.  Ultimately, the efficiency depends on the specific task being\nevaluated.  For instance, in extractive question answering tasks where multi-hop reasoning is\nnot required a lower k (e.g., k = 1) may result in lower computational cost.  Conversely, tasks\nthat require deeper reasoning may benefit from a higher k. Therefore, the optimal value of k is\ndependent on both the nature of the task and the level of performance required.\nFigure  13:  Trade-off  curves  between  (a)  model  performance  and  (b)  token  percentage  as  a\nfunction of k.[45]\nThe other hybrid approach RAGCache works by caching the key-value tensors of retrieved\ndocuments across multiple requests, this is done to try and minimize redundant computation for\nefficiency gains.",
  "Figure 14: RAGCache Overview[44]\nCache Structure and",
  "Replacement Policy operate differently from traditional cache systems\nthat cache individual objects.  Instead, this method caches the key-value tensor of the retrieved\ndocuments, which are sensitive to the order in which they are referenced. For instance consider\ntwo document sequences [D\n,D\n] with key-value tensors KV  and [D\n,D\n] with KV\n′\nrespec-\ntively.  Although KV [1] and KV\n′\n[1] both contain D\n, their value differs.  This occurs because\nthe key-value tensor of a given token being generated based on the preceding tokens, thus un-\nderscoring the order-dependence of key-value tensors. To aid retrieval speed while maintaining\ndocument order, RAGCache structures the document’s key-value tensors with a knowledge tree\n15.  The knowledge tree assigns each document to a node that refers to the memory addresses\nof the document’s key-value tensors. Similarly to vLLM [47], RAGCache also stores key-value\ntensors in non-contiguous memory blocks, allowing the KV cache to be reused.  The root of\nthe tree, S, corresponds to the shared system prompt.  A path from the root to any node rep-\nresents a unique sequence of documents, thus allowing RAGCache to handle multiple requests\nsimultaneously by leveraging overlapping paths.\nRAGCache retrieves tensors by performing prefix matching along these paths.  If a subse-\nquent document in a sequence cannot be found among the child nodes, the traversal is termi-\nnated, and the longest identified document sequence is returned. This method ensures efficiency\nwith a time complexity of O(h), where h is the height of the tree.\nFigure 15: Knowledge Tree[44]\nPrefix-aware Greedy-Dual-Size-Frequency (",
  "node placement optimizer.  The knowledge tree decides each node’s placement within a hier-\narchical  cache.   For  example,  nodes  that  are  constantly  accessed  are  ideally  stored  in  GPU\nmemory, while those that are accessed less often are stored in slower host memory or are freed\ncompletely. The node placement optimization occurs when RAGCache uses a PGDSF replace-\nment policy that evaluates each node based on the access frequency, size, and access cost, due\nto its limited storing capacity the priority is defined by:\nPriority = Clock +\nFrequency× Cost\nSize\n[44](6)\nNodes that have a lower priority get freed first while the opposite is also true.  The Clock\ntracks node access frequency, but to adapt to the cache hierarchy, there are two separate logical\nclocks: one for the GPU and another for the host memory. The Clock starts at zero and updates\nevery eviction, when a document is retrieved its clock is set and its priority gets adjusted this\nimposes that nodes with older clocks meaning less recent use, receive lower priorities.\nClock = max\nn∈E\nPriority(n)    [44]\nFrequency in Equation 6 represents the total retrieval count for a document within a time\nframe, this count is reset upon system start or cache clearance. Priority is directly linked to\nthe frequency so the more frequently a document is accessed the higher the priority. Size is the\nnumber of tokens in the given document post-tokenization, thus directly linked to the memory\nrequired for its key-value tensors.  The Cost is defined as the time taken to compute a docu-\nment’s key-value tensors, this can vary depending on G",
  "PU performance as well as document\nsize and the sequence of preceding documents.\nFigure 16: Cost estimation PGDSF[44]\nPrefix awareness for RAG is achieved by the PGDSF method through two primary com-\nponents, cost estimation and node placement.  Accurately determining the computational cost\nfor RAG operations is challenging due to the complex dynamics of LLM generation.  Figure\n16 illustrates this challenge by showing the cost differences for an identical request, denoted as\n[S,D\n,D\n,Q], under different caching conditions.\nEstimating the cost contribution of D\nis imprecise because the marginal cost depends heav-",
  "ily on the cache prefix.  For instance,  if the prefix [S,D\n] is already cached,  the subsequent\ncomputational cost includes the generation of key-value tensors for both D\nand Q.  Making\nit extremely difficult to isolate the cost attributed solely to D\n.  To address this issue, PGDSF\nreplaces the Cost/Size term in Formula 6 with the following Equation:\nCost Size =\nm\nm\nX\ni=1\nCost\ni\nNewSize\ni\n[44](7)\nIn Equation 7, m represents the number of requests that access a document not currently\nin cache.  The term Cost\ni\n/NewSize\ni\ndenotes the compute time per non-cached token for the\ni-th request.  This approach effectively amortizes the computational cost across all non-cached\ntokens, thereby incorporating the document’s size into the priority calculation. The cost, Cost\ni\n, is determined through an offline profiling process where RAGCache measures the LLM prefill\ntime for multiple combinations of cached and non cached token lengths.  Subsequently, it em-\nploys bilinear interpolation to estimate the cost for any giv",
  "en request at runtime. Each document\nretrieval event triggers an update to the corresponding node’s frequency, its cost estimation, and\nthe clock mechanism within the knowledge tree.  Furthermore, if a retrieved document is not\nalready in the cache, a new node is created for it in the tree.\nThe  management  of  the  node  placement  on  the  GPU,  host,  or  free  is  performed  by  the\nPGDSF as seen on Figure 15. The nodes in GPU serve as parent nodes to those in host memory,\nestablishing a hierarchical structure.  RAGCache also manages node eviction across these two\nsegments for efficiency, which is especially true when GPU memory is full. When this occurs,\nRAGCache swaps the lowest-priority node in the leaf nodes to the host memory; this process\nalso occurs on the host memory when it is full,  though in that case,  it is an eviction.   This\nstrategy takes into account the Knowledge tree hierarchical partitioning, which is one key point\nto align with memory sensitivity and prefix sensitivity in LLM generation.  Due to the node\nneeding its parent node for key-value tensor calculation, the required placement of the parent\nnode is prioritized this is so rapid retrieval can be achieved.\nBecause of PCIe limitations when connecting the GPU with the host memory in comparison\nwith GPU HBM, RAGCache adopts a swap-only-once strategy depicted in Figure 15, where\nyou can see that the key-value tensors of a node are swapped out to the host memory only for\nthe first eviction.  The host memory is responsible for keeping the key-value tensors until the\nnode is fully evicted from the entire cache.   For any subsequent evictions in GPU memory,\nRAGCa",
  "che directly frees the node node without copying any data Due to the size of the host\nmemory being two orders of magnitude larger than the GPU memory, keeping one copy of the\nkey-value tensors in the host memory is acceptable.",
  "Figure 17: Cache aware Reordering[44]\nCache hit rate is vital for RAG Cache’s cache efficiency, but when paired with the unpre-\ndictability of the arrival pattern in user requests, this results in substantial cache trashing.\nRequests that refer to the same document may not be issued on the same time frame thus af-\nfecting the cache efficiency. For example given the requests{Q\ni\n,i%2 == 0} and{Q\ni\n,i%2 ==\n1} that target the documents D\nand D\nrespectively.   When the cache capacity is only one\ndocument, the sequence{Q\n,Q\n,Q\n} causes frequent swapping of the key-value cache of D\nand D\n, making it a zero cache hit rate.  But if a bit of attention is paid to rearrange requests\nto{Q\n,Q\n,Q\n,Q\n,Q\n,Q\n,Q\n,...} this achieves a cache hit rate of 66% thus optimizing cache\nutilization. This shows how strategic request ordering can mitigate cache volatility and improve\ncache efficiency.  To introduce the cache-aware reordering algorithm, two scenarios were con-\nsidered to show the key insights, the recomputation cost was assumed to be proportional to the\nrecomputation length. The first scenario is shown on Figure 17, (a) where it considers requests\nwith identical recomputation demands but varying cached context lengths with a limit of four\ncached documents.  With the initial order of Q\n,Q\nthe system must clear Q\n’s cache space to\nfit Q\n’s computation, then reallocate memory for Q\n’s p",
  "rocessing effectively uses Q\n’s cache\nwhile discarding Q\n’s, resulting in a computational cost of 2 + 1 + 2 = 5.  On the other hand,\nif the order was given as Q\n,Q\nthis would result in a usage of Q\n’s cache but discarding Q\n’s,\nwhich would increase computation to six due to 2+2+2 = 6. This is why cache-aware reorder-\ning advocates to prioritize requests with larger cached context thus improving cache efficiency\nas this brings larger benefits. In the second scenario (b), the aim was to examine requests with\nsimilar cached context lengths but varying recomputation demands, with a cache capacity of\nfive documents. On a sequence{Q\n,Q\n}, the system must clear Q\n′\ns cache to allocate space for\nQ\n′\ns computation, given only one available memory slot. This makes it necessary to recompute\nQ\nentirely, which results in a cost of 2 + 2 + 1 = 5. On the other hand the sequence{Q\n,Q\n}\nallows for direct computation of Q\n, due to adequate cache availability. It also reduces the total\ncomputation cost to 2 + 1 = 3, thus the reason why cache-aware reordering is beneficial when\nit prioritizes requests with shorter recomputation segments, this way results in a minimization\nof the adverse side effects on cache efficiency.  RAGCache uses a priority queue for manag-\ning incoming requests, this prioritizes the incoming requests based on their impact on cache\nperformance, the priority metric is defined by:\nOrderPriority =\nCached Length\nComputation Length\n[44](8)",
  "Equation 8, directly prioritizes the requests that will probably lead to enhanced cache ef-\nficiency.   This is directly linked to the increase in the cache hit rate an",
  "d the decreased total\ncomputation time of RAGCache.  Model performance and resource usage are also improved\nthanks to this implementation. To avoid possible starvation when requests don’t align with the\ncached documents RAGCache sets a window for each request to ensure that all requests are\nprocessed in a timely manner.\nOn an LLM enhanced with RAG, the key performance bottleneck is usually the LLM gen-\neration, however, if the vector database grows to a larger scale or a higher accuracy is needed in\nthe retrieval, this may cause the retrieval step to incur a substantial latency.\nTo control the impact of retrieval latency, RAGCache employs dynamic speculative pipelin-\ning to overlap knowledge retrieval and LLM inference, and one thing that can occur is that the\nvector search may produce results earlier in the retrieval step, which can be used by the LLM\nfor speculative generation ahead of time.  This works by a vector search maintaining a queue\nof top-k candidate documents, which are ranked based on their similarity to the request.  Dur-\ning the retrieval process the top-k documents are being constantly updated this is done so that\ndocuments that still being discovered, might come with greater similarity and so they get in-\nserted into the top-k. What could also occur is that the final documents may emerge early in the\nretrieval step [48].  Based on that RAGCache introduced a speculative pipelining strategy that\nsplits a request’s retrieval process into several stages. In each stage RAGCache ticks the vector\ndatabase to send the possible document to the LLM for a speculative generation, if the received\ndocument is changed then the LLM",
  "will start a new speculative generation and terminate the\nprevious one, if that doesn’t happen then the LLM engine just continues with the generation.\nWhen the top-k documents are finalized and there are no more changes to the top-k these are\nsent by RAGCache to the LLM engine and if they match with the ones previously received the\nengine simply returns the latest speculative generation.  Otherwise, the LLM performs a new\ngeneration with the new top-k documents.\nFigure 18: Speculative Pipelining[44]",
  "documents in candidate queue are [D\n,D\n], [D\n,D\n], [D\n,D\n] and [D\n,D\n] in the four stages.\nAfter the first stage is concluded, RAGCache sends [D\n,D\n] to the the LLM engine for specu-\nlative generation. When stage two is concluded, RAGCache sends [D\n,D\n] to the LLM engine,\nthe LLM engine is responsible for checking if the [D\n,D\n] and [D\n,D\n] are different if that’s",
  "the case it terminates the previous speculative generation and starts a new one with the correct\ndocuments. In stage three, the LLM engine receives the exact same documents as for stage two,\nso it continues with the previously started speculative generation. In the last stage, RAGCache\nsends the final top-2 documents to the engine which are still the exact same as the ones in stage\ntwo so there is no change to the speculative generation which is directly returned by the LLM\nengine as the result.\nThe speculative pipelining allows RAGCache to overlap the retrieval and generation steps,\nand this greatly improves the end-to-end latency of RAG systems.   But th",
  {
    "type": "image",
    "src": "/images/methodology/page_64_img_1.png",
    "alt": "Page 64 Image - Figure\n18, some speculative generations are incorrect and need to be recalculated.  This can lead to",
    "caption": "Figure\n18, some speculative generations are incorrect and need to be recalculated.  This can lead to"
  },
  "is can introduce a\nlot of extra steps in the computation of the engine response.  This can be seen above Figure\n18, some speculative generations are incorrect and need to be recalculated.  This can lead to\nperformance degradation under high system loads,  but to solve this RAGCache dynamically\nenables speculative pipelining based on the system load. As an example, they assumed that the\nvector search and the LLM both serve only one request at a time.  This vector search produces\ncandidate retrieval results at the end of each stage with a fixed time interval d.  Since the batch\nsize was set to only one, they could terminate any incorrect speculative generation requests.\nFigure 19: Optimal speculative pipelining strategy [44]\nRAGCache assumes that the LLM engine can schedule requests in the queue in any order,\nbut  it  processes  speculative  generation  requests  for  a  single  request  sequentially.   Figure  19\nillustrates the optimal speculative pipelining strategy under this setting.\n5.4    Challenges and Applications of Quantization in RAG\n5.5    Retrieval Methods to enhance HyDE\nDue to the approach taken by RAG and HyDE on how they retrieve documents, these meth-\nods could be adapted or specifically chosen based on their ability in certain tasks.  Retrievers\nare the basis for content that was retrieved from an external document corpus to enhance the\nLLM output, as well as provide grounds for the generated information on accurate documents.\nA more in-depth research will be done about some of these retrievers.",
  "5.5.1    Contriever\nThe original implementation of HyDE uses the Contriever model as its retriever.  This ap-\nproach w",
  {
    "type": "image",
    "src": "/images/methodology/page_65_img_1.png",
    "alt": "Page 65 Image - Page 65 Image",
    "caption": "Page 65 Image"
  },
  "orks on the basis of contrastive learning, which is based on the fact that every document\nis, in some way, unique. According to [49], this is the only available information in the absence\nof manual supervision. A contrastive loss is used to learn by discriminating between documents.\nThis loss compares either a positive loss when they are the same document or negative when\nit’s from different documents. The formula responsible for this is:\nL(q,k\n+\n) =−\nexp (s(q,k\n+\n)/τ)\nexp\n\u0010\ns(q,k\n+\n)\nτ\n\u0011\n+\nP\nK\ni=1\nexp\n\u0010\ns(q,k\ni\n)\nτ\n\u0011\n′\n[49](9)\nIn Equation 9, q corresponds to the given query, which has an associated positive document\nk+, and a pool of negative documents(k\ni\n)\ni=1..k\n, τ  is the temperature parameter used to adjust\nthe sensitivity of the Contriver.  This function’s construction encourages positive pairs to have\nhigh scores and negative pairs to have low scores.  One crucial piece of this method is how to\nbuild positive pairs from a single input. This could be done in two main ways: the Inverse Cloze\nTask or Independent cropping.\nThe  usage  of  the  Inverse  Cloze  Task  is  a  data  augmentation  strategy  that  generates  two\nmutually exclusive views of a document.  This approach was first described in [50].  The first\nview is obtained by randomly sampling a span of tokens from a segment of text, and the second\nview is obtained by using the complement of the span.  This is done by in a given sequence of\ntext (w\n,...,w\nn\n), ICT samples a span (w\na\n,...,w\nb\n), where 1 ≤ a ≤ b ≤ n, and then uses the\ntokens of the span as the query and the complement (w\n,...,w\na−1\n,w\nb+1\n,...,w\nn\n) as the key.\nThe usage of the Independent is critica",
  "l for matching the query with the document directly.\nThis method is commonly used on images where multiple views are generated independently\nby cropping the input. Since this implementation is used for text it is done by sampling a span\nof tokens.  Because of the importance of the positive pairs this strategy samples independently\ntwo spans from a document to form the needed pair.  Contrary to the inverse Cloze task in the\ncropping stage both views of the example correspond to the same contiguous subsequence of the\noriginal data. Another difference is that between cropping and ICT is that independent random\ncropping is symmetric meaning both of the queries and documents follow the same distribution.\nThis also causes overlap between the two views of the data, this being one of the reasons that\nencourages the network to learn exact matches between query and document. This works very\nsimilar to how lexical matching methods function, BM25 being a great example of this. So you\ncould either fix the length of the span for the query and key or sample them both.\nA big part of contrastive learning is how the system handles negative pairs this includes\nsampling a large set of negatives. This was tested by [49] using two methods, in-batch negative\nsampling and MoCo.\nThe first approach is to generate the negatives by using the other samples from the same",
  "batch.  For example, each item in the batch is transformed twice to generate the positive pairs,\nand the negatives are generated by using the other examples views from the batch, they called\nthese ”in-batch negatives”.  In this specific case, the gradient is back-propagated through the\nrepres",
  "entations  of  both  the  queries  and  the  keys.   The  main  downside  to  this  method  is  the\nrequirement for extremely large batch sizes to work well.\nThe other approach Negative pairs across batches tries to solve the problem by storing the\nrepresentations from previous batches in a queue and using these as negative examples in the\nloss calculation.  This makes it possible to have a smaller batch size but may slightly change\nthe loss by making it asymmetric between the queries.   This being the view generated from\nthe elements of the current batch, and the keys, which are the elements already stored in the\nqueue.  This occurs as the gradient is only back-propagated through the queries,  leaving the\nrepresentation of the keys fixed. This is caused by the features being already stored in the queue\nfrom prior batches coming from past interactions with the network. A problem occurs when the\nnetwork is rapidly evolving during training can cause the performance to drop.\nInstead, they used the approach called MoCo [51] which generates representation keys from\na second network that is updated more slowly, the two networks are as follows one is responsible\nfor the keys, parametrized by 0\nk\n, and another network for the query, parametrized by 0\nq\n.  The\nparameter for the query network gets updated using back-propagation and stochastic gradient\ndescent.   This works similarly to when in-batch negatives are used.   On the other hand,  the\nkey network also called Momentum encoder is only updated from the parameters of the query\nnetwork using an exponential moving average.\n5.6    Self-Knowledge Guided Retrieval Augmentation\nThis approach u",
  "ses the few-shot prompts to make the LLM judge if it knows the answer or\nnot.  In the case that the answer isn’t known, SKR [52] proceeds to the retrieval using RAG to\nimprove on the model response.\nTheir method works by three main ways: Collecting Self-Knowledge, Eliciting Self-knowledge,\nand Using Self-Knowledge. These represent the main pipeline for SKR, as shown in Figure 20.\nFigure 20: The SKR Pipeline and its component interactions.[52]",
  "5.6.1    Collecting Self-Knowledge\nGiven a dataset D with question-answer pairs{q\nj\n,a\nj\n}\n|D|\nj=1\n, the model M is used to generate\nthe answers for each entry q\ni\n:\nˆa(M,q\ni\n) =M(q\n◦ a\n,...,q\nd\n◦ a\nd\n,q\ni\n)    [52](10)\nWhere ◦ denotes the concatenation and {q\nj\n◦ a\nj\n}\nd\nj=1\nare d demonstrations.   Equation 10\nrepresents the generated answer with ˆa(M,q\ni\n), this also represents the internal knowledge to the\nquestion q\ni\nin M .  The other approach is to possibly find passages from external resources that\nmay be related with said question q\ni\n, these passages can then be used as additional information\nprovided on the model input. This is done per query, using a pretrained retriever represented by\nR to find the related information from the corpus C:\np\ni\n={p\ni1\n,p\ni2\n,...,p\nik\n} =R(q\ni\n,C),[52](11)\nAccording to Equation 11 the top-k retrieved passages for the question q\ni\nare represented\nby p\ni\n= {p\ni1\n,p\ni2\n,...,p\nik\n}.  A dense passage retriever is used [36] for R, and C  consists of\npassage chunks from Wikipedia.  Then they use M  again to generate the answer with retrieval\naugmentation:\nˆa\nR\n(M,q\ni\n) =M(q\n◦ p\n◦ a\n,...,q\nd\n◦ p\nd\n◦ a\nd\n,q\ni\n◦ p\ni\n).    [52](12)\nBased on both answer",
  "s ˆa(M,q\ni\n), ˆa\nR\n(M,q\ni\n) 12, and the ground-truth answer a\ni\n, they can cat-\negorize each question into a positive subset D\n+\nand a negative sub-set D\n−\nusing the differences\nbetween the results:\nq\ni\n∈\n\n\n\nD\n+\n,    if E[ˆa(M,q\ni\n)]≥ E[ˆa\nR\n(M,q\ni\n)];\nD\n−\n,    otherwise,\n[52]\nIn Equation 5.6.1 E is an evaluation metric like accuracy and exact match score, but they\nget discarded if the question q\ni\n, answer ˆa(M,q\ni\n) and ˆa\nR\n(M,q\ni\n) are incorrect.  They then split\nthe training set into a subset D\n+\n= {q\n+\ni\n,...,q\n+\nm\n} which include questions that M  can directly\ngive correct answers to without external knowledge R and the other subset D\n−\n={q\n−\n,...,q\n−\nn\n}\nwhere the R is needed for more accurate results.\n5.6.2    Eliciting Self-Knowledge of LLMs\nThe four different strategies proposed to detect the self knowledge of the target questions\nare direct prompting, in-context learning, training classifier, and nearest neighbor search. These\nwork by on the first two using the LLM itself and the latter two using smaller nodes purposely\nbuilt.",
  "Direct Prompting given a question q\nt\n, a straight-forward approach to detect wether LLMs\nare capable of solving it is to ask them directly:\nFigure 21: Direct Prompting [52]\nOn this method the prompt is used in conjunction with ’Do you need additional information\nto answer this question?’ to detect self-knowledge based on the response provided by the LLM.\nThis approach results in direct prompting the model and it may work.  Although this doesn’t\nuse any of the collected training questions shown previously.  To improve on that 3 different\nstrategies where created.\nIn-Context Learning some qu",
  {
    "type": "image",
    "src": "/images/methodology/page_70_img_1.png",
    "alt": "Page 70 Image - Figure 22: In-Context Learning [52]",
    "caption": "Figure 22: In-Context Learning [52]"
  },
  "estions where selected from D\n+\nand D\n−\nas demonstrations to\nshow the self-knowledge of the question q\nt\n:\nFigure 22: In-Context Learning [52]",
  "In here answer templates where used, ”No, I don’t need...” or ”Yes, I need...” in demonstra-\ntions based on wether the answer comes from the positive set D\n+\nor the negative one D\n−\n.\nThis direct prompting and in-context learning methods can induce self-knowledge of LLMs\nto some extent.  But they come with three main drawbacks.  First one being that both methods\nrequire designing prompts and calling the LLMs for each new question, making it cumbersome.\nSecond, in-context learning could be also unstable due to contextual bias and sensitivity which\nis difficult to address in closed source LLMs. Third, use of all questions cannot be guaranteed,\ndue to the maximum tokens input of the LLMs. To avoid the above issues smaller models were\nused to help elicit self-knowledge.\nA classifier was trained using D\n+\nand D\n−\n, as a two-way classification problem using the\nsamples to train a BERT\nBase\nclassifier [12]:\nˆy\ni\n= softmax(Wh\ncls\n(q\ni\n) + b),(13)\nWhere q\ni\n∈ D\n+\n∪D\n−\nis a training question, h\ncls\n(q\ni\n) is the sentence-level representation\nfrom BERT\nBase\n, W  and b are parameters used by the classification head.  The parameters can\nbe optimized to improve the cross-entropy loss between the predicted label distribution ˆy\ni\nand\nthe ground-truth label of q\ni\n. Latter the training model can also be used to infer the label of new\nquestion q\nt\ndescribed in equation 13.\nThe other method also tested was Nearest Neighbor Search this method doesn’t require\ntraining since the inference can be directly done",
  "based on the label of the questions through\nk-nearest-neighbor (kNN) search using a pre-trained fixed encoder as showed by Figure 23.\nThe kNN [53] is an algorithm widely used for a range of NLP tasks. This idea comes from\nthe similarity between the semantically embedded space of two questions if these are closely\nrelated then the knowledge needed for the model to answer would also be similar.\nFigure 23: k-nearest-neighbor to understand model knowledge [52]\nEach question was encoded into embeddings, and computed the semantic similarity through\ncosine distance sim(q\nt\n,q\ni\n) = (\ne(q\nt\n)·e(q\ni\n)\n||e(q\nt\n)||·||e(q\ni\n)||\n), where q\ni\n∈ {q\n+\n,...,q\n+\nm\n,q\n−\n,...,q\n−\nn\n}, e(·) being\nthe representation of a sentence encoder.  Then the search for the top-k nearest neighbors can\nbe done based on the results that include the l positive ones and k− l negative ones.  This can",
  "be used to label the question q\nt\nas positive if\nl\nk−l\n≥\nm\nn\nor negative if\nl\nk−l\n<\nm\nn\nm and n are the\nnumber of questions from D\n+\nand D\n−\nrespectively.\n5.6.3    Using Self-Knowledge for Adaptive Retrieval Augmentation\nThe self knowledge acquired previously from the LLMs responses or the predicted labels\nreflect the necessity or not of retrieving external knowledge, for each question q\nt\naccordingly.\nSo that retrieval can be done or not depending on these results.\n5.7    Model Comparison\nThe model comparison will be used to help pick the correct model based on the require-\nments, these could be based on VRAM, performance or open-sourceness. A well-known leader-\nboard for LLM performance is llm.extratum.io [54].   This leader-board can be sorted based\non  many  char",
  {
    "type": "image",
    "src": "/images/methodology/page_72_img_1.png",
    "alt": "Page 72 Image - Page 72 Image",
    "caption": "Page 72 Image"
  },
  "acteristics  like  VRAM  usage,  quantization  level,  model  size,  and  many  more.\nLlm.extratum.io also sorts based on performance that is measured on key evaluation data sets\nand metrics like GPQA, MUSR, BBH, IFEval, ARC, HellaSwag, MMLU, ThrutfulQA, Wino-\nGrande, GSM8K, MATH Lvl5 and MMLU Pro. These are some of the best methods to quantize\na model’s performance on multiple aspects and will be discussed further in the next sections.\n5.7.1    MMLU\nThe  MMLU  [55]  or  Massive  Multitask  Language  Understanding  is  an  LLM  benchmark\nthat consists of a dataset designed to be a comprehensive test of the model’s ability to respond\ncorrectly on a diverse range of tasks and topics.  It includes 57 different subjects that include\nknowledge across many disciplines like humanities, STEM, social sciences, and professional\nfields.   The  questions  in  the  benchmark  are  multiple-choice  of  four  options  there  are  over\n15.000 questions ranging from simple elementary math questions all the way to professional\nmedicine making it a very diverse benchmark. Some of the more important features include the\nstandardized evaluation metrics, calibrated difficulty levels, comprehensive coverage of human\nknowledge, and professional-level expertise requirements.  All of these points transform this\nbenchmark into a very important benchmark in the field due to its variety and complexity. This\ntest has however become easy for the most recent models like LLaMA 3 70B which achieved\n80.06 out of 100 [54]. To counteract the new advancements an improved version of the MMLU\nwas developed, MMLU-Pro[56].\n5.7.2    MMLU-Pro\nMMLU-Pro[56] is a more robust and",
  {
    "type": "image",
    "src": "/images/methodology/page_73_img_1.png",
    "alt": "Page 73 Image - Page 73 Image",
    "caption": "Page 73 Image"
  },
  {
    "type": "image",
    "src": "/images/methodology/page_73_img_2.png",
    "alt": "Page 73 Image - Page 73 Image",
    "caption": "Page 73 Image"
  },
  "challenging multi-task language understanding bench-\nmark. This was achieved by increasing the complexity of the options expanding from 4 options\nto 10 thus reducing the probability of guessing the correct answer by chance this also made the",
  "benchmark more challenging and more discriminative. This was done using GPT4-Turbo to in-\ntroduce six additional choices. These are created with the intuition of being plausible distractors\nthat need discerning reasoning to pick the correct answer.  The questions were also improved\nadding to the quality of the benchmark by eliminating trivial and noisy questions from the orig-\ninal MMLU. Which contained some that were found to be too easy by using a list of small\nLLMs when more than four were able to answer the question this question was then removed.\nThe benchmark was also improved by increasing the portion of more challenging questions by\nadding a bigger share of college-level exam problems.  All of these changes were then verified\nby two rounds of expert reviews to reduce the dataset noise. This proved to make the benchmark\nmore robust making it less sensitive to prompt variation changing from 4%− 5% to just 2%.\nThis came with the benefit of generating a more stable performance across different prompt\nstyles showing a greater consistency in model evaluation.  These improvements achieved a big\nimprovement in the discrimination between results of different models that previously scored\nsimilarly, the prior gap between GPT-4o and GPT-4-Turbo was 1% with MMLU-Pro it’s 9%.\n5.7.3    GPQA\nGPQA  [57]  this  benchmark  differs  a  lot  from  others  since  this  benchmark  was  made  to\nevaluate an",
  "LLM’s ability to respond to 448 multiple-choice questions.  Which were created\nby domain experts in biology, physics, and chemistry.  These questions were tested on experts\npursuing PhDs in the corresponding domain and still only reached at best 74% when discounting\nthe clear mistakes the experts had identified in retrospect.  One good point to mention is that\nthis was also tested on what the paper describes as highly skilled non-expert validators where\nthe accuracy was only 34%.  This was done with an average of 30 minutes per question and\naccess to the web to research the topic.  This dataset questions were first written by a domain\nexpert, this was then answered by another domain expert who would give constructive feedback\nto improve the clarity of the question and would also suggest revisions if needed.  After said\nrevision by the writer of the question, it is sent to another different domain expert and three\nnon-expert validators who are experts in other domains to access the quality of the query and\nvarious options of answer.\nThis method made sure that the questions are proven and tested by at least two different\ndomain experts and three other area experts.  GPQA is divided into 3 subsets Extended, Main\nset, and Diamond. The extended subset contains all of the validated questions with 546 different\nquestions.\nGPQA is the name of the main not containing non-objective questions, these being those that\nboth domain expert validators got wrong yet the three non-expert validators got right. This set is\ncomposed of 448 questions, these are also composed of questions that where one of the domain\nexpert validators answered incorrectly",
  {
    "type": "image",
    "src": "/images/methodology/page_75_img_1.png",
    "alt": "Page 75 Image - Page 75 Image",
    "caption": "Page 75 Image"
  },
  "but agreed that they made a clear mistake after being\nshown the solution. The strictest set is the Diamond where only 198 questions are present, these\nare the highest quality and thus only include questions where both experts answered correctly",
  "and the majority of non-experts answered incorrectly.  Though this also includes the questions\nwhere the second domain expert validator got the answer wrong yet explains his mistake once\nshown the correct one similarly to the previous set, but in this case, the first domain expert must\nanswer correctly.\nThis  benchmark  proved  very  efficient  at  achieving  the  proposed  results  since  even  with\ninternet access GPT-4 only achieved 39.4% which was an increase of just 0.4% from the original\nscore without internet access.  At that point in time when this benchmark was launched GPT-4\nwas the state of the art with older models like GPT-3.5 only achieving 28% check Table 11 for\nmore tests.\nTable 11: Accuracy on each set [57]\n5.7.4    MUSR\nThis benchmark was developed to evaluate LLMs on multistep soft reasoning tasks specific\nto the natural language narrative [58].  This is done by three main domains murder mysteries,\nobject placements, and team allocation. All of these are synthetic meaning, these were produced\nby an LLM. The creators choose to use synthetic stories due to two main reasons scalability and\nthe ability to regenerate the story due to possible data leaks.  The scalability is important since\nmore capable LLMs are being created every year,the dataset could be adapted as needed to\nbecome more complex and add longer narratives thus introducing more difficult access for the\nLLMs.",
  "The ability to regenerate the story is also very important since a data leak could mean\nthat the LLMs could be trained with the data of the benchmark which would take away the\nzero-shoot aspect of this benchmark.",
  "Table 12: LLMs using CoT+, Humans scores on the multiple domains[58]\nThis data set was constructed using an LLM that is prompted to generate the gold facts\nrequired to deduce the correct answer.  Subsequently, these facts form the basis of a recursive\nquerying process, where the LLM is used to establish the reasoning that connects them, therefor\nconstructing a reasoning tree.  This tree components get then used one by one to generate the\nfinal narrative.  This method generates a narrative that is a hard for machines yet solvable by\nhumans  refer  to  Table  12.   This  is  true  even  when  using  multiple  prompting  strategies  and\nneurosymbolic approaches like chain of thought plus.  The limiting factor discovered by the\nMUSR creators is the limitation that LLMs encounter when generating the deep reasoning trees\nthis greatly limits the narrative complexity.\n5.7.5    BBH\nBBH or Big-Bench-Hard is the improvement of the original Big-Bench which is a bench-\nmark  consisting  of  204  tasks  from  405  authors  across  132  institutions.   This  extensive  and\ndiverse authorship is a significant strength of the benchmark, as it serves to lower any potential\nfor institutional or individual biases that might come from more limited set of contributors.\nThe topics covered by the benchmark are exceptionally diverse, spanning a wide range of\ndisciplines including linguistics, childhood development, mathematics, common-sense reas",
  "on-\ning, biology, physics, social bias, and software development, among others. The tasks selected\nfrom these domains were specifically chosen because they were considered to be beyond the\ncapabilities of state-of-the-art models at the time of its creation.\nBBH was formed, by curating a specific subset of tasks from the original Big Bench collec-\ntion. The selection process was designed to isolate the most dificult challenges for contemporary\nmodels. The resulting benchmark consists of merely 23 tasks, that were chosen exclusively due\nto the performance being lower for State-of-Art models than those achieved by human raters.\nFrom this group of tasks that proved difficult for machines,  only the most demanding were\nselected to form the final BBH set.\nThe  new  benchmark  prompting  also  differed  in  prompting  since  the  new  approach  uses\nChain-of-Thought prompting.  With this prompting style all models suffered an improvement",
  "some as much as 28.5%.  The categories of this dataset are also relevant since they don’t just\nfocus on algorithmic tasks, as they also have natural language understanding, world knowledge,\nand multilingual.  The last one being great addition since it transforms this dataset into a mul-\ntilingual one.  Though the linguistics part doesn’t affect the score by much due to its size in\nrelation with the whole dataset.\n5.7.6    IFEval\nIFEval [59] or instruction-following evaluation is used to project the ability of LLMs to\nadhere to verifiable instructions.\nThis  evaluation  task  consists  of  25  verifiable  instructions,  these  are  divided  into  seven\ngroups, keywords, language, length constraints, dete",
  "ctable content, combination, case change,\nstart with / end with, and punctuation.  The instruction also comes with a description to exem-\nplify what the model is required to do.\nThis metric is designed to evaluate the model’s ability to adhere to the instructions provided\nby the proposed system, as illustrated in Figure 27. The method also accounts for errors in the\nmodel’s text formatting relative to the given instruction.  This is accomplished using a flexible\naccuracy metric that tolerates superficial differences, such as formatting variations, provided\nthe core intent of the instruction is fulfilled.\nTo provide a comprehensive assessment, instruction-following is evaluated at two granular-\nities.  These being the per-prompt basis and the per-instruction basis.  This dual-level analysis\nreveals whether the model can maintain adherence to a sequence of instructions or if it only\nfollows the initial ones successfully.\n5.7.7    ARC\nAI2 Reasoning Challenge [60] consists of a dataset and evaluation framework created with\nthe intuition of assessing and advancing the reasoning capabilities of AI systems. This system is\nspecially designed for assessing the models capabilities at answering challenging grade-school-\nlevel science questions.\nARC contains two different sets, the challenge consists of 2590 queries and the easy con-\ntaining 5197.  The hard set is composed with queries that both retrieval-based solutions and\nword co-occurrence fail to solve, on the other hand the easy set is composed of questions that\ndon’t require a lot of reasoning to be answered.\nThe key difference on this data-set comes from the fact that to answer the quest",
  {
    "type": "image",
    "src": "/images/methodology/page_79_img_1.png",
    "alt": "Page 79 Image - Page 79 Image",
    "caption": "Page 79 Image"
  },
  "ion on the\nchallenge-set the LLM require deeper reasoning, due to the fact that these can’t be answered\nusing surface-level cues or simple retrieval methods.\nThe paper also talks about how even models that used IR or Pointwise Mutual Information\n(PMI) failed to outperform random guessing on the Challenge set.",
  "5.7.8    HellaSwag\nHellaSwag [61] is a test set created to evaluate LLMs ability in commonsense natural lan-\nguage inference (NLI). The task is to choose the most reasonable continuation of a context,\nfrom four possibilities.\nThe test set is made to be simple for human participants with an accuracy of 95.6%, yet\nsimultaneously difficult for state-of-the-art models, with accuracy below 50%.\nThe dataset is an extension of the original SWAG dataset but with the inclusion of Adver-\nsarial Filtering (AF). This is done to increase the difficulty of the task.  AF works by contin-\nuously selecting wrong answers generated by adversarial machines,  thereby keeping a chal-\nlenging dataset even for state-of-the-art models like BERT. The novelty lies in generating a\n”Goldilocks zone” of text difficulty, where the wrong answers are nonsensical to humans but\nare often misclassified by models.\nHellaSwag consists of 70,000 examples that are gathered from ActivityNet video captions\nand WikiHow text, thus contributing to the diversity of contexts in addition to the length and\ndifficulty of the examples. The dataset also includes zero-shot test classes, in which models are\ntested on unseen domains to estimate their ability to generalize.\nThe assessment is centered on whether models can reason on what is likely the next event\nor employ dataset-s",
  "pecific bias. Results indicate that even the top models, i.e., BERT\nLarge\n, fail\nto generalize but instead depend on shallow lexical patterns rather than actual commonsense\nreasoning.\nThis  benchmark  indicates  the  weaknesses  of  existing  language  models  in  reasoning  and\nunderstanding the world, demonstrating that natural language processing progress demands the\ndevelopment of benchmarks that evolve together with progress in model capabilities.\n5.7.9    ThrutfulQA\nThrutfulQA [62] is a benchmark designed to evaluate the truthfulness of LLMs when an-\nswering questions. This benchmark is constructed with 817 questions across 38 different cate-\ngories, such as health, law, finance, and conspiracies. These questions were specially designed\nto test whether models generate imitative falsehoods, these are answers that mimic common\nhuman misconceptions or misinformation found in the training data, including falsehoods. For\nexample, some models might say that cracking knuckles causes arthritis, even though this is\na false statement.  This benchmark also shows a big gap in human to LLM performance, with\nhumans achieving 94% compared to just 58% of the UnifiedQA LLM. This method can be used\nto see if techniques like fine-tuning to prioritize truthfulness over imitation are achieving the\nwanted results. ThrutfulQA is a great tool to stop the spread of misinformation and reduce the\ndeception caused by the usage of LLMs by users that think these models are truthful.",
  "5.7.10    WinoGrande\nWinoGrande is an expanded version of the Winograd Schema Challenge, which originally\nconsisted of 273 expert-crafted pronoun resolution problems.  These",
  "problems are trivial for\nhumans but challenging for machine learning algorithms, as these require commonsense rea-\nsoning rather than reliance on statistical patterns or word association.  But with the state-of-art\nmodels achieving near-perfect scores there was a need to improve on the original method so\nWinoGrande was created.\nThis method introduces 44000 problems inspired by the previous method but this time de-\nsigned to be more challenging and also scalable. These new problems were created using crowd-\nsourcing and after-validated to ensure their trivialness to humans while still being difficult for\nstate-of-the-art models.\nThis data set shows a big gap in performance from LLMs to SLMs.\n5.7.11    GSM8K\nGSM8K [63] is a dataset specially crafted to evaluate the LLM’s ability to perform multi-\nstep mathematical reasoning.   This data set consists of 8.5K high-quality grade school math\nword  problems,  this  data  set  is  split  with  7.5k  on  the  data  set  and  1k  for  the  testing.   The\nproblems are linguistically diverse and need 2 to 8 steps to solve, these are focused on basic\narithmetic operations like addition, subtraction, multiplication, and division.  The solutions to\nthese are provided in natural language to encourage the model’s interpretability and reasoning.\nThis benchmark is used to test a model’s performance on informal reasoning and problem-\nsolving capabilities.\n5.7.12    Math Lvl5\nThis data set MATH [64] consists of various levels of math problems with five being the\nmost challenging tier. This test is designed to test advanced reasoning and heuristic applications.\nThe problems require a deep understanding o",
  "f mathematical concepts, creative problem-solving\nstrategies, and the ability to aggregate various techniques to find a solution.  The performance\nof LLM models like LLaMA 3.1 8B achieves only 5.36% [54], while International Mathemat-\nical Olympiad gold medalists achieve a near-perfect score.  This highlights the significant gap\nin performance between current AI models and expert-level human reasoning.  The problems\nfound in this data set require logical chaining, abstraction, and error-free computation, areas\nwhere LLM’s performance tends to be lackluster.\n5.7.13    RAGEval\nRAGEval  [65]  is  more  than just  a  dataset,  it  is  a  framework  design  to  assess RAG  sys-\ntems across various scenarios by generating high-quality documents, questions, answers, and",
  "references.  All of these are generated by a schema-based pipeline to maintain accuracy.  This\napproach allows them to implement metrics that differ from the standard and are more aligned\nwith factual accuracy, there are three metrics for this, Completeness, Hallucination, and Irrele-\nvance.\nThe process to generate all the needed files is as follows: S −→ C −→ D −→ (Q,A)−→ R −→\nKeypoints\nThis sequence shows all the steps taken by this approach starting with the schema summary\nS that leads to the configuration generation C, followed by the document generation D.  With\nthe  document  formed  the  question  answer  pairs  are  formed (Q,A)  and  the  references  need\nto come that answer identified R and then the keypoints are extracted, representing the most\ncritical information in the answers.\nThe schema summary is an abstract representation of the key elements in a scenar",
  {
    "type": "image",
    "src": "/images/methodology/page_83_img_1.png",
    "alt": "Page 83 Image - Page 83 Image",
    "caption": "Page 83 Image"
  },
  "io-specific\ntext generation, these key elements encapsulate the aspects of essential factual knowledge from\nthe  input  documents.   This  schema  acts  as  a  backbone  to  ensure  that  the  content  is  diverse\nand reliable while maintaining a standard across various scenarios.   The schema defines the\nstructural framework of key elements for domain-specific documents without containing actual\ndata.  As an example in medicine,  it can outline categories for symptoms and treatments,  in\nfinance it could establish classifications for metrics, sectors, and organizations.  One concrete\nexample of a schema generation starts with the initial generation by the LLM, the model gets\nfeed with carefully chosen seed documents, these are real legal documents that represent the\nkind of knowledge and structure that the schema is to take. After the schema is created a series\nof iterative refinements are taken by a human using it’s intuition and contextual understanding\nto fix any nuances the model had generated. Due to the fact that this process occurs more than\nonce, it ensures that a balance between comprehensiveness, accuracy, and generalization, thus\nsupporting content generation across diverse sub-scenarios.\nGenerating a document that is rich with factual information and isn’t contradictory, is cru-\ncial to creating high quality datasets, ensuring that the generated content can be evaluated ac-\ncurately and used effectively in downstream tasks.  To generate documents with that quality,\nfirst the configurations C are generated, these derive from the previously established schema S.\nThese configurations are used as references and constrains fo",
  {
    "type": "image",
    "src": "/images/methodology/page_84_img_1.png",
    "alt": "Page 84 Image - Figure 24:  RAGEval System:  1 summarizing a schema containing specific knowledge from",
    "caption": "Figure 24:  RAGEval System:  1 summarizing a schema containing specific knowledge from"
  },
  "r text generation, thus maintaining\nconsistency across the document.  To generate these configurations a hybrid approach is taken\nthat combines rule-based methods with LLMs to assign values to the schema elements.  These\nrule-based methods like selecting values randomly from predefined scenario-specific options,\nensure that high accuracy and factual consistency is maintained for a more structured data. This\nand the more complex or diverse content, balances consistency and creativity.  After the con-\nfiguration is ready a GPT-4o is used to convert the factual information from the C into a more\nstructured narrative format that is more aligned with a specific scenario. For example, in medi-\ncal records the generated document can include categories that add a more complex background\nto the document these can be a patient information, medical history, or a treatment plan.  The",
  "same is done with other topics but with categories that better align with them.\nFigure 24:  RAGEval System:  1 summarizing a schema containing specific knowledge from\nseed documents.  2 filling in factual information based on this schema to generate diverse con-\nfigurations.   3  generating  documents  according  to  the  configurations.   4  creating  evaluation\ndata composed of questions, answers, and references derived from the configurations and doc-\numents.[65]\nQuestion-Reference-Answer (QRA) to generate these RAGEval uses the documents D and\nconfigurations C  these are used to establish a robust evaluation framework ready to be used\non information retrieval and reasoning capable applications.  The configurations C are used to\nguide the generation of the qu",
  {
    "type": "image",
    "src": "/images/methodology/page_85_img_1.png",
    "alt": "Page 85 Image - Page 85 Image",
    "caption": "Page 85 Image"
  },
  "estions as well as the initial answers, this forces the generated\ncontent to be aligned with the schema elements.  To address different types of questions like,\nmulti-hop reasoning,  summarization,  and multi-document questions,  each one is specifically\ndesigned to evaluate specific facets of language understanding. To ensure diversity and control-\nlability of these questions 7 main question types were designed. The model gets provided with\ndetailed instructions and examples for the question type needed to be generated, the model then\noutputs the question Q as well as the initial answer A. Using the Q and A the relevant informa-\ntion fragments get extracted R from the documents D. This is done using an extraction prompt,\nthus ensuring that the generated answer is grounded in the source material this improves the\nreliability and traceability.  To reduce the misalignment between A and R, the answers get it-\neratively refined thus also improving the coherence and accuracy. If references contain content\nmissing from the answers they supplement them accordingly.  To reduce the hallucinations a\nlook is taken at the answers to find any unsupported content that either gets corrected with rel-\nevant references or removed.  Finally the keypoints get generated from the answers A for each\nquestion Q to highlight the critical information in the responses. Normally each answer A gets\nbroken down into 3-5 keypoints, that encompass all essential factual detail, as well as relevant\ninferences, and conclusions.\nDragonBall dataset which means Diverse RAG Omni-Benchmark for All scenarios.  This\ndataset was created using all the methods described above",
  "and RAG questions across 3 main domains finance,  law,  and medical.  This dataset consists\nof both Chinese and English texts, that serve as a comprehensive resource for multi-language\nscenario-specific research. Due to its big size of 6.711 questions it can be used on just English\nor Chinese assessment.\n5.7.14    HotPotQA\nHotPotQA [66] is a dataset with 113k question-answer pairs that contain 4 main features\nthat tell it apart.  Number one being the question requires finding and reasoning over multiple\ndocuments to achieve a correct answer. Number two is the variety these questions present, these\nare diverse and aren’t constrained to any pre-existing knowledge bases or schemas.  Number\nthree this dataset provides sentence-level supporting facts that are needed for reasoning. Lastly\nnumber four it also introduces factoid comparison questions that test wether QA systems can,\nnot only extract relevant facts, but also compare them.\nThe data used for this dataset creation was gathered by crowd-workers on Amazon Mechan-\nical Turk.  The gathered data had it’s origin on English Wikipedia, the process followed five\nmajor steps.  Starting with finding a paragraph on a specific Wikipedia page, next is the navi-\ngation to a hyperlynk found in that paragraph, this will later be a related article. The following\ntask is the formation of a question about the two articles, this formed query can’t be answered\nwith just the information of one and needs the two of them to be complete.   The following\nstep is the generation of a answer to the previous question this answer gets information from\nboth pages. The final s",
  {
    "type": "image",
    "src": "/images/methodology/page_87_img_1.png",
    "alt": "Page 87 Image - Page 87 Image",
    "caption": "Page 87 Image"
  },
  "tep is to identify the supporting facts, this is a crucial explainability step\nwhere the worker must select the specific sentences from the two articles that are needed to\nreason through and arrive to the final answer. The specific sentences found are the ground-truth\nsupporting facts. These steps form the generation pipeline of this method.\nThis method being specially develop with reasoning in mind, comes with two main types of\nreasoning Bridge-Entity, and Comparison Reasoning.\nBridge-Entity  Reasoning  is  most  prevalent  question  in  HotPotQA  and  involves  a  bridge\nentity, meaning the two necessary documents are linked by a common person, place, or thing.\nTo  answer  the  question,  the  model  must  first  use  one  document  to  identify  this  bridge  and\nthen use the second document to find the final piece of information.  As for the Comparison\nReasoning this requires the model to extract information and form various facts,  these facts\nneed then to be compared between themselves to find the final solution. This can be something\nlike ”when was the last public record of Astra systems publicized”.   This would require the\nmodel to find all the public records about Astra systems, then it would need to compare dates\nof all the documents, and only then would the final answer be evident.\nThis dataset also comes with a evaluation framework specially designed to work with the\ndifferences found from more common datasets. The evaluation is measured in answer accuracy\nthat is the standard evaluation of wether the final answer is correct, this is done using a Exact\nMatch (EM) and F1 score.  To measure the Supporting Facts Accuracy th"
]
  },,
  abstract: {
    title: "Abstract",
    content: "As the computational and financial costs of state-of-the-art large language models (LLMs) continue to grow, deploying them becomes harder for resource-constrained organizations as improvement methods such as Retrieval-Augmented Generation (RAG), Chain-of-Thought (CoT), HyDE, and related techniques enhance quality but incur variable overheads. This work presents a dynamic query-routing framework, in which a compact LLM (8B parameters) is paired with an adaptive controller that selects from three routes per query: direct answer, CoT or, RAG. Therefore the controller builds on iterative prompt refinement, proceeding through six instruction designs that evolve from format-driven heuristics to profile-based classification, and employs a voting-style post-processor to ensure robust decision extraction. The proposed framework is evaluated on routing accuracy, end-to-end answer correctness, and detailed energy profiling (CPU and GPU) using a composite dataset that combines retrieval-heavy general-knowledge and reasoning-focused science questions (ARC-Easy and HotPotQA-style items), on a single-GPU workstation. Results show that profile-based prompts can improve routing balance: mature versions reach 85%+ answer accuracy on ARC-style queries while remaining much more energy efficient than larger models. Moreover, analyses show that incorrect answers consume more energy, and that instruction design shifts the energy burden between CPU-heavy retrieval and GPU-heavy reasoning. Consequently our results indicate that architectural control and prompt engineering can close the performance gap between small and mid-sized models while achieving significant efficiency gains and providing a practical path to high-quality IR and QA systems under tight resource constraints and data security requirements.", \npara uma classificação baseada em perfil, e emprega um \npós-processador do tipo votação para garantir uma extração \nde decisão robusta. A estrutura proposta é avaliada em \ntermos de precisão de encaminhamento, correção da \nresposta de ponta a ponta e perfil energético detalhado \n(CPU e GPU), utilizando um conjunto de dados compósito \nque combina conhecimento geral com forte dependência de \nrecuperação de informação e questões de ciência focadas \nem raciocínio (itens ao estilo ARC-Easy e HotPotQA), num \ncomputador com uma única GPU. Os resultados mostram \nque os prompts baseados em perfil podem melhorar o \nequilíbrio do encaminhamento: as versões mais \ndesenvolvidas atingem uma precisão de resposta superior a \n85% em consultas do tipo ARC, mantendo-se muito mais \neficientes em termos energéticos do que modelos maiores. \nAlém disso, as análises demonstram que as respostas \nincorretas consomem mais energia e que o design da \ninstrução transfere o consumo de energia entre a \nrecuperação de informação (RAG), intensiva em CPU, e o \nraciocínio, intensivo em GPU. Consequentemente, os \nnossos resultados indicam que o controlo arquitetónico e a \nengenharia de prompts podem diminuir a diferença de \ndesempenho entre modelos de pequena e média dimensão, \nenquanto alcançam ganhos de eficiência significativos e \nfornecem um caminho prático para sistemas de \nRecuperação de Informação (IR) e de Pergunta-Resposta \n\n- ii - \n \n \n(QA) de alta qualidade, sob fortes restrições de recursos e \nrequisitos de segurança de dados. \n \n \n \n \n  \n\n- iii - \n \n \n  \n\n- iv - \n \n \nKeywords \n \nAI; RAG; Quantization; HyDE\n\n; Large Language Models; \nInstruction Optimization. \n \n \nabstract \n \nAs the computational and financial costs of state-of-the-art \nlarge language models (LLMs) continue to grow, deploying \nthem becomes harder for resource-constrained \norganizations as improvement methods such as Retrieval-\nAugmented Generation (RAG), Chain-of-Thought (CoT), \nHyDE, and related techniques enhance quality but incur \nvariable overheads. This work presents a dynamic query-\nrouting framework, in which a compact LLM (8B \nparameters) is paired with an adaptive controller that selects \nfrom three routes per query: direct answer, CoT or, RAG. \nTherefore the controller builds on iterative prompt \nrefinement, proceeding through six instruction designs that \nevolve from format-driven heuristics to profile-based \nclassification, and employs a voting-style post-processor to \nensure robust decision extraction. The proposed framework \nis evaluated on routing accuracy, end-to-end answer \ncorrectness, and detailed energy profiling (CPU and GPU) \nusing a composite dataset that combines retrieval-heavy \ngeneral-knowledge and reasoning-focused science \nquestions (ARC-Easy and HotPotQA-style items), on a \nsingle-GPU workstation. Results show that profile-based \nprompts can improve routing balance: mature versions \nreach 85%+ answer accuracy on ARC-style queries while \nremaining much more energy efficient than larger models. \nMoreover, analyses show that incorrect answers consume \nmore energy, and that instruction design shifts the energy \nburden between CPU-heavy retrieval and GPU-heavy \nreasoning. Consequently our results indicate that \narchitectural control and prompt en\n\ngineering can close the \nperformance gap between small and mid-sized models \nwhile achieving significant efficiency gains and providing a \npractical path to high-quality IR and QA systems under \ntight resource constraints and data security requirements. \n \n \n \n  \n\n- v - \n \n \n \n \n\nContents\n1    Abstract2\n2    Resumo3\n3    Acknowledgments11\n4    Introduction15\n4.1Motivation .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .15\n4.2Aims and Research Questions    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .16\n4.3Document Outline .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .17\n5    State-of-the-Art17\n5.1Transformers and Language Models   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .17\n5.2Quantization .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .21\n5.2.1Post-Training Quantization (PTQ)  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .21\n5.2.2Weight-Only Quantization   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .23\n5.2.3Non Uniform Weight Quantization .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .24\n5.2.4Weight + Activation Quantization   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .30\n5.2.5Mixed Precision Quantization   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .30\n5.2.6Quantization-Aware Training (QAT)  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .32\n5.3Retrieval-Augmented Generation (RAG)  .  .  .  .  .  .  .  .  .  .  .  .  . \n\n .  .  .  .  .  .  .  .34\n5.3.1Hypothetical Document Embeddings (HyDE)  .  .  .  .  .  .  .  .  .  .  .  .  .  .36\n5.3.2Cache Augmented Generation   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .39\n5.3.3Hybrid approaches   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .40\n5.4Challenges and Applications of Quantization in RAG  .  .  .  .  .  .  .  .  .  .  .  .  .  .48\n5.5Retrieval Methods to enhance HyDE  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .48\n5.5.1Contriever .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .49\n5.6Self-Knowledge Guided Retrieval Augmentation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .50\n5.6.1Collecting Self-Knowledge .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .51\n5.6.2Eliciting Self-Knowledge of LLMs    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .51\n5.6.3Using Self-Knowledge for Adaptive Retrieval Augmentation.  .  .  .  .54\n5.7Model Comparison   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .54\n5.7.1MMLU   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .54\n5.7.2MMLU-Pro  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .54\n5.7.3GPQA .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .55\n5.7.4MUSR    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .56\n5.7.5BBH    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .\n\n  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .57\n5.7.6IFEval .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .58\n5.7.7ARC    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .58\n4\n\n5.7.8HellaSwag   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .59\n5.7.9ThrutfulQA  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .59\n5.7.10   WinoGrande   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.11   GSM8K .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.12   Math Lvl5    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.13   RAGEval  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .60\n5.7.14   HotPotQA    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .63\n6    Methodology64\n6.1Overview of the Query Rewriting Flow    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .64\n6.2Hardware and Software Environment .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .66\n6.3Rewriting Approaches    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .66\n6.3.1Straight LLM  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .66\n6.3.2Chain-of-Thought    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n\n .  .  .  .  .  .  .  .  .  .66\n6.3.3RAG    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .67\n6.3.4Selecting the Approach .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .67\n6.4Dataset Augmentation    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5Query-Answer Validation  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.1Automated Preparation  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.2AI-Powered Triage  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.3Human Verification  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .69\n6.5.4Dataset formation .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.6Power Data Collection   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.6.1GPU    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.6.2CPU .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .70\n6.7Evaluation Framework   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .71\n6.7.1Retrieval Performance Metrics  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .72\n6.7.2Straight Model   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .73\n6.7.3Chain-of-Thought Reasoning Performance Me\n\ntrics  .  .  .  .  .  .  .  .  .  .  .73\n6.7.4Automated Evaluation Script .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .73\n6.7.5Efficiency Metrics   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .74\n6.8Optimizing Query Classification through Iterative Prompt Refinement   .  .  .  .  .74\n6.8.1Instruction V1: A Simple Baseline .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .75\n6.8.2Instruction V2: An Aggressive, Safety-First Heuristic .  .  .  .  .  .  .  .  .  .77\n6.8.3Instruction V3: Introducing Balanced Criteria  .  .  .  .  .  .  .  .  .  .  .  .  .  .80\n6.8.4Instruction V4: A Shift to Profile-Based Classification   .  .  .  .  .  .  .  .  .84\n6.8.5Instruction V5: Final Refinement with a Guiding Principle  .  .  .  .  .  .  .88\n6.8.6Instruction V6: A Strategic Pivot to Efficiency    .  .  .  .  .  .  .  .  .  .  .  .  .92\n6.9Analysis of Energy Consumption and Efficiency .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .94\n5\n\n6.10  Detailed Energy Consumption Profiles  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .98\n6.10.1   Overall Energy Trends Across Instruction Versions  .  .  .  .  .  .  .  .  .  .  .98\n6.10.2   CPU vs. GPU: Deconstructing the Energy Cost  .  .  .  .  .  .  .  .  .  .  .  .  .   100\n6.10.3   The Energetic Cost of Correcting Errors  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   101\n6.10.4   Energy Distribution and Consumption Predictability   .  .  .  .  .  .  .  .  .  .   104\n6.10.5   Overall Performance Quadrant:  Synthesizing Accuracy and Efficiency\nfor ARC queries    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .",
  },
  introduction: {
    title: "Introduction",
    content: [
      "Answering\nPMIPointwise Mutual Information\nPPLPerplexity\nPTQPost-Training Quantization\nQATQuantization-Aware Training\nQRAQuestion-Reference-Answer\nRAGRetrieval-Augmented Generation\nRNNRecurrent Neural Network\nRPTQReorder-based Post-training Quantization\nRTNRounding to Nearest-Number\nRTRLReal-Time Recurrent Learning\nSKRSelf-Knowledge Guided Retrieval\nSLMsSmall Language Models\nSMEsSmall to Medium Enterprises\nSMLsSmall Language Models\nSpQRSparse-Quantized Representation\nT-PTLMsTransformer-based Pre-trained Language Models\nThrutfulQATruthfulQA\nvLLMA high-throughput LLM serving engine\nWinoGrandeWinograd Schema Challenge\n14",
      "4    Introduction\nArtificial Intelligence (AI) has revolutionized natural language processing (NLP) through\nthe  advent  of  Large  Language  Models  (LLMs),  which  demonstrate  exceptional  capabilities\nin  understanding  and  generating  human-like  language,  with  widespread  applications  across\ndiverse  industries.   However,  deploying  these  models  in  real-world,  regulated  environments\npresents substantial challenges.",
      "Organizations like banks, hospitals, and government offices are likely to handle sensitive\ninformation that should not exit their premise under strict data privacy laws like GDPR and\nCCPA. Legal restrictions like these inhibit the use of AI models that are often hosted on external\nservers,  where there is limited transparency in data processing operations.",
      "Furthermore,  the\ncomputational demands of LLMs make them prohibitively expensive for small and medium-\nsized enterprises (SMEs), which often lack access to high-performance hardware infrastructure.",
      "ntization and the development of lightweight LLMs,\na major gap still remains in effectively adapting these models to specialized, domain-specific\ntasks under limited computational resources.  Methods like Retrieval-Augmented Generation\n(RAG) and Hypothetical Document Embedding (HyDE) have emerged as strong contenders\nin the sense that they can integrate external knowledge into the models with ease.",
      "However,\nsuccess in such applications largely relies on the quality of query rewriting and retrieval.\nThis thesis focuses on retrieval-based question answering in such contrained domains, lever-\naging query rewriting to improve relevance and reduce computational overhead. This is achieved\nby  at  first  optimizing  Small  Language  Models  (SLMs)  through  advanced  quantization  tech-\nniques, which significantly reduce their computational and memory footprint.",
      "However, this ap-\nproach introduces a critical challenge, that is the degradation in model performance and knowl-\nedge retention.  To counter this effect, the system integrates a powerful Retrieval-Augmented-\nGeneration (RAG) framework.  This framework is not merely just add-on for external knowl-\nedge but it serves as a targeted mechanism to recover most of the performance lost during the\nquantization stage.",
      "By leveraging instructions optimization, and Chain-of-Thought reasoning,\nthe RAG component ensures that the quantized SLM can access and effectively utilize precise,\nrelevant information from external document.\n4.",
      "(LLMs).  Despite their capabili-\nties, significant challenges remain in deploying these models in highly regulated and resource-\nconstrained environments.  Organizations such as banks and public institutions face significant\nbarriers in adopting AI models due to strict data protection laws (eg., GDPR, CCPA) and high\ncomputational  requirements.   These  constraint  limit  their  ability  to  utilize  externally  hosted\nLLMs  or  fine-tune  large  models  for  domain-specific  tasks.",
      "demands of  LLMs make them expensive  to deploy,  requiring  powerful  hardware infrastruc-\nture that is often unaffordable for small to medium enterprises (SMEs).  While advancements\nin quantization techniques and lightweight models have made LLMs more accessible, there is\nstill a gap in optimizing these models for domain-specific tasks without extensive computa-\ntional resources.",
      "Retrieval-Augmented Generation (RAG) and HyDE methods have emerged\nas promising solutions, enabling models to integrate external knowledge efficiently.  However,\ntheir effectiveness depends on the quality of query rewriting and retrieval mechanisms.  This\nthesis seeks to address these challenges by evaluating lightweight, domain-aware strategies for\nquery rewriting within a RAG framework.",
      "By leveraging techniques such as query augmen-\ntation, voting system, Retrieval Augmented Generation and Chain-of-Thought reasoning, the\ngoal is to improve retrieval relevance and performance in regulated and resource-constrained\nsettings.",
      "ing practical insights for deploying AI systems in real-world ap-\nplications.\n4.2    Aims and Research Questions\nThis thesis aims to optimize small language models (SLMs), explore the trade-offs between\nquantized  and  non-quantized  models,  investigate  retrieval  methods  to  enhance  SLM  perfor-\nmance, and outline directions for future research.",
      "SLM optimization will be approached by ex-\nploring recent techniques identified throughout the course of this work, with the goal of making\nthese models more practical and resource-efficient. Analyzing the trade-offs between quantized\nand non-quantized models is an important step, as it will help determine whether future research\nshould focus on quantizing larger models or on further refining smaller ones.",
      "Retrieval methods are a good way to achieve better performance on SMLs on tasks that\nnormally would require training with more specific data-sets.  Selecting an effective retrieval\nstrategy is key to developing a lightweight, high-performing system.\nModel optimization may also include simple strategies such as query injection.  These ap-\nproaches will be evaluated to determine their usefulness and relevance to the overall objectives\nof the thesis.",
      "This research aims to answer the following key questions:\nRQ1:  How can a system using smaller models still compete with larger ones in terms of\nperformance and efficiency?\nRQ2:Can HyDE be applied to a system designed for efficiency?\nRQ3:  What are the trade-offs among answer quality,  inference latency,  and energy con-\nsumption for each rewriting strategy?\nRQ4: Can an efficient approach still achieve high accuracy while remaining useful?",
      "4.3    Document Outline\n1.  Introduction\n•  Sets the stage by explaining the motivation, challenges, and research questions.\n2.  Background and State-of-the-Art\n•  Reviews existing techniques and it’s limitations.\n3.  Methodology\n•  Details the approaches used to optimize SLMs and enhance retrieval.\n4.  Evaluation Framework\n•  Explains how the methods are assessed using different metrics.\n5.  Results and Discussion\n•  Analyzes the outcomes and trade-offs of the proposed methods.\n6.",
      "Conclusion and Future Work\n•  Summarizes the contributions and suggests future work.\n5    State-of-the-Art\n5.1    Transformers and Language Models\nNatural language processing (NLP) has been improving significantly over the last decades\ndue in part to the resurgence of deep neural networks (DNNs) [1].  The first sequential archi-\ntecture, RNN [2], had limitations regarding it’s ability to capture temporal dependencies.",
      "This\nlimitation is related to the vanishing or exploding gradient, which results in the impossibility for\nRNN to retain information over longer sequences. The longer the sequence, the more the gradi-\nents would diminish to near zero or infinity, resulting in less relevant weight updates. LSTM [3]\nand GRU [4] are two sequential models developed to overcome this limitation.",
      "LSTM was the\nfirst to use an algorithm to consider gradient-based learning, which could bridge time intervals\nin excess of 1000 steps. This was achieved using memory cells and gating mechanisms (input,\nforget and output).  This allowed the networks to retain, update, or forget information in the\nmemory cell, avoiding the vanishing/exploding gradient.",
      "U, doesn’t have\na separate memory cell; instead, it directly updates the hidden state using two gates: an update\ngate that combines the forget and input gates of the LSTM model and a reset gate that gives the\nnetwork the ability to control how much information it forgets.\n17",
      "Table  2:  Maximum  path  lengths,  per-layer  complexity  and  minimum  number  of  sequential\noperations for different layer types. [5]",
      {
        type: 'image',
        src: '/images/methodology/page_64_img_1.png',
        alt: 'Page 64 Image - Figure 1: Transformer model architecture',
        caption: 'Page 64 - Figure 1: Transformer model architecture [5]'
      },
      {
        type: 'image',
        src: '/images/methodology/page_65_img_1.png',
        alt: 'Page 65 Image',
        caption: 'Page 65'
      },
      "Transformer  models  introduced  many  changes  to  try  to  solve  all  of  the  problems  in  the\nprevious  models.   Starting  with  the  Self-attention  Mechanism  which  lowers  the  complexity\nfor short sequences.",
      "This is due to the fact that the computational complexity is O(n\n2\n∗ d)\nwhich is more efficient than Recurrent layers O(n∗ d\n2\n) when the sequence n is smaller than\nthe dimensionality d.  When parallelization is used it requires only O(1) sequential operations\nbut on the recurrent layers they need O(n) sequential operations due to their sequential nature.",
      "Due to the short path length between any two positions in the input and output sequences the\nsignals can travel between all pairs of positions, this proved to be a big improvement for leaning\nlong-range dependencies. Moreover which can be useful to improve the global context since it\ncan attend to all positions in the sequence at once.",
      "adjust based on the input sequence, this can also be modified to make the model focus on just\nthe local context.\nThe ability to interpret attention mechanisms is a significant advantage for understanding\nhow the model works. Papers suc",
      "h as [6] investigate what the model focuses on, which in turn\nprovides insights into its decision-making process.\nMulti-Head Attention this mechanism instead of performing a single attention function with\nd\nmodel\n-dimensional keys, values and queries they linearly project the queries, keys and values\nh times with different learned linear projections to d\nk\n(Queries),d\nk\n(Keys) and d\nv\n(V alues) di-\nmensions.",
      "The queries represent the search The way Multi-Head Attention computes attention\nindependently for each head allows the model to focus on different types of relationships and\nfeatures in parallel capturing more information, this also makes it possible to capture informa-\ntion that with a single head would normally be lost like complex relationships [7].\nMulti-Head Attention extends the standard attention mechanism by applying multiple atten-\ntion functions in parallel.",
      "Instead of performing a single attention operation with d\nmodel\ndimen-\nsional keys, values, and queries, the mechanism linearly projects the queries, keys, and values\nh times using different learned linear transformations into dimensions d\nk\n(Queries),d\nk\n(Keys)\nand d\nv\n(V alues).  These projections allow each head to compute attention independently, en-\nabling the model to focus on different types of relationships and features simultaneously.",
      "This\nparallelization captures a richer set of dependencies and patterns, including complex relation-\nships that might otherwise be lost with a single attention head [7].",
      {
        type: 'image',
        src: '/images/methodology/page_70_img_1.png',
        alt: 'Page 70 Image',
        caption: 'Page 70'
      },
      {
        type: 'image',
        src: '/images/methodology/page_72_img_1.png',
        alt: 'Page 72 Image',
        caption: 'Page 72'
      },
      {
        type: 'image',
        src: '/images/methodology/page_73_img_1.png',
        alt: 'Page 73 Image 1',
        caption: 'Page 73 Image 1'
      },
      {
        type: 'image',
        src: '/images/methodology/page_73_img_2.png',
        alt: 'Page 73 Image 2',
        caption: 'Page 73 Image 2'
      },
      {
        type: 'image',
        src: '/images/methodology/page_75_img_1.png',
        alt: 'Page 75 Image',
        caption: 'Page 75'
      },
      "s by scaling the dot products\nto prevent their values from becoming too large.\nIn natural languages,  word order plays a crucial role in conveying meaning and context.\nTherefore, it is essential for models to incorporate positional information.  Unlike recurrent or\nconvolutional models, the Transformer does not have an inherent mechanism to capture token\norder.",
      "To address this, positional encoding is introduced to inject information about the relative\nor absolute position of tokens within the input sequence.",
      "To make sure the the weight adapts to\nthe size of the query it uses the wave length of sin and cos that increases exponentially with the\ndimension of the index i, this grants that the positional encoding adjusts to compensate for any\ndimension length:\nPE(pos, 2i) = sin\n\npos\n10000\n2i\nd\nmodel\n\n[5]\nPE(pos, 2i + 1) = cos\n\npos\n10000\n2i\nd\nmodel\n\n[5]\nAs a benefit to how the model treats the positional encoding, it is able to adapt to sequences\nlonger than the ones found in training.",
      "tween them:\nFFN(x) = max(0,xW\n1\n+ b\n1\n)W\n2\n+ b\n2\n[5]\nReLU or rectified linear unit is used to add non-linearity to the network allowing it to learn more\ncomplex functions.   The non linearity comes from the activation function max(0,x),  which\nwhen the input x is positive it passes through without any change,  on the other hand when\nthe input is negative it outputs 0.",
      "r-layer for 2048 this means that w\n1\nwill transform from\n512 to 2048 and then w\n2\nwill transform the 2048 into 512.  ”Another way of describing this is\nas two convolutions with kernel size 1.”[5].\nThe evolution of the Transformer models came with GPT and BERT [8].",
      {
        type: 'image',
        src: '/images/methodology/page_79_img_1.png',
        alt: 'Page 79 Image',
        caption: 'Page 79'
      },
      "GPT and BERT\nwere the first T-PTLMs developed based on transformer decoder and encoder layers respec-\ntively, these models where the basis for the discovery that performance of T-PTLMs could be\nincreased just by increasing the size of the model which triggered the development of mod-\nels like GPT-3 (175B)[9], PANGU- (200B)[10] and even a model with 1.6 trillions of tokens\nnamed Switch-Transformers [11].",
      "Although performance is not strictly linear and depends on\nmany factors, the number of tokens plays a significant role. This was only made possible in part\ndue to the parallelization ability of the Transformer model.  Bert[12] differs from the original\nTransformer model due to its bidirectional architecture contrary to the original model which\nwas unidirectional.  Bert uses its bidirectional architecture (left-to-right, right-to-left) to access\ncontext from both directions simultaneously.",
      "Another  important  feature  introduced  during  pretraining  is  Masked  Language  Modeling\n(MLM), in which 15% of the tokens in the input are randomly masked and the model learns to\npredict them using bidirectional context. Additionally, Next Sentence Prediction (NSP) is used\nto train the model to determine whether two sentences logically follow each other.",
      "odel in conjunction with segment embeddings\nto handle sentence pairs, this allowed BERT to handle both single sentence and sentence pair\ntasks.  GPT differs significantly in its training approach by combining both supervised and un-\nsupervised learning. In the unsupervised phase, the model is trained on a large text corpus (e.g.,\nBooksCorpus) using standard language modeling. This is followed by a fine-tuning phase using\nsupervised learning on specific downstream tasks.",
      "This two-stage process allows the model to\nfirst learn general language patterns and then adapt to more specialized tasks.  GPT also uses\nonly a Transformer decoder architecture, consisting of 12 layers of masked self-attention.  Be-\ncause it relies solely on a decoder, the model can only attend to previous tokens in the sequence,\nmaking it well-suited for auto-regressive language modeling.",
      {
        type: 'image',
        src: '/images/methodology/page_83_img_1.png',
        alt: 'Page 83 Image',
        caption: 'Page 83'
      },
      {
        type: 'image',
        src: '/images/methodology/page_84_img_1.png',
        alt: 'Page 84 Image',
        caption: 'Page 84'
      },
      {
        type: 'image',
        src: '/images/methodology/page_85_img_1.png',
        alt: 'Page 85 Image',
        caption: 'Page 85'
      },
      {
        type: 'image',
        src: '/images/methodology/page_87_img_1.png',
        alt: 'Page 87 Image',
        caption: 'Page 87'
      },
      "This model construction makes it\nsuitable to need minimal architecture changes when adapting to different tasks.",
      "5.2    Quantization\nQuantization is one of the most important techniques used to improve the performance and\nefficiency of large language models (LLMs), which are increasingly applied across a wide range\nof domains from customer service to scientific research.  However, as models grow in size and\ncomputational demands increase, a major challenge arises:  the hardware required to run these\nmodels becomes a limiting factor. High-performance hardware can be extremely expensive and\nenergy-intensive.",
      "s to widespread adoption. To give an example, Microsoft’s Phi-3-mini-4k-instruct model\n[13] requires 512 H100-80G GPUs to be run consecutively for 10 days with each costing around\nC30,000. Although such needs refer to training, which is done only once, running this kind of\nmodel still proves to be a computationally heavy task.\nThe most frequently used method to improve on this challenge is quantization, which re-\nduces the computational and memory requirements of the machine learning model.",
      "It achieves\nthis by converting the model weights and, in some cases, the activations from high-precision\n32FP to a lower precision representation such as INT8. This reduces the memory consumption\nof the model on the GPU and can accelerate computation because integer operations consume\nfewer resources compared to floating-point operations.\nThe two major methods of quantization will be discussed in more detail in subsequent sec-\ntions: Post-Training and Quantization-Aware Training.\n5.2.",
      "1    Post-Training Quantization (PTQ)\nTThis is the most commonly used quantization method because it does not require access\nto the model’s training process.  The quantization is applied after the model has already been\ntrained, making it especially useful when training resources or data are unavailable.  This type\nof quantization has been proven not to be as accurate at lower bit levels and there is a tendency\nof degradation if quantized to lower than 8-bits.",
      "[14] However since this method is less resource\nintensive, it has attracted more attention with a remarkable surge in post-training quantization\nmethods in the recent years.",
      "t efficient,  as it directly quantizes 16-bit values to\n8-bit using row-wise symmetric quantization.   While this method is straightforward,  it typi-\ncally results in only negligible degradation in perplexity. However, this breaks down with 4-bit\nquantization as it witnesses a significant drop in perplexity [15].  To improve the quantization\nperformance for low-bit applications, ZeroQuant-V2 [15] proposed Low-Rank Compensation\n(LoRC) method.",
      "This method approximates the error E between the original weight matrix W\nand the quantized weight matrix\nˆ\nW  using storage-efficient low-rank matrix\n ̃\nE so that\nˆ\nW +\n ̃\nE\nwould be a better approximation of the original weight W [16].",
      "distributions could achieve even lower bit-width quantization.  This is due to the weight distri-\nbution after training being nonuniform so it makes sense for the weight distribution not being\nquantized uniformly.  This is done by allowing the quantization process to allocate more pre-\ncision to the ranges of weights that are more densely populated while leaving larger intervals\nfor less frequent weight ranges.",
      "Building upon these methodologies OPTQ[19] emerged as an\nadvancement in quantization for big LLM’s.   Making it possible to run OPT-175B on just a\nsingle Nvidia A100 GPU or only two of the more cost-effective A6000.  OPTQ also provided\ngreater results in the extreme quantization regime where models were quantized all the way\ndown to 2 bits, or even ternary values.",
      "he weights in a greedy order [21] this\nmeans that the weight picked for the next quantization was picked based on minimum quan-\ntization error, this performs well but compared to arbitrary order quantization only offered a\nnegligible improvement in large, heavily-parameterized layers.",
      "The likely reasons for the lack\nof improvement were that large individual quantization errors were balanced out overall, and\nthat these errors occurred later in the quantization process when fewer weights remained to\nbe quantized, leaving less opportunity for adjustment.  But with the OPTQ approach instead\nof quantizing the weights row-by-row, this method aimed to quantize the weights in all rows\nsimultaneously and in the same order.",
      "This can be shown by how the unquantized weights F\nand the inverse layer hessian (H\n−1\nF\n) depend only on the input activations (X\nF\n) and not on the\nweights themselves, this proves that the quantization of a column affected all rows uniformly.\nColumns  within  blocks  are  quantized  recursively  and  at  each  step,  unquantized  weights  are\nupdated based on the quantized weights.\nFigure 2: RN18 squared error.",
      "[21]\nWith that it also achieved efficiency gains compared to OBQ which achieved O(d\nrow\n· d\n3\ncol\n)\ncomparing  it  to  OPTQ  which  achieves O(max{d\nrow\n· d\n2\ncol\n,d\n3\ncol\n}),  reducing  it  by  a  factor  of\n{d\nrow\n,d\ncol\n}.  For larger models this can be proven to be more efficient into several orders of\nmagnitude. See Table 3 for details on runtime analysis.",
      "The second step involves the use of lazy batch updates, which were introduced to improve\nupon the original Optimal Brain Quantization (OBQ) method.",
      "original approach, weights\nwere iteratively quantized, requiring updates to all elements of a potentially large matrix while\n22",
      "ParameterValue\nd\nrow\n10000\nd\ncol\n100\nRuntime TypeCalculation\nOBQ RuntimeO(10000· 100\n3\n) = O(10, 000· 1, 000, 000) = O(10\n10\n)\nOPTQ Runtime\nMax Term: O(max{10000· 100\n2\n, 100\n3\n})\nCalculations: 10000· 100\n2\n= 10, 000· 10, 000 = 10\n8\n100\n3\n= 10\n6\nResult: O(max{10\n8\n, 10\n6\n}) = O(10\n8\n)\nTable 3: Runtime Analysis\nusing only a few FLOPs per entry[21].   This means that the GPU usage was limited by the\nspeed of the memory bandwidth.",
      "Lazy Batch-Updates addressed this issue by grouping updates\nacross B × B  blocks of the inverse Hessian matrix H\n−1\n,  with B  being typically set to 128\ncolumns at a time. These blocks are processed and after that, they are used to apply updates to\nthe matrix.  Thus avoiding the need for frequent recalculations throughout the matrix, cutting\non memory-bound operations.",
      "The final modification was the Cholesky Reformulation that was used to improve on two\nkey issues, numerical inaccuracies and error accumulation, the numerical inaccuracies occur as\nmodel sizes increase beyond a few billion parameters, these can lead to instability.  This hap-\npens because the matrix H\n−1\nF\nbecomes indefinite during iterative updates causing erratic weight\nupdates, the error accumulation is caused by the compounding numerical errors of the matrix\ninversion.",
      "Though previously they used dampening techniques like adding a small constant λ\n(which was normally always 1% of the average diagonal value) to the diagonal elements of H .",
      "Cholesky decom-\nposition, precomputation of necessary rows, and dampening prevents the H\n−1\nF\nfrom becoming\nindefinite mitigating the accumulation of numerical errors, and makes the algorithm suitable\nfor large models.  A deeper dive into some of these methods are described in the subsequent\nsections.\n5.2.2    Weight-Only Quantization\nThe Weight-Only Quantization focuses on quantizing only the weights of the LLM and not\nthe activations, this reduces the model size and memory transfer time.",
      "However, this doesn’t\nbenefit from hardware-accerlerated low-bit operations.",
      "is rounding to nearest-nember (RTN).[22], this works by quantizing a tensor x into k-bits.\nQ[x] = s× clamp\n\nx\ns\n,l\nmin\n,l\nmax\n\n[22]\nHere the s is the quantization scale, l\nmin\nand l\nmax\nare the low and upper bound clipping, which\nis then rounded to the nearest number⌊·⌋. Usually setting l\nmin\n=−2\nk−1\n+ 1 and l\nmax\n= 2\nk\n− 1\nand set s to be the maximum absolute value in x.  There are two main ways to find the best\nconfiguration in weight only quantization.",
      "The first one is by minimizing the reconstruction\nerror of the weight parameter which is define as\nr(W) :=∥Q[W]− W∥\n2\n[22]\nOn the function above only the weights are accessed therefore it’s a data-free process. However\nrecent studies ([19], [23]) propose the usage of output error as compensation.\ne(W) =\nX\nX∈D\n∥Q[W]X − WX∥\n2\n[22]\nD corresponds to the calibration set sampled form of the original training data, for optimization.",
      "By regularizing the model with its training data, more promising results are achieved com-\npared to the reconstruction-based method.",
      "s of these 2 methods have a two\nbig draw backs, the first one being that there’s a requirement of having the original training data\nof the model since most quantizations are done by others than the creators off the model makes\nit hard to find exactly which data was used to train the model. The second problem is that using\nthe same data again can jeopardize the ability of generalization of the model due to the model\nover-fitting to the calibration set.",
      "For this two reasons it is clearly very important to achieve a\nData-free quantization.\n5.2.3    Non Uniform Weight Quantization\nMost of the typical quantization methods handle the weights differently from this approach.\nThis method quantizes weights differently depending on their importance to the LLM. Not all\nweights  are  of  equal  importance  to  the  performance  of  a  model,  and  so  they  should  not  be\ntreated similarly, which is the case with techniques such as EasyQuant.",
      "EasyQuant[22] proposes the usage of the reconstruction error as the regulation metric since\nthis can be used to optimize the quantized model indirectly improving the generalization ability\nof the model. According to [22] the performance gap of the quantized model (INT4) and the full\nprecision model is due to two main factors.",
      "The first being that normally the quantization range\nis picked as the maximum absolute value of the weight thus inducing a large reconstruction error\nfor low-bits quantization.  The latter refers to the fact that the 0.",
      "model’s performance.Keeping this in m",
      "ind, if we try to define the outliers using the condition:\n|W\ni,j\n− mean(W)|≥ n· var(W)    [22]\nFor  any  weight W ,  where W\nij\nis  the  (i,j)-th  weight  and  (n)  representing  the  threshold  for\nidentifying the outliers, we can classify certain weights as outliers [22]. However, the challenge\nis that simply detecting the outliers and avoiding their quantization is not sufficient to achieve\ngood model performance.",
      "Furthermore,  if the percentage of outliers becomes too large,  the\noverhead introduced by the dequantization kernel increases, which can lead to a reduction in\noverall throughput.\nTable 4: PPL results on Wikitext2 of BLOOM-7B with and without outlier isolation. [22]\nEasyQuant also experimented with an ablation study focusing on three aspects, the outlier\ninfluence, outlier distribution, and the Quantization Range.  The ablation study began by pre-\nserving 10% of the weights in FP16.",
      "This resulted in an 8% increase in perplexity, compared to\nonly a 1% increase achieved with EasyQuant.  These findings suggest that simply isolating the\noutliers was not sufficient to maintain the expected perplexity levels. To check the outlier influ-\nence on EasyQUant, outlier isolation is key however this can only impose an indirect influence\non the model accuracy.",
      "The phenomenon found is the outliers behave like a gating mechanism\nmeaning that without the outlier isolation, the model performance deteriorates significantly with\nsmaller reconstruction error, and with outliers in FP16 the model shows continuous improve-\nment decreasing the perplexity with smaller reconstruction error Table 4.",
      "h influence outliers with big weight magnitude and small weight\nmagnitude have on the model performance, this was done by pruning 1% of the values (accord-\ning to their magnitude) in the weights into 0 and see the perplexity results.\nTable 5: PPL results after pruning 1% weight with different magnitude [22]\nBased on Table 5 [22] shows that the largest magnitude outliers imposed the same influence\non the model performance as the normal values. This suggests that outliers exert a similar direct\n25",
      "influence on model accuracy as regular weights, thereby indicating that isolating outliers has an\nimportant indirect impact on the overall performance of the model.",
      "Table 6: Outlier fraction distribution in different modules in BLOOM-7B under 3-sigma thresh-\nold [22]\nTable 7:  Outlier fraction distribution in different layer index in BLOOM-7B under 3-sigma\nthreshold [22]\nOn  the  outlier  distribution,  it  was  explored  the  distribution  along  different  modules  and\nlayers, and it showed that the fraction of the outliers share different patterns in different modules\nand layers, refer to Tables 6 and 7.\nAnother characteristic found was that the FFN.",
      "2 module showed a significantly higher frac-\ntion of outliers, however there was no pattern along the layer index.\nOn the quantization range, it was observed that the dynamic quantization range of different\noptimization steps and concluded that the range decreased fast in the early stages of training\nmeaning a smaller quantization range facilitating more precise quantization of parameters.",
      "g that the optimal range had already been achieved.  In deep neural networks, not all\nweights have the same influence on the model’s performance some contribute more significantly\nthan others [22]. This implies that relying solely on the magnitude of the weights is insufficient\nto fully capture the impact of each element on the model’s behavior.  A good benchmark to\ndetect parameter sensitivity is the Hessian metric.",
      "This occurs due to the fact of the Hessian\nmatrix being leveraged to assess the salience of parameters in each under-binarized layer.",
      "The H  represents the Hessian matrix of each layer and the w\ni\nwhich represents the original\nvalue of each element. The s\ni\nis then used as a criterion for assessing the weight significance of\nthe element also used as a feature indicator for a structured selection.\nStructural search selection can be implemented using unstructured selection, allowing the\nmodel to cover all salient weights.",
      "However, this approach requires an additional 1-bit bitmap\nindex  [25],  which  increases  the  average  bit-width.   This  proves  to  be  inefficient,  especially\nfor the Hessian outlier weights that are only less than 1% of the total.  According to [24] the\nmajority  of  the  weights  that  are  sensitive  Hessian  values  are  predominantly  concentrated  in\nspecific columns or rows.\nFigure 3:  The Hessian metrics (sensitivity) and magnitude (value) of weights in LLMs.",
      "[24]\nThis pattern is due to the convergence effects inherent in multi-head self-attention mecha-\nnism of the models, thus needing a structured approach to select salient weights, reducing the\nadditional bit-map. The approach described in [24] is to employ a per-channel or per row type of\nbinarization, they determine salience through a per-column segmentation on the whole matrix.\nFigure 4:  Illustration of salient weight binarization.",
      "The B1 binarized from salient weight is\nmade into a residual with the original value and then binarized again to obtain B2.[24]\nThe  main  idea  is  to  rank  the  columns  by  their  salience  in  descending  order  and  use  an\noptimized search algorithm to minimize quantization error. This process determines the optimal\nnumber of columns to include in the salient group.",
      "where W\nb\ncorresponds to the binarized output, α denotes the scaling factor and W\nf\ndenotes the\nweights at full precision (FP16).  This was then used to define the objective of the binarization\nquantization, used in this equation:\narg min\nα,B\n∥W − αB∥\n2\n,[24](1)\nwhere the B is the number of selected columns, α and B can simply be solved as α =\n∥W∥\nℓ\n1\nn×k\nand B = sign(W).",
      "Then the optimization function to select salient columns is defined as:\narg min\nW\nuns\n∥W − (α\nsal\nsign(W\nsal\n)∪ α\nuns\nsign(W\nuns\n))∥\n2\n,[24]\nwhere W\nsal\ndenotes the column-wise combination of the original weight and W\nuns\nis the\nleft non-salient part. W can be determined by W\nsal\n∪W\nuns\nso the only variable parameter is the\nnumber of rows in W\nsal\n.",
      "allenge of preserving\nsalient weights which are limited in quantity, but exhibit significant variance when aggregated.\nIf  these  weights  are  preserved  at  their  original  formats  FP16  or  INT8  it  increased  the  aver-\nage weight bit-width, reducing the compression beneficts of binarization.  However traditional\nmethods of binarization result in a substantial quantization errors.",
      "Contrary to the comprehen-\nsive high-order quantization [26] which also applies quantization to the entire weight matrix,\nthe approach described in [24] uses a residual approximation method. This approach focuses on\nbinarizing only a subset of salient weights minimizing the error through a second-order aprox-\nimation.  This method grants the precision of salient weights while simultaneously decreasing\nbit-width overhead.",
      "As shown in Figure 3 this approach incorporates a recursive computation\nstrategy for weight binarization compensation, applying a subsequent binarization process to\nthe residuals remaining after the initial binary process.",
      "Based on the equation 1 they redesigned\nthe residual approximation optimization specifically for salient weights by implementing:\n\n\n\n\n\nα\n∗\no\n,B\n∗\no\n= arg min\nα\no\n,B\no\n∥W − α\no\nB\no\n∥\n2\n,\nα\n∗\nr\n,B\n∗\nr\n= arg min\nα\nr\n,B\nr\n∥(W − α\n∗\no\nB\n∗\no\n)− α\nr\nB\nr\n∥\n2\n[24]\nthe B\no\nrepresents the original binary tensor, while B\nr\ndenotes the residual binarized matrix\nwith the same size as B\no\n.",
      "the",
      "direct one of 1. The residual binarization error was defined byE .\nE\nrb\n=∥W − α\n∗\no\nB\n∗\no\n− α\n∗\nr\nB\n∗\nr\n∥\n2\n2\n[24]\nThe original binarized quantization error is calculated as∥W − α\n∗\no\nB\n∗\no\n∥\n2\n2\n, and from the second\nsub-equation of equation 5.2.3 it’s determined that loss E\nrb\n≤ ∥W − α\n∗\no\nB\n∗\no\n∥\n2\n.",
      "Thus showing\nthat the method of residual approximation proved to be able to further reduce the binary quanti-\nzation error of salient weights with ultra-low bit-width storage compared to retaining the salient\nweights at their full precision or even INT8.",
      "The performance of a LLaMA model with this super low quantization method is still impres-\nsive, only losing about 45% of the performance of the original model on a suit of benchmarks\nconsisting of PIQA, ARC-e, ARC-c, HellaSwag and WinoGrand, as depicted on Figure 8.",
      "5.2.4    Weight + Activation Quantization\nAlthough weights are already extremely hard to quantize, incorporating the activations into\nthe quantization pipeline adds another degree of complexity, especially for large language mod-\nels. Compared to weights, activations have some unique challenges since they are dynamic, and\ntheir range and statistics are unknown until runtime.",
      "The LLMs also have a unique problem,\nnot broadly seen for other transformer-based models: the systematic outlier activations.  These\noutliers if clipped during quantization can cause significant degradation in performance and\nrequire special attention if model accuracy is to be preserved [16].",
      "the  input  activations  and\nweights of a linear layer in OPT-13B[29]\nFigure 5: Comparison of activations and weights in LLAMA2-7B and OPT-13B models.\nAnother problem that makes it hard to quantize is the significant variations in value range\nacross different channels, which can be troublesome for the quatization algorithm. However, a\nstrong motivation for undertaking this complexity is the efficiency gained by quantizing both\nweights and activations to low-bit data types on specific hardware.",
      "This efficiency is demon-\nstrated by the performance of SmoothQuant, as shown in Figure 8.\n5.2.5    Mixed Precision Quantization\nRPTQ, or Reorder-based Post-Training Quantization, differs from per-tensor quantization\ntechniques in that it does not apply the same quantization parameters uniformly across the entire\ntensor. This distinction is important, as uniform quantization can sometimes lead to suboptimal\nresults.",
      "One key problem consists of the range of quantization being too wide to cover a large value\nrange, as this can cause increased quantization errors in channels with smaller value ranges.",
      "the other hand if the quantization range is too narrow it could lead to truncation of the outliers\nresulting in quantization errors.\nFor example if one channel as a range of -100 to -50 and another 80 to 100 when trying to\ncover their ranges by quantizing them form -100 to 100 this will result in a significant quanti-\nzation error for both channels.\nTo address this researchers have proposed several methods one of them being LLM.",
      "utliers in activations and low precision data types (INT8) for the remaining values. As\nexplained above this improves model performance preventing errors caused by the quantization\nof a wide range of values.  Another method for quantizing the activation SmoothQuant [29]\nsolved the problem by introducing a process that is meant to ”smooth” the input activation by\ndividing it by a per-channel smoothing factor s∈R, C\ni\n.",
      "To keep the mathematical equivalence\nof a linear layer the weights are scaled accordingly in the reversed direction:\nY = (X diag(s)\n−1\n)· (diag(s)W) =\nˆ\nX\nˆ\nW[29]\nThe next point of SmoothQuant is called Migrate Quantization Difficulty and the idea is to con-\ntrol the trade-off between the quantization difficulty of activations and weights by redistributing\ntheir values scale, meaning migrating the difficulty from activation to weights.",
      "The idea works by choosing a per-channel smoothing factor s such that\nˆ\nX = X diag(s)\n−1\nso it’s easier to quantize.  To reduce quantization error, the effective quantization bits for all\nchannels should be increased.   This maximizes the total effective quantization bits when all\nchannels share the same maximum magnitude, making the optimal choice the scale factor s\nj\n=\nmax(|X\nj\n|), j = 1, 2,...,C\ni\nwhere j corresponds to the j-th input channel.",
      "This choice grants\nthat after the division, all the channels will have the same maximum value, which makes it easy\nto quantize.  However, this formula shifts all the quantization difficulty to the weights.  As a\nresult, the quantization errors tend to be larger in the weights, leading to significant accuracy\ndegradation shown in Figure 6.",
      "6: Finding the sweet spot for the migration strength[29]\nHowever there is a possibility off also pushing all of the quantization difficulty from the\n31",
      "weights to the activations by choosing s\nj\n=\n1\nmax(|W\nj\n|)\n.  Similarly the model performance will\ndegrade heavily due to the activation quantization errors this introduces,  therefore there is a\nneed to split all of the quantization difficulty between weights and activations so they are both\neasier to quantize.",
      "SmoothQuant achieves this by introducing a hyper-parameter migration strength, depicted\nin Figure 6, to control how much difficulty will be migrated from activation to weights, this is\ndone using:\ns\nj\n=\nmax(|X\nj\n|)\nα\nmax(|W\nj\n|)\n1−α\n[29]\nFigure 7: Main idea of SmoothQuant when α is 0.5. The smoothing factor s is obtained on cal-\nibration samples and the entire transformation is performed offline. At runtime, the activations\nare smooth without scaling.",
      "[29]\nThis formula ensures that the weights and activations at the corresponding channel share a\nsimilar maximum value, thus sharing the same difficulty[29]. Figure 7 illustrates the smoothing\ntransformation when α = 0.5, this works for models where the activation outliers aren’t very\nsignificant, on models where the outliers are more significant (e.g. GLM-130B[31] which hap-\npens to have∼ 30% outliers), a larger α can be picked to migrate more quantization difficulty\nto the weights(like 0.7).",
      "The performance results for this method are very promising on models like OPT-175B [32]\nshow an efficient quantization at INT8 quantization level since the method can match the FP16\naccuracy on all evaluation datasets.",
      "true for LLaMA [33]. Although the\noutliers in this model tend to be less severe, SmoothQuant still performed well, with an average\nperformance drop of only 0.4%. The PyTorch implementation also proved effective, achieving\na 1.51× speedup and a 1.96× memory reduction for OPT models.\n5.2.6    Quantization-Aware Training (QAT)\nLLM-QAT is an advanced method for Quantization-Aware training (QAT) specifically de-\nsigned for LLMs[14].",
      "This method as been proven to be accurate to quantization levels as low as\n4-bits. This method helps in keeping the origianl output distribution and allows the quantization\nof weights, activations, and the key-value cache.  There are three core components, Symetric\nMinMax Quantization,  Student-Teacher Framework and Data Generation Process.",
      "ric MinMax Quantization is a method used to preserve the outliers in large language models\n(LLMs) and maintain their performance.\nX\ni\nQ\n=\n\nX\ni\nR\nα\n\n,  α =\nmax(|X\nR\n|)\n2\nN−1\n− 1\n,[14]\nIn the function above the X\nQ\nrepresents the quantized values, X\nR\nrepresents the full preci-\nsion and the α is the scaling factor this is a general quantization formula and it can be applied\nto both weights and activations, but the method to quantize differs depending on the target.",
      "In\nthe case of weights, per-channel quantization is used, meaning that at each channel (or filter)\nin the weight tensor it will be quantized independently.  This approach allows the quantization\nprocess to adapt accordingly to the specific range of values in every channel, preserving more\ninformation and reducing the possible quantization error.",
      "ntization is applied.  In this case, the quantization is performed independently\nfor each token, allowing the method to account for the diverse ranges of activation values across\nthe multiple tokens. This distinction ensures that the quantization process is purposely selected\nto the specific characteristics of weights, activations, and KV cache, optimizing the trade-off\nbetween efficiency and accuracy for each case.",
      "Then  LLM-QAT uses  the  student-teacher  model framework  to  ensure  that the  quantized\nmodel retains the performance of the full-precision model.  This works by having the teacher\nmodel the full-precision version guide the student, which is the newly quantized model.  The\nguidance is provided through cross-entropy-based logits distillation.",
      "L\nCE\n=−\n1\nn\nX\nc\nn\nX\ni=1\np\nT\nc\n(X\ni\n) log(p\nS\nc\n(X\ni\n))    [14](2)\nOn Equation 2 the i represents the i-th sample in the batch , c denotes the number of classes\n(vocabulary  size),  and T  and S  are  the  Teacher  and  the  Student  models,  respectively.   The\nnext-token data generation is based on the full-precision model and is proposed as a method to\nsynthesize a distribution similar to the pre-training data.",
      "This data is generated by the teacher\nmodel,  which  begins  with  a  random  start  token  and  iteratively  generates  subsequent  tokens\nuntil  it  reaches  the  end  of  the  sentence  or  the  maximum  sequence  length.",
      "from the full precision SoftMax output distribution.  Lastly, the\ngenerated  data  is  then  used  as  input  for  fine-tuning  the  quantized  model,  where  the  teacher\nmodel’s predictions serve as labels to guide training thus achieving a performance close to the\noriginal model yet quantized to a specified level.\n33",
      "5.3    Retrieval-Augmented Generation (RAG)\nLarge language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [34] this is accomplished without access to any outside data [5] this comes with\nsome pros and cons.  On the pros side, there’s the ability of the model to capture a lot of data\nand compress it, on the other hand, this comes with the downside of not being able to expand\nthe model knowledge or even revise their memory.",
      "Another downside is the hallucinations that\nsometimes are produced by this models. These limitations can be addressed using a method pro-\nposed by [35], known as Retrieval-Augmented Generation (RAG). This method consists of four\nmain components:  a query encoder (q), a retriever (p\nn\n), a document indexer, and a generator\n(p\n0\n).\nFigure 8: RAG implementation overview[35]\nThe first required model is a retriver DPR based on [36].",
      "This retriver works by indexing all\nthe passages in a low-dimensional and continuous space, so that later the top K passages can\nbe retrived efficiently. At run-time, passages that are relevant to the input question are retrieved\nfor the reader module.  The number of passages from which the model can select is extremely\nlarge the paper refers to a corpus of 21 million passages, with the value of K (i.e.",
      ") ranging between 20 and 100.\nDPR represented by p\nn\n(z|x) follows a bi-encoder architecture:\np\nη\n(z | x)∝ exp(d(z)⊤q(X)),  d(z) = BERT\nd\n(z),  q(x) = BERT\nq\n(x)    [35]\nWhere d(z)  is  a  dense  representation  of  a  document  produced  by  a BERT\nBASE\ndocument\nencoder [12] and q(x) a query representation produced by a query encoder, in this case also\nusing BERT\nBASE\n.",
      "The  retrieval  is  done  using  (MIPS)  Maximum  Inner  Product  Search  to\ncalculate the top-k(p\nη\n(· | x)), which represents the list of k  documents z  with highest prior\nprobability p\nn\n(z|x).  MIPS identifies these documents by finding those with the largest inner\nproduct between their dense representations and the query representation.",
      "This process can be\napproximately solved in sub-linear time using an efficient approximation to the nearest neighbor\nsearch, like FAISS or hierarchical navigable small-world graphs.  This is crucial for enabling\nthe scalability of large document collections such as Wikipedia.",
      "ument index.  The Generator is used to combine the input x with the retrieved content z using\nBART, these are simply concatenated.\nThe generator component given by p (y\ni\n| x,z,y\n1:i−1\n) could be modeled using any encoder\ndecoder, however BART-large [37] a pre-trained seq2seq transformer with 400M parameters, is\ncommonly used.\nBART pre trainned using a denoising objective which served as its foundation.",
      "encourage contextual understanding [38].\nThey refer to the BART Generator parameters 0 as the parametric memory and all of the\nretrieved external knowledge as non-parametric knowledge.  The marginalization can be done\nvia two  methods described  in the  paper RAG-Sequence  model [39]  uses the  same retrieved\ndocument to generate the entire output sequence.",
      "Specifically, it treats the retrieved document\nas a single latent variable, which is marginalized to obtain the sequence-to-sequence probability\np(y—x) using a top-K approximation.",
      "p\nRAG-Sequence\n(y|x)≈\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)p\nθ\n(y|x,z)\n=\nX\nz∈top-k(p(·|x))\np\nη\n(z|x)\nN\nY\ni=1\np\nθ\n(y\ni\n|x,z,y\n1:i−1\n)\n[35](3)\nThis marginalization allows the model to combine information from the topk  document,\neffectively converging the information from diverse sources within the same document to gen-\nerate a coherent and contextually accurate output.",
      "The final retrieval step ensures that the most\nrelevant documents are selected based on their dense representations,\nThe second method is RAG-Token model which can be used to draw a different latent doc-\nument for each target token and marginalize accordingly.  This makes it possible for the gen-\nerator to choose the content from several documents when producing the answer.  The top-K\ndocuments are retrieved using a retriever.",
      "The generator then produces a probability distribu-\ntion for the next output token for each retrieved document.",
      "5]\nIn the case for sequence classification tasks RAG-Sequence and RAG-Token can be used by\nconsidering the target class as a target sequence of length one.\n35",
      "AspectRAG-TokenRAG-Sequence\nDocument UsageDifferent  documents  for  each  to-\nken.\nSame document for the entire se-\nquence.\nMarginalizationPer-token over top-K documents.Per-sequence   over   top-K   docu-\nments.\nBeam SearchStandard beam search.Beam search for each document.\nEfficiencyComputationally efficient.Thorough Decoding is expensive\nFlexibilityCombines information from multi-\nple documents dynamically.\nRelies on a single document for the\nentire sequence.",
      "Table 9: Comparison of Decoding Methods\n5.3.1    Hypothetical Document Embeddings (HyDE)\nHyDE [40] is another retrieval method that aims to increase the performance of the model\non zero-shot scenarios,  meaning that they can retrieve relevant documents without requiring\nspecific training.   This model is meant to work with any type of NLP model and is able to\ngeneralize across multiple tasks.",
      "This method differs from RAG [35] in that it uses a generator\nto produce a hypothetical document based on the input query.  This document does not need\nto be factually correct, as it is only used by the retriever (e.g., Contriever), which transforms\nit into a dense embedding vector.  This embedding represents the hypothetical document in a\nhigh-dimensional vector space.",
      "The retriever is then used to search the corpus for real documents that are similar in the\nembedding space,  this similarity is measured using inner product similarity between the hy-\npothetical document embeddings and the real document embeddings.",
      "similar\ndocument is fed into the model which also receives the input query, it then generates the output\nfor the query based on the retrieved document.\nFigure 9: An illustration of the HyDE model.[40]\nThe main issue addressed by [40] is the dependence on a separate query encoder required\n36",
      "by RAG [35] systems.  This is because dense retrievers compute similarity between the query\nand documents using inner product similarity, which necessitates a dedicated query encoder.\nFirstly it uses two encoder enc\nq\nenc\nd\nthat maps the query q and the document d into d di-\nmension vectors v\nq\n,v\nd\n, whose inner product is used as the similarity measurement.",
      "sim(q,d) =⟨enc\nq\n(q), enc\nd\n(d)⟩ =⟨v\nq\n,v\nd\n⟩    [40]\nThis is where zero-shot dense retrieval problems lie, it requires learning two embedding func-\ntions one for the query and the other for the document, these need to align into the same em-\nbedding space where the inner product can capture the document’s relevance. HyDE solves this\nproblem by performing a search in the document-only embedding space that captures the doc-\nument’s similarity.",
      "This method can be easily learned using unsupervised contrastive learning\n[41]. They set the enc\nd\ndirectly as the contrastive encoder enc\nc\non as follows:\nf = enc\nd\n= enc\nc\non    [40](4)\nThis  unsupervised  contrastive  encoder  is  be  shared  by  all  incoming  document  corpus.   The\nfunction 4 is also denoted as f .",
      "they call it InstructLM so it’s easy to represent. It then takes the query q and a textual\ninstruction INST  and follows them to perform the task specified in INST , like so:\nTo build the query vector, they use an instruction-following LLM in this case, text-davinci-\n003 from OpenAI’s GPT-3 series[42]. This model was specifically chosen for its strong gener-\nalization capabilities and is referred to as InstructLM  for ease of representation.",
      "It takes the\nquery q and a textual instruction INST , and follows the instruction to perform the specified\ntask, as shown below:\ng(q, INST) = InstructLM(q, INST)    [40]\nThe g can be used to map queries to the hypothetical documents by sampling from g, the INST\nis set to be ”write a paragraph that answers the question”, the generated document isn’t real and\nmay be factually incorrect due to models hallucinations [42].",
      "However, this is not important\nbecause  the  hypothetical  document  is  used  only  to  capture  the  relevance  pattern.   Then  the\nrelevance modeling is offloaded to an NLG that has the ability to generalize more easily, nat-\nurally,  and more effectively.",
      "query-document relevance.\nE[v\nq\nij\n] = E[f(g(q\nij\n, INST\ni\n))]    [40](5)\nThe f  corresponds to the document encoder, g defines a probability distribution based on the\nchain  rule.   In  their  implementation,  they  assume  that  the  distribution  of v\nq\nij  is  uni-modal,\nimplying that the query is not ambiguous.  To estimate Equation 5, they sample N  docume",
      "nts\nfrom g,\nh\nˆ\nd\n1\n,\nˆ\nd\n2\n,...,\nˆ\nd\nN\ni\n.\nˆv\nq\nij\n=\n1\nN\nX\nˆ\nd\nk\n∼g(q\nij\n,INST\ni\n)\nf(d\nk\n)\n=\n1\nN\nN\nX\nk=1\nf(\nˆ\nd\nk\n)    [40]\nThey also consider the query as a possible hypothesis,\nˆv\nq\nij\n=\n1\nN + 1\n\"\nN\nX\nk=1\nf(\nˆ\nd\nk\n) + f(q\nij\n)\n#\n[40]\nThe inner product is computed between ˆv\nqij\nand the set of all document vectors{f(d)|d∈ D\ni\n},\nthen the most similar documents are retrieved.",
      "In their implementation, the encoder function\nf acts as a lossy compressor, producing dense vectors in which unnecessary details are filtered\nout.  This enables the system to use a generated document even if it lacks factual accuracy to\neffectively search for the correct one.\nAccording to their study,  HyDE remains competitive even when compared to fine-tuned\nmodels.",
      "Another strong result comes from the web research setting, where the performance\nof  HyDE  is  particularly  impressive  even  when  compared  to  methods  that  rely  on  relevance\njudgments. Notably, HyDE achieves these results without requiring such judgments.\nTable  10:  Results  for  web  search  on  DL19/20.   Best  performing  w/o  relevance  and  overall\nsystem(s) are marked bold.  DPR, ANCE and ContrieverFT are in-domain supervised models\nthat are finetuned on MS MARCO training data.",
      "5.3.2    Cache Augmented Generation\nRAG is the most used approach for enhancing language models but as discussed previously\nit has two main drawbacks, specifically the retrieval latency and the potential errors in document\nselection [43].  LLMs have been increasing their context size over the years and the CAG [43]\napproach proposes a method that uses this increased context size to reduce the model",
      "latency\nand potential errors that could occur on RAG systems.\nFigure 10: Comparison RAG on the top and CAG on the botom[43]\nThis  approach  works  by  enabling  retrieval-free  knowledge  integration.   This  is  done  by\npreloading external knowledge sources, such as a collection of documents D ={d\n1\n,d\n2\n,...,d\nn\n}\nand precomputing these documents key-value (KV) cache C\nK\nV ,  this addresses some of the\ncomputational challenges and inefficiencies inherent to real-time retrieval on RAG systems.",
      "This approach is divided into three main steps:\nExternal Knowledge Preloading, represents the adaptation and preprocessing of the collec-\ntion of documents D that are relevant to the target application, this adapts them to fit within the\nmodel’s context window.",
      "This is done by the LLM M , with parameters 0, and processed the\ndocuments D, thus transforming it into a precomputed KV cache:\nC\nK\nV = KV − ENCODE(D)    [43]\nThis KV  cache which contains the inference state of the LLM, is stored on disk or in memory\nfor future use.  This implementation brings some of the computational cost down because the\ncost of processing D is incurred only once even if used in multiple subsequent queries.",
      "then the LLM uses this cached context to generate the responses:\nR = M(Q|C\nK\nV )    [43]\nBy giving the model the external cached knowledge it eliminates the retrieval latency and re-\nduces  the  risk  of  errors  or  omissions  that  comes  with  dynamic  retrieval.   The  prompt P =\nConcat(D,Q) ensures a unified understanding of the user query and the external knowle",
      "dge.\nCache Reset is the part of the system is responsible for maintaining performance across\nmultiple inference sessions. Since the KV  cache grows in an append-only manner sequentially\nstoring new tokens t\n1\n,t, 2,...,t\nk\nthe context may eventually need to be freed.  This is achieved\nby truncating the oldest tokens to prevent the cache from exceeding memory limits.\nC\nreset\nKV\n= Truncate(C\nKV\n,t\n1\n,t\n2\n,...",
      ",t\nk\n)    [43]\nThis ensures that the re-initialization is fast without the need to reload it from disk, ensuring a\nconstant speed and responsiveness.\n5.3.",
      "3    Hybrid approaches\nThere are two main Hybrid approaches focusing on different implementations RAGCache\n[44] and Self-Route [45] though these can’t be compared directly in terms of their implemen-\ntation since RAGCache is a system-level optimization for RAG and Self-Route is an approach\nthat selects the optimal way to give context to the model.",
      "Starting with Self-Route, this method combines the benefits of Retrieval-Augmented Gen-\neration (RAG) notably its proven effectiveness and efficiency in leveraging external knowledge\nwith the capabilities of recent long-context (LC) models, which can directly process and un-\nderstand extended contexts.  Models like Gemini 1.5 Pro [46] is able to achieve near-perfect\nrecall of up to 1M tokens and maintains this recall performance of up to 10M tokens.",
      "If this\ntrend of bigger and bigger context sizes continues this method could be a big improvement over\ntraditional RAG implementations.",
      "ions.  However, RAG\nis significantly more efficient, with its cost estimated to be around 20% of that required by LC\nmodels.\n40",
      "Figure 11: Comparison between performance and costs on multiple models using LC, RAG and\nSelf-Route[45]\nAs seen in Figure 11 the performance is maintained if not improved on some models but the\ncost on most cases is less than half. This is due to the nature of the approach, which combines\nthe strengths of both methods. When RAG can be applied, it offers reduced computational costs\nwhile maintaining most of the performance.",
      "However, despite the performance gap, there is a\nhigh degree of overlap in the predictions made by both methods.\nFigure 12: Distribution of the difference of prediction scores between RAG and LC[45]\nFigure 12 shows the differences between RAG prediction scores S\nRAG\nand LC prediction\nscores S\nLC\nthese are not just similar, in 63% of queries the model predictions are exactly identi-\ncal, and for 70% of queries, the score difference is less than 10%.",
      "This is also true for incorrect\nanswers when looking at the red color which corresponds to an accuracy of 0 it can be seen that\nRAG and LC make similar errors as well.\nSelf-Route  uses  the  LLM  itself  to  route  queries  based  on  self-reflection,  under  the  idea\nthat LLMs are well-calibrated in predicting whether a query is answerable given the provided\ncontext, this is done using a two-step approach RAG-and-Route and long-context prediction.",
      "RAG-and-Route works by providing the original query along with the retrieved text chunks\nto the LLM, prompting it to assess whether the query is answerable based on the given context.\nIf the mo",
      "del determines that the query can be answered,  it proceeds to generate a response.\nHowever, if it deems the query unanswerable, it is instructed to return the phrase ”unanswer-\nable” as per the prompt:  ”Write ’unanswerable’ if the query cannot be answered based on the\nprovided text.” In such cases, a fallback approach is then triggered.",
      "This approach consists of giving the LLM the full LC which consists of all documents able\nto fit the context , although this is not explained in the paper, it gives some hints that this must be\nthe case. This as seen on Figure11 giving a better result although with a higher cost. This trade-\noff is most cost-efficient when k = 5, meaning the number of retrieved documents is five.",
      "This\nis because, as k increases, the cost of RAG also increases, but so does the number of queries\nthat can be successfully routed.  Ultimately, the efficiency depends on the specific task being\nevaluated.  For instance, in extractive question answering tasks where multi-hop reasoning is\nnot required a lower k (e.g., k = 1) may result in lower computational cost.  Conversely, tasks\nthat require deeper reasoning may benefit from a higher k.",
      "Therefore, the optimal value of k is\ndependent on both the nature of the task and the level of performance required.\nFigure  13:  Trade-off  curves  between  (a)  model  performance  and  (b)  token  percentage  as  a\nfunction of k.[45]\nThe other hybrid approach RAGCache works by caching the key-value tensors of retrieved\ndocuments across multiple requests, this is done to try and minimize redundant computation for\nefficiency gains.",
      "Figure 14: RAGCache Overview[44]\nCache Structure and",
      "Replacement Policy operate differently from traditional cache systems\nthat cache individual objects.  Instead, this method caches the key-value tensor of the retrieved\ndocuments, which are sensitive to the order in which they are referenced. For instance consider\ntwo document sequences [D\n1\n,D\n3\n] with key-value tensors KV  and [D\n2\n,D\n3\n] with KV\n′\nrespec-\ntively.  Although KV [1] and KV\n′\n[1] both contain D\n3\n, their value differs.",
      "This occurs because\nthe key-value tensor of a given token being generated based on the preceding tokens, thus un-\nderscoring the order-dependence of key-value tensors. To aid retrieval speed while maintaining\ndocument order, RAGCache structures the document’s key-value tensors with a knowledge tree\n15.  The knowledge tree assigns each document to a node that refers to the memory addresses\nof the document’s key-value tensors.",
      "Similarly to vLLM [47], RAGCache also stores key-value\ntensors in non-contiguous memory blocks, allowing the KV cache to be reused.  The root of\nthe tree, S, corresponds to the shared system prompt.  A path from the root to any node rep-\nresents a unique sequence of documents, thus allowing RAGCache to handle multiple requests\nsimultaneously by leveraging overlapping paths.\nRAGCache retrieves tensors by performing prefix matching along these paths.",
      "If a subse-\nquent document in a sequence cannot be found among the child nodes, the traversal is termi-\nnated, and the longest identified document sequence is returned. This method ensures efficiency\nwith a time complexity of O(h), where h is the height of the tree.",
      "PGDSF) replacement policy is the name for the\n43",
      "node placement optimizer.  The knowledge tree decides each node’s placement within a hier-\narchical  cache.   For  example,  nodes  that  are  constantly  accessed  are  ideally  stored  in  GPU\nmemory, while those that are accessed less often are stored in slower host memory or are freed\ncompletely.",
      "The node placement optimization occurs when RAGCache uses a PGDSF replace-\nment policy that evaluates each node based on the access frequency, size, and access cost, due\nto its limited storing capacity the priority is defined by:\nPriority = Clock +\nFrequency× Cost\nSize\n[44](6)\nNodes that have a lower priority get freed first while the opposite is also true.",
      "The Clock\ntracks node access frequency, but to adapt to the cache hierarchy, there are two separate logical\nclocks: one for the GPU and another for the host memory. The Clock starts at zero and updates\nevery eviction, when a document is retrieved its clock is set and its priority gets adjusted this\nimposes that nodes with older clocks meaning less recent use, receive lower priorities.",
      "Clock = max\nn∈E\nPriority(n)    [44]\nFrequency in Equation 6 represents the total retrieval count for a document within a time\nframe, this count is reset upon system start or cache clearance. Priority is directly linked to\nthe frequency so the more frequently a document is accessed the higher the priority. Size is the\nnumber of tokens in the given document post-tokenization, thus directly linked to the memory\nrequired for its key-value tensors.",
      "PU performance as well as document\nsize and the sequence of preceding documents.\nFigure 16: Cost estimation PGDSF[44]\nPrefix awareness for RAG is achieved by the PGDSF method through two primary com-\nponents, cost estimation and node placement.  Accurately determining the computational cost\nfor RAG operations is challenging due to the complex dynamics of LLM generation.",
      "Figure\n16 illustrates this challenge by showing the cost differences for an identical request, denoted as\n[S,D\n1\n,D\n2\n,Q], under different caching conditions.",
      "ily on the cache prefix.  For instance,  if the prefix [S,D\n1\n] is already cached,  the subsequent\ncomputational cost includes the generation of key-value tensors for both D\n1\nand Q.  Making\nit extremely difficult to isolate the cost attributed solely to D\n2\n.",
      "To address this issue, PGDSF\nreplaces the Cost/Size term in Formula 6 with the following Equation:\nCost Size =\n1\nm\nm\nX\ni=1\nCost\ni\nNewSize\ni\n[44](7)\nIn Equation 7, m represents the number of requests that access a document not currently\nin cache.  The term Cost\ni\n/NewSize\ni\ndenotes the compute time per non-cached token for the\ni-th request.",
      "This approach effectively amortizes the computational cost across all non-cached\ntokens, thereby incorporating the document’s size into the priority calculation. The cost, Cost\ni\n, is determined through an offline profiling process where RAGCache measures the LLM prefill\ntime for multiple combinations of cached and non cached token lengths.",
      "en request at runtime. Each document\nretrieval event triggers an update to the corresponding node’s frequency, its cost estimation, and\nthe clock mechanism within the knowledge tree.  Furthermore, if a retrieved document is not\nalready in the cache, a new node is created for it in the tree.\nThe  management  of  the  node  placement  on  the  GPU,  host,  or  free  is  performed  by  the\nPGDSF as seen on Figure 15.",
      "The nodes in GPU serve as parent nodes to those in host memory,\nestablishing a hierarchical structure.  RAGCache also manages node eviction across these two\nsegments for efficiency, which is especially true when GPU memory is full. When this occurs,\nRAGCache swaps the lowest-priority node in the leaf nodes to the host memory; this process\nalso occurs on the host memory when it is full,  though in that case,  it is an eviction.",
      "This\nstrategy takes into account the Knowledge tree hierarchical partitioning, which is one key point\nto align with memory sensitivity and prefix sensitivity in LLM generation.  Due to the node\nneeding its parent node for key-value tensor calculation, the required placement of the parent\nnode is prioritized this is so rapid retrieval can be achieved.",
      "Because of PCIe limitations when connecting the GPU with the host memory in comparison\nwith GPU HBM, RAGCache adopts a swap-only-once strategy depicted in Figure 15, where\nyou can see that the key-value tensors of a node are swapped out to the host memory only for\nthe first eviction.  The host memory is responsible for keeping the key-value tensors until the\nnode is fully evicted from the entire cache.",
      "che directly frees the node node without copying any data Due to the size of the host\nmemory being two orders of magnitude larger than the GPU memory, keeping one copy of the\nkey-value tensors in the host memory is acceptable.\n45",
      "Figure 17: Cache aware Reordering[44]\nCache hit rate is vital for RAG Cache’s cache efficiency, but when paired with the unpre-\ndictability of the arrival pattern in user requests, this results in substantial cache trashing.\nRequests that refer to the same document may not be issued on the same time frame thus af-\nfecting the cache efficiency. For example given the requests{Q\ni\n,i%2 == 0} and{Q\ni\n,i%2 ==\n1} that target the documents D\n1\nand D\n2\nrespectively.",
      "When the cache capacity is only one\ndocument, the sequence{Q\n1\n,Q\n2\n,Q\n3\n} causes frequent swapping of the key-value cache of D\n1\nand D\n2\n, making it a zero cache hit rate.  But if a bit of attention is paid to rearrange requests\nto{Q\n1\n,Q\n3\n,Q\n5\n,Q\n2\n,Q\n4\n,Q\n6\n,Q\n6\n,...} this achieves a cache hit rate of 66% thus optimizing cache\nutilization. This shows how strategic request ordering can mitigate cache volatility and improve\ncache efficiency.",
      "To introduce the cache-aware reordering algorithm, two scenarios were con-\nsidered to show the key insights, the recomputation cost was assumed to be proportional to the\nrecomputation length. The first scenario is shown on Figure 17, (a) where it considers requests\nwith identical recomputation demands but varying cached context lengths with a limit of four\ncached documents.",
      "rocessing effectively uses Q\n1\n’s cache\nwhile discarding Q\n2\n’s, resulting in a computational cost of 2 + 1 + 2 = 5.  On the other hand,\nif the order was given as Q\n2\n,Q\n1\nthis would result in a usage of Q\n2\n’s cache but discarding Q\n1\n’s,\nwhich would increase computation to six due to 2+2+2 = 6. This is why cache-aware reorder-\ning advocates to prioritize requests with larger cached context thus improving cache efficiency\nas this brings larger benefits.",
      "In the second scenario (b), the aim was to examine requests with\nsimilar cached context lengths but varying recomputation demands, with a cache capacity of\nfive documents. On a sequence{Q\n1\n,Q\n2\n}, the system must clear Q\n′\n2\ns cache to allocate space for\nQ\n′\n1\ns computation, given only one available memory slot. This makes it necessary to recompute\nQ\n2\nentirely, which results in a cost of 2 + 2 + 1 = 5.",
      "On the other hand the sequence{Q\n2\n,Q\n1\n}\nallows for direct computation of Q\n2\n, due to adequate cache availability. It also reduces the total\ncomputation cost to 2 + 1 = 3, thus the reason why cache-aware reordering is beneficial when\nit prioritizes requests with shorter recomputation segments, this way results in a minimization\nof the adverse side effects on cache efficiency.",
      "Equation 8, directly prioritizes the requests that will probably lead to enhanced cache ef-\nficiency.   This is directly linked to the increase in the cache hit rate an",
      "d the decreased total\ncomputation time of RAGCache.  Model performance and resource usage are also improved\nthanks to this implementation. To avoid possible starvation when requests don’t align with the\ncached documents RAGCache sets a window for each request to ensure that all requests are\nprocessed in a timely manner.",
      "On an LLM enhanced with RAG, the key performance bottleneck is usually the LLM gen-\neration, however, if the vector database grows to a larger scale or a higher accuracy is needed in\nthe retrieval, this may cause the retrieval step to incur a substantial latency.",
      "To control the impact of retrieval latency, RAGCache employs dynamic speculative pipelin-\ning to overlap knowledge retrieval and LLM inference, and one thing that can occur is that the\nvector search may produce results earlier in the retrieval step, which can be used by the LLM\nfor speculative generation ahead of time.  This works by a vector search maintaining a queue\nof top-k candidate documents, which are ranked based on their similarity to the request.",
      "Dur-\ning the retrieval process the top-k documents are being constantly updated this is done so that\ndocuments that still being discovered, might come with greater similarity and so they get in-\nserted into the top-k. What could also occur is that the final documents may emerge early in the\nretrieval step [48].  Based on that RAGCache introduced a speculative pipelining strategy that\nsplits a request’s retrieval process into several stages.",
      "will start a new speculative generation and terminate the\nprevious one, if that doesn’t happen then the LLM engine just continues with the generation.\nWhen the top-k documents are finalized and there are no more changes to the top-k these are\nsent by RAGCache to the LLM engine and if they match with the ones previously received the\nengine simply returns the latest speculative generation.  Otherwise, the LLM performs a new\ngeneration with the new top-k documents.",
      "Figure 18: Speculative Pipelining[44]\nFigure 18 shows how RAGCache splits the retrieval process into four stages.   The top-2\ndocuments in candidate queue are [D\n1\n,D\n3\n], [D\n1\n,D\n2\n], [D\n1\n,D\n2\n] and [D\n1\n,D\n2\n] in the four stages.\nAfter the first stage is concluded, RAGCache sends [D\n1\n,D\n3\n] to the the LLM engine for specu-\nlative generation.",
      "the case it terminates the previous speculative generation and starts a new one with the correct\ndocuments. In stage three, the LLM engine receives the exact same documents as for stage two,\nso it continues with the previously started speculative generation. In the last stage, RAGCache\nsends the final top-2 documents to the engine which are still the exact same as the ones in stage\ntwo so there is no change to the speculative generation which is directly returned by the LLM\nengine as the result.",
      "The speculative pipelining allows RAGCache to overlap the retrieval and generation steps,\nand this greatly improves the end-to-end latency of RAG systems.",
      "is can introduce a\nlot of extra steps in the computation of the engine response.  This can be seen above Figure\n18, some speculative generations are incorrect and need to be recalculated.  This can lead to\nperformance degradation under high system loads,  but to solve this RAGCache dynamically\nenables speculative pipelining based on the system load. As an example, they assumed that the\nvector search and the LLM both serve only one request at a time.",
      "This vector search produces\ncandidate retrieval results at the end of each stage with a fixed time interval d.  Since the batch\nsize was set to only one, they could terminate any incorrect speculative generation requests.\nFigure 19: Optimal speculative pipelining strategy [44]\nRAGCache assumes that the LLM engine can schedule requests in the queue in any order,\nbut  it  processes  speculative  generation  requests  for  a  single  request  sequentially.",
      "Figure  19\nillustrates the optimal speculative pipelining strategy under this setting.\n5.4    Challenges and Applications of Quantization in RAG\n5.5    Retrieval Methods to enhance HyDE\nDue to the approach taken by RAG and HyDE on how they retrieve documents, these meth-\nods could be adapted or specifically chosen based on their ability in certain tasks.",
      "Retrievers\nare the basis for content that was retrieved from an external document corpus to enhance the\nLLM output, as well as provide grounds for the generated information on accurate documents.\nA more in-depth research will be done about some of these retrievers.",
      "5.5.1    Contriever\nThe original implementation of HyDE uses the Contriever model as its retriever.  This ap-\nproach w",
      "orks on the basis of contrastive learning, which is based on the fact that every document\nis, in some way, unique. According to [49], this is the only available information in the absence\nof manual supervision. A contrastive loss is used to learn by discriminating between documents.\nThis loss compares either a positive loss when they are the same document or negative when\nit’s from different documents.",
      "The formula responsible for this is:\nL(q,k\n+\n) =−\nexp (s(q,k\n+\n)/τ)\nexp\n\ns(q,k\n+\n)\nτ\n\n+\nP\nK\ni=1\nexp\n\ns(q,k\ni\n)\nτ\n\n′\n[49](9)\nIn Equation 9, q corresponds to the given query, which has an associated positive document\nk+, and a pool of negative documents(k\ni\n)\ni=1..k\n, τ  is the temperature parameter used to adjust\nthe sensitivity of the Contriver.  This function’s construction encourages positive pairs to have\nhigh scores and negative pairs to have low scores.",
      "One crucial piece of this method is how to\nbuild positive pairs from a single input. This could be done in two main ways: the Inverse Cloze\nTask or Independent cropping.\nThe  usage  of  the  Inverse  Cloze  Task  is  a  data  augmentation  strategy  that  generates  two\nmutually exclusive views of a document.  This approach was first described in [50].",
      "The first\nview is obtained by randomly sampling a span of tokens from a segment of text, and the second\nview is obtained by using the complement of the span.  This is done by in a given sequence of\ntext (w\n1\n,...,w\nn\n), ICT samples a span (w\na\n,...,w\nb\n), where 1 ≤ a ≤ b ≤ n, and then uses the\ntokens of the span as the query and the complement (w\n1\n,...,w\na−1\n,w\nb+1\n,...,w\nn\n) as the key.",
      "l for matching the query with the document directly.\nThis method is commonly used on images where multiple views are generated independently\nby cropping the input. Since this implementation is used for text it is done by sampling a span\nof tokens.  Because of the importance of the positive pairs this strategy samples independently\ntwo spans from a document to form the needed pair.",
      "Contrary to the inverse Cloze task in the\ncropping stage both views of the example correspond to the same contiguous subsequence of the\noriginal data. Another difference is that between cropping and ICT is that independent random\ncropping is symmetric meaning both of the queries and documents follow the same distribution.\nThis also causes overlap between the two views of the data, this being one of the reasons that\nencourages the network to learn exact matches between query and document.",
      "This works very\nsimilar to how lexical matching methods function, BM25 being a great example of this. So you\ncould either fix the length of the span for the query and key or sample them both.\nA big part of contrastive learning is how the system handles negative pairs this includes\nsampling a large set of negatives. This was tested by [49] using two methods, in-batch negative\nsampling and MoCo.",
      "batch.  For example, each item in the batch is transformed twice to generate the positive pairs,\nand the negatives are generated by using the other examples views from the batch, they called\nthese ”in-batch negatives”.  In this specific case, the gradient is back-propagated through the\nrepres",
      "entations  of  both  the  queries  and  the  keys.   The  main  downside  to  this  method  is  the\nrequirement for extremely large batch sizes to work well.\nThe other approach Negative pairs across batches tries to solve the problem by storing the\nrepresentations from previous batches in a queue and using these as negative examples in the\nloss calculation.  This makes it possible to have a smaller batch size but may slightly change\nthe loss by making it asymmetric between the queries.",
      "This being the view generated from\nthe elements of the current batch, and the keys, which are the elements already stored in the\nqueue.  This occurs as the gradient is only back-propagated through the queries,  leaving the\nrepresentation of the keys fixed. This is caused by the features being already stored in the queue\nfrom prior batches coming from past interactions with the network. A problem occurs when the\nnetwork is rapidly evolving during training can cause the performance to drop.",
      "Instead, they used the approach called MoCo [51] which generates representation keys from\na second network that is updated more slowly, the two networks are as follows one is responsible\nfor the keys, parametrized by 0\nk\n, and another network for the query, parametrized by 0\nq\n.  The\nparameter for the query network gets updated using back-propagation and stochastic gradient\ndescent.   This works similarly to when in-batch negatives are used.",
      "On the other hand,  the\nkey network also called Momentum encoder is only updated from the parameters of the query\nnetwork using an exponential moving average.\n5.",
      "ses the few-shot prompts to make the LLM judge if it knows the answer or\nnot.  In the case that the answer isn’t known, SKR [52] proceeds to the retrieval using RAG to\nimprove on the model response.\nTheir method works by three main ways: Collecting Self-Knowledge, Eliciting Self-knowledge,\nand Using Self-Knowledge. These represent the main pipeline for SKR, as shown in Figure 20.\nFigure 20: The SKR Pipeline and its component interactions.[52]\n50",
      "5.6.1    Collecting Self-Knowledge\nGiven a dataset D with question-answer pairs{q\nj\n,a\nj\n}\n|D|\nj=1\n, the model M is used to generate\nthe answers for each entry q\ni\n:\nˆa(M,q\ni\n) =M(q\n1\n◦ a\n1\n,...,q\nd\n◦ a\nd\n,q\ni\n)    [52](10)\nWhere ◦ denotes the concatenation and {q\nj\n◦ a\nj\n}\nd\nj=1\nare d demonstrations.   Equation 10\nrepresents the generated answer with ˆa(M,q\ni\n), this also represents the internal knowledge to the\nquestion q\ni\nin M .",
      "The other approach is to possibly find passages from external resources that\nmay be related with said question q\ni\n, these passages can then be used as additional information\nprovided on the model input. This is done per query, using a pretrained retriever represented by\nR to find the related information from the corpus C:\np\ni\n={p\ni1\n,p\ni2\n,...,p\nik\n} =R(q\ni\n,C),[52](11)\nAccording to Equation 11 the top-k retrieved passages for the question q\ni\nare represented\nby p\ni\n= {p\ni1\n,p\ni2\n,...,p\nik\n}.",
      "A dense passage retriever is used [36] for R, and C  consists of\npassage chunks from Wikipedia.  Then they use M  again to generate the answer with retrieval\naugmentation:\nˆa\nR\n(M,q\ni\n) =M(q\n1\n◦ p\n1\n◦ a\n1\n,...,q\nd\n◦ p\nd\n◦ a\nd\n,q\ni\n◦ p\ni\n).",
      "s ˆa(M,q\ni\n), ˆa\nR\n(M,q\ni\n) 12, and the ground-truth answer a\ni\n, they can cat-\negorize each question into a positive subset D\n+\nand a negative sub-set D\n−\nusing the differences\nbetween the results:\nq\ni\n∈\n\n\n\nD\n+\n,    if E[ˆa(M,q\ni\n)]≥ E[ˆa\nR\n(M,q\ni\n)];\nD\n−\n,    otherwise,\n[52]\nIn Equation 5.6.1 E is an evaluation metric like accuracy and exact match score, but they\nget discarded if the question q\ni\n, answer ˆa(M,q\ni\n) and ˆa\nR\n(M,q\ni\n) are incorrect.",
      "They then split\nthe training set into a subset D\n+\n= {q\n+\ni\n,...,q\n+\nm\n} which include questions that M  can directly\ngive correct answers to without external knowledge R and the other subset D\n−\n={q\n−\n1\n,...,q\n−\nn\n}\nwhere the R is needed for more accurate results.\n5.6.2    Eliciting Self-Knowledge of LLMs\nThe four different strategies proposed to detect the self knowledge of the target questions\nare direct prompting, in-context learning, training classifier, and nearest neighbor search.",
      "These\nwork by on the first two using the LLM itself and the latter two using smaller nodes purposely\nbuilt.",
      "Direct Prompting given a question q\nt\n, a straight-forward approach to detect wether LLMs\nare capable of solving it is to ask them directly:\nFigure 21: Direct Prompting [52]\nOn this method the prompt is used in conjunction with ’Do you need additional information\nto answer this question?’ to detect self-knowledge based on the response provided by the LLM.\nThis approach results in direct prompting the model and it may work.",
      "Although this doesn’t\nuse any of the collected training questions shown previously.  To improve on that 3 different\nstrategies where created.",
      "estions where selected from D\n+\nand D\n−\nas demonstrations to\nshow the self-knowledge of the question q\nt\n:\nFigure 22: In-Context Learning [52]\n52",
      "In here answer templates where used, ”No, I don’t need...” or ”Yes, I need...” in demonstra-\ntions based on wether the answer comes from the positive set D\n+\nor the negative one D\n−\n.\nThis direct prompting and in-context learning methods can induce self-knowledge of LLMs\nto some extent.  But they come with three main drawbacks.  First one being that both methods\nrequire designing prompts and calling the LLMs for each new question, making it cumbersome.",
      "Second, in-context learning could be also unstable due to contextual bias and sensitivity which\nis difficult to address in closed source LLMs. Third, use of all questions cannot be guaranteed,\ndue to the maximum tokens input of the LLMs. To avoid the above issues smaller models were\nused to help elicit self-knowledge.",
      "A classifier was trained using D\n+\nand D\n−\n, as a two-way classification problem using the\nsamples to train a BERT\nBase\nclassifier [12]:\nˆy\ni\n= softmax(Wh\ncls\n(q\ni\n) + b),(13)\nWhere q\ni\n∈ D\n+\n∪D\n−\nis a training question, h\ncls\n(q\ni\n) is the sentence-level representation\nfrom BERT\nBase\n, W  and b are parameters used by the classification head.  The parameters can\nbe optimized to improve the cross-entropy loss between the predicted label distribution ˆy\ni\nand\nthe ground-truth label of q\ni\n.",
      "Latter the training model can also be used to infer the label of new\nquestion q\nt\ndescribed in equation 13.",
      "based on the label of the questions through\nk-nearest-neighbor (kNN) search using a pre-trained fixed encoder as showed by Figure 23.\nThe kNN [53] is an algorithm widely used for a range of NLP tasks. This idea comes from\nthe similarity between the semantically embedded space of two questions if these are closely\nrelated then the knowledge needed for the model to answer would also be similar.",
      "Figure 23: k-nearest-neighbor to understand model knowledge [52]\nEach question was encoded into embeddings, and computed the semantic similarity through\ncosine distance sim(q\nt\n,q\ni\n) = (\ne(q\nt\n)·e(q\ni\n)\n||e(q\nt\n)||·||e(q\ni\n)||\n), where q\ni\n∈ {q\n+\n1\n,...,q\n+\nm\n,q\n−\n1\n,...,q\n−\nn\n}, e(·) being\nthe representation of a sentence encoder.  Then the search for the top-k nearest neighbors can\nbe done based on the results that include the l positive ones and k− l negative ones.",
      "be used to label the question q\nt\nas positive if\nl\nk−l\n≥\nm\nn\nor negative if\nl\nk−l\n<\nm\nn\nm and n are the\nnumber of questions from D\n+\nand D\n−\nrespectively.\n5.6.3    Using Self-Knowledge for Adaptive Retrieval Augmentation\nThe self knowledge acquired previously from the LLMs responses or the predicted labels\nreflect the necessity or not of retrieving external knowledge, for each question q\nt\naccordingly.\nSo that retrieval can be done or not depending on these results.\n5.",
      "7    Model Comparison\nThe model comparison will be used to help pick the correct model based on the require-\nments, these could be based on VRAM, performance or open-sourceness. A well-known leader-\nboard for LLM performance is llm.extratum.io [54].",
      "acteristics  like  VRAM  usage,  quantization  level,  model  size,  and  many  more.\nLlm.extratum.io also sorts based on performance that is measured on key evaluation data sets\nand metrics like GPQA, MUSR, BBH, IFEval, ARC, HellaSwag, MMLU, ThrutfulQA, Wino-\nGrande, GSM8K, MATH Lvl5 and MMLU Pro. These are some of the best methods to quantize\na model’s performance on multiple aspects and will be discussed further in the next sections.\n5.7.",
      "1    MMLU\nThe  MMLU  [55]  or  Massive  Multitask  Language  Understanding  is  an  LLM  benchmark\nthat consists of a dataset designed to be a comprehensive test of the model’s ability to respond\ncorrectly on a diverse range of tasks and topics.  It includes 57 different subjects that include\nknowledge across many disciplines like humanities, STEM, social sciences, and professional\nfields.   The  questions  in  the  benchmark  are  multiple-choice  of  four  options  there  are  over\n15.",
      "000 questions ranging from simple elementary math questions all the way to professional\nmedicine making it a very diverse benchmark. Some of the more important features include the\nstandardized evaluation metrics, calibrated difficulty levels, comprehensive coverage of human\nknowledge, and professional-level expertise requirements.  All of these points transform this\nbenchmark into a very important benchmark in the field due to its variety and complexity.",
      "This\ntest has however become easy for the most recent models like LLaMA 3 70B which achieved\n80.06 out of 100 [54]. To counteract the new advancements an improved version of the MMLU\nwas developed, MMLU-Pro[56].\n5.7.",
      "challenging multi-task language understanding bench-\nmark. This was achieved by increasing the complexity of the options expanding from 4 options\nto 10 thus reducing the probability of guessing the correct answer by chance this also made the\n54",
      "benchmark more challenging and more discriminative. This was done using GPT4-Turbo to in-\ntroduce six additional choices. These are created with the intuition of being plausible distractors\nthat need discerning reasoning to pick the correct answer.  The questions were also improved\nadding to the quality of the benchmark by eliminating trivial and noisy questions from the orig-\ninal MMLU.",
      "Which contained some that were found to be too easy by using a list of small\nLLMs when more than four were able to answer the question this question was then removed.\nThe benchmark was also improved by increasing the portion of more challenging questions by\nadding a bigger share of college-level exam problems.  All of these changes were then verified\nby two rounds of expert reviews to reduce the dataset noise.",
      "This proved to make the benchmark\nmore robust making it less sensitive to prompt variation changing from 4%− 5% to just 2%.\nThis came with the benefit of generating a more stable performance across different prompt\nstyles showing a greater consistency in model evaluation.  These improvements achieved a big\nimprovement in the discrimination between results of different models that previously scored\nsimilarly, the prior gap between GPT-4o and GPT-4-Turbo was 1% with MMLU-Pro it’s 9%.\n5.7.",
      "LLM’s ability to respond to 448 multiple-choice questions.  Which were created\nby domain experts in biology, physics, and chemistry.  These questions were tested on experts\npursuing PhDs in the corresponding domain and still only reached at best 74% when discounting\nthe clear mistakes the experts had identified in retrospect.  One good point to mention is that\nthis was also tested on what the paper describes as highly skilled non-expert validators where\nthe accuracy was only 34%.",
      "This was done with an average of 30 minutes per question and\naccess to the web to research the topic.  This dataset questions were first written by a domain\nexpert, this was then answered by another domain expert who would give constructive feedback\nto improve the clarity of the question and would also suggest revisions if needed.",
      "After said\nrevision by the writer of the question, it is sent to another different domain expert and three\nnon-expert validators who are experts in other domains to access the quality of the query and\nvarious options of answer.\nThis method made sure that the questions are proven and tested by at least two different\ndomain experts and three other area experts.  GPQA is divided into 3 subsets Extended, Main\nset, and Diamond.",
      "The extended subset contains all of the validated questions with 546 different\nquestions.\nGPQA is the name of the main not containing non-objective questions, these being those that\nboth domain expert validators got wrong yet the three non-expert validators got right.",
      "but agreed that they made a clear mistake after being\nshown the solution. The strictest set is the Diamond where only 198 questions are present, these\nare the highest quality and thus only include questions where both experts answered correctly\n55",
      "and the majority of non-experts answered incorrectly.  Though this also includes the questions\nwhere the second domain expert validator got the answer wrong yet explains his mistake once\nshown the correct one similarly to the previous set, but in this case, the first domain expert must\nanswer correctly.\nThis  benchmark  proved  very  efficient  at  achieving  the  proposed  results  since  even  with\ninternet access GPT-4 only achieved 39.4% which was an increase of just 0.",
      "4% from the original\nscore without internet access.  At that point in time when this benchmark was launched GPT-4\nwas the state of the art with older models like GPT-3.5 only achieving 28% check Table 11 for\nmore tests.\nTable 11: Accuracy on each set [57]\n5.7.4    MUSR\nThis benchmark was developed to evaluate LLMs on multistep soft reasoning tasks specific\nto the natural language narrative [58].  This is done by three main domains murder mysteries,\nobject placements, and team allocation.",
      "All of these are synthetic meaning, these were produced\nby an LLM. The creators choose to use synthetic stories due to two main reasons scalability and\nthe ability to regenerate the story due to possible data leaks.  The scalability is important since\nmore capable LLMs are being created every year,the dataset could be adapted as needed to\nbecome more complex and add longer narratives thus introducing more difficult access for the\nLLMs.",
      "The ability to regenerate the story is also very important since a data leak could mean\nthat the LLMs could be trained with the data of the benchmark which would take away the\nzero-shoot aspect of this benchmark.\n56",
      "Table 12: LLMs using CoT+, Humans scores on the multiple domains[58]\nThis data set was constructed using an LLM that is prompted to generate the gold facts\nrequired to deduce the correct answer.  Subsequently, these facts form the basis of a recursive\nquerying process, where the LLM is used to establish the reasoning that connects them, therefor\nconstructing a reasoning tree.  This tree components get then used one by one to generate the\nfinal narrative.",
      "This method generates a narrative that is a hard for machines yet solvable by\nhumans  refer  to  Table  12.   This  is  true  even  when  using  multiple  prompting  strategies  and\nneurosymbolic approaches like chain of thought plus.  The limiting factor discovered by the\nMUSR creators is the limitation that LLMs encounter when generating the deep reasoning trees\nthis greatly limits the narrative complexity.\n5.7.",
      "5    BBH\nBBH or Big-Bench-Hard is the improvement of the original Big-Bench which is a bench-\nmark  consisting  of  204  tasks  from  405  authors  across  132  institutions.   This  extensive  and\ndiverse authorship is a significant strength of the benchmark, as it serves to lower any potential\nfor institutional or individual biases that might come from more limited set of contributors.",
      "on-\ning, biology, physics, social bias, and software development, among others. The tasks selected\nfrom these domains were specifically chosen because they were considered to be beyond the\ncapabilities of state-of-the-art models at the time of its creation.\nBBH was formed, by curating a specific subset of tasks from the original Big Bench collec-\ntion. The selection process was designed to isolate the most dificult challenges for contemporary\nmodels.",
      "The resulting benchmark consists of merely 23 tasks, that were chosen exclusively due\nto the performance being lower for State-of-Art models than those achieved by human raters.\nFrom this group of tasks that proved difficult for machines,  only the most demanding were\nselected to form the final BBH set.\nThe  new  benchmark  prompting  also  differed  in  prompting  since  the  new  approach  uses\nChain-of-Thought prompting.",
      "some as much as 28.5%.  The categories of this dataset are also relevant since they don’t just\nfocus on algorithmic tasks, as they also have natural language understanding, world knowledge,\nand multilingual.  The last one being great addition since it transforms this dataset into a mul-\ntilingual one.  Though the linguistics part doesn’t affect the score by much due to its size in\nrelation with the whole dataset.\n5.7.",
      "6    IFEval\nIFEval [59] or instruction-following evaluation is used to project the ability of LLMs to\nadhere to verifiable instructions.",
      "ctable content, combination, case change,\nstart with / end with, and punctuation.  The instruction also comes with a description to exem-\nplify what the model is required to do.\nThis metric is designed to evaluate the model’s ability to adhere to the instructions provided\nby the proposed system, as illustrated in Figure 27. The method also accounts for errors in the\nmodel’s text formatting relative to the given instruction.",
      "This is accomplished using a flexible\naccuracy metric that tolerates superficial differences, such as formatting variations, provided\nthe core intent of the instruction is fulfilled.\nTo provide a comprehensive assessment, instruction-following is evaluated at two granular-\nities.  These being the per-prompt basis and the per-instruction basis.",
      "This dual-level analysis\nreveals whether the model can maintain adherence to a sequence of instructions or if it only\nfollows the initial ones successfully.\n5.7.7    ARC\nAI2 Reasoning Challenge [60] consists of a dataset and evaluation framework created with\nthe intuition of assessing and advancing the reasoning capabilities of AI systems. This system is\nspecially designed for assessing the models capabilities at answering challenging grade-school-\nlevel science questions.",
      "ARC contains two different sets, the challenge consists of 2590 queries and the easy con-\ntaining 5197.  The hard set is composed with queries that both retrieval-based solutions and\nword co-occurrence fail to solve, on the other hand the easy set is composed of questions that\ndon’t require a lot of reasoning to be answered.",
      "ion on the\nchallenge-set the LLM require deeper reasoning, due to the fact that these can’t be answered\nusing surface-level cues or simple retrieval methods.\nThe paper also talks about how even models that used IR or Pointwise Mutual Information\n(PMI) failed to outperform random guessing on the Challenge set.\n58",
      "5.7.8    HellaSwag\nHellaSwag [61] is a test set created to evaluate LLMs ability in commonsense natural lan-\nguage inference (NLI). The task is to choose the most reasonable continuation of a context,\nfrom four possibilities.\nThe test set is made to be simple for human participants with an accuracy of 95.6%, yet\nsimultaneously difficult for state-of-the-art models, with accuracy below 50%.",
      "The dataset is an extension of the original SWAG dataset but with the inclusion of Adver-\nsarial Filtering (AF). This is done to increase the difficulty of the task.  AF works by contin-\nuously selecting wrong answers generated by adversarial machines,  thereby keeping a chal-\nlenging dataset even for state-of-the-art models like BERT. The novelty lies in generating a\n”Goldilocks zone” of text difficulty, where the wrong answers are nonsensical to humans but\nare often misclassified by models.",
      "HellaSwag consists of 70,000 examples that are gathered from ActivityNet video captions\nand WikiHow text, thus contributing to the diversity of contexts in addition to the length and\ndifficulty of the examples. The dataset also includes zero-shot test classes, in which models are\ntested on unseen domains to estimate their ability to generalize.",
      "pecific bias. Results indicate that even the top models, i.e., BERT\nLarge\n, fail\nto generalize but instead depend on shallow lexical patterns rather than actual commonsense\nreasoning.\nThis  benchmark  indicates  the  weaknesses  of  existing  language  models  in  reasoning  and\nunderstanding the world, demonstrating that natural language processing progress demands the\ndevelopment of benchmarks that evolve together with progress in model capabilities.\n5.7.",
      "9    ThrutfulQA\nThrutfulQA [62] is a benchmark designed to evaluate the truthfulness of LLMs when an-\nswering questions. This benchmark is constructed with 817 questions across 38 different cate-\ngories, such as health, law, finance, and conspiracies. These questions were specially designed\nto test whether models generate imitative falsehoods, these are answers that mimic common\nhuman misconceptions or misinformation found in the training data, including falsehoods.",
      "For\nexample, some models might say that cracking knuckles causes arthritis, even though this is\na false statement.  This benchmark also shows a big gap in human to LLM performance, with\nhumans achieving 94% compared to just 58% of the UnifiedQA LLM. This method can be used\nto see if techniques like fine-tuning to prioritize truthfulness over imitation are achieving the\nwanted results.",
      "ThrutfulQA is a great tool to stop the spread of misinformation and reduce the\ndeception caused by the usage of LLMs by users that think these models are truthful.",
      "5.7.10    WinoGrande\nWinoGrande is an expanded version of the Winograd Schema Challenge, which originally\nconsisted of 273 expert-crafted pronoun resolution problems.  These",
      "problems are trivial for\nhumans but challenging for machine learning algorithms, as these require commonsense rea-\nsoning rather than reliance on statistical patterns or word association.  But with the state-of-art\nmodels achieving near-perfect scores there was a need to improve on the original method so\nWinoGrande was created.\nThis method introduces 44000 problems inspired by the previous method but this time de-\nsigned to be more challenging and also scalable.",
      "These new problems were created using crowd-\nsourcing and after-validated to ensure their trivialness to humans while still being difficult for\nstate-of-the-art models.\nThis data set shows a big gap in performance from LLMs to SLMs.\n5.7.11    GSM8K\nGSM8K [63] is a dataset specially crafted to evaluate the LLM’s ability to perform multi-\nstep mathematical reasoning.   This data set consists of 8.5K high-quality grade school math\nword  problems,  this  data  set  is  split  with  7.",
      "5k  on  the  data  set  and  1k  for  the  testing.   The\nproblems are linguistically diverse and need 2 to 8 steps to solve, these are focused on basic\narithmetic operations like addition, subtraction, multiplication, and division.  The solutions to\nthese are provided in natural language to encourage the model’s interpretability and reasoning.\nThis benchmark is used to test a model’s performance on informal reasoning and problem-\nsolving capabilities.\n5.7.",
      "12    Math Lvl5\nThis data set MATH [64] consists of various levels of math problems with five being the\nmost challenging tier. This test is designed to test advanced reasoning and heuristic applications.",
      "f mathematical concepts, creative problem-solving\nstrategies, and the ability to aggregate various techniques to find a solution.  The performance\nof LLM models like LLaMA 3.1 8B achieves only 5.36% [54], while International Mathemat-\nical Olympiad gold medalists achieve a near-perfect score.  This highlights the significant gap\nin performance between current AI models and expert-level human reasoning.",
      "The problems\nfound in this data set require logical chaining, abstraction, and error-free computation, areas\nwhere LLM’s performance tends to be lackluster.\n5.7.",
      "references.  All of these are generated by a schema-based pipeline to maintain accuracy.  This\napproach allows them to implement metrics that differ from the standard and are more aligned\nwith factual accuracy, there are three metrics for this, Completeness, Hallucination, and Irrele-\nvance.",
      "The process to generate all the needed files is as follows: S −→ C −→ D −→ (Q,A)−→ R −→\nKeypoints\nThis sequence shows all the steps taken by this approach starting with the schema summary\nS that leads to the configuration generation C, followed by the document generation D.",
      "With\nthe  document  formed  the  question  answer  pairs  are  formed (Q,A)  and  the  references  need\nto come that answer identified R and then the keypoints are extracted, representing the most\ncritical information in the answers.",
      "io-specific\ntext generation, these key elements encapsulate the aspects of essential factual knowledge from\nthe  input  documents.   This  schema  acts  as  a  backbone  to  ensure  that  the  content  is  diverse\nand reliable while maintaining a standard across various scenarios.   The schema defines the\nstructural framework of key elements for domain-specific documents without containing actual\ndata.",
      "As an example in medicine,  it can outline categories for symptoms and treatments,  in\nfinance it could establish classifications for metrics, sectors, and organizations.  One concrete\nexample of a schema generation starts with the initial generation by the LLM, the model gets\nfeed with carefully chosen seed documents, these are real legal documents that represent the\nkind of knowledge and structure that the schema is to take.",
      "After the schema is created a series\nof iterative refinements are taken by a human using it’s intuition and contextual understanding\nto fix any nuances the model had generated. Due to the fact that this process occurs more than\nonce, it ensures that a balance between comprehensiveness, accuracy, and generalization, thus\nsupporting content generation across diverse sub-scenarios.",
      "Generating a document that is rich with factual information and isn’t contradictory, is cru-\ncial to creating high quality datasets, ensuring that the generated content can be evaluated ac-\ncurately and used effectively in downstream tasks.  To generate documents with that quality,\nfirst the configurations C are generated, these derive from the previously established schema S.",
      "r text generation, thus maintaining\nconsistency across the document.  To generate these configurations a hybrid approach is taken\nthat combines rule-based methods with LLMs to assign values to the schema elements.  These\nrule-based methods like selecting values randomly from predefined scenario-specific options,\nensure that high accuracy and factual consistency is maintained for a more structured data. This\nand the more complex or diverse content, balances consistency and creativity.",
      "After the con-\nfiguration is ready a GPT-4o is used to convert the factual information from the C into a more\nstructured narrative format that is more aligned with a specific scenario. For example, in medi-\ncal records the generated document can include categories that add a more complex background\nto the document these can be a patient information, medical history, or a treatment plan.",
      "same is done with other topics but with categories that better align with them.\nFigure 24:  RAGEval System:  1 summarizing a schema containing specific knowledge from\nseed documents.  2 filling in factual information based on this schema to generate diverse con-\nfigurations.   3  generating  documents  according  to  the  configurations.   4  creating  evaluation\ndata composed of questions, answers, and references derived from the configurations and doc-\numents.",
      "[65]\nQuestion-Reference-Answer (QRA) to generate these RAGEval uses the documents D and\nconfigurations C  these are used to establish a robust evaluation framework ready to be used\non information retrieval and reasoning capable applications.",
      "estions as well as the initial answers, this forces the generated\ncontent to be aligned with the schema elements.  To address different types of questions like,\nmulti-hop reasoning,  summarization,  and multi-document questions,  each one is specifically\ndesigned to evaluate specific facets of language understanding. To ensure diversity and control-\nlability of these questions 7 main question types were designed.",
      "The model gets provided with\ndetailed instructions and examples for the question type needed to be generated, the model then\noutputs the question Q as well as the initial answer A. Using the Q and A the relevant informa-\ntion fragments get extracted R from the documents D. This is done using an extraction prompt,\nthus ensuring that the generated answer is grounded in the source material this improves the\nreliability and traceability.",
      "To reduce the misalignment between A and R, the answers get it-\neratively refined thus also improving the coherence and accuracy. If references contain content\nmissing from the answers they supplement them accordingly.  To reduce the hallucinations a\nlook is taken at the answers to find any unsupported content that either gets corrected with rel-\nevant references or removed.",
      "Finally the keypoints get generated from the answers A for each\nquestion Q to highlight the critical information in the responses. Normally each answer A gets\nbroken down into 3-5 keypoints, that encompass all essential factual detail, as well as relevant\ninferences, and conclusions.\nDragonBall dataset which means Diverse RAG Omni-Benchmark for All scenarios.",
      "and encompasses a range of texts\n62",
      "and RAG questions across 3 main domains finance,  law,  and medical.  This dataset consists\nof both Chinese and English texts, that serve as a comprehensive resource for multi-language\nscenario-specific research. Due to its big size of 6.711 questions it can be used on just English\nor Chinese assessment.\n5.7.14    HotPotQA\nHotPotQA [66] is a dataset with 113k question-answer pairs that contain 4 main features\nthat tell it apart.",
      "Number one being the question requires finding and reasoning over multiple\ndocuments to achieve a correct answer. Number two is the variety these questions present, these\nare diverse and aren’t constrained to any pre-existing knowledge bases or schemas.  Number\nthree this dataset provides sentence-level supporting facts that are needed for reasoning.",
      "Lastly\nnumber four it also introduces factoid comparison questions that test wether QA systems can,\nnot only extract relevant facts, but also compare them.\nThe data used for this dataset creation was gathered by crowd-workers on Amazon Mechan-\nical Turk.  The gathered data had it’s origin on English Wikipedia, the process followed five\nmajor steps.",
      "Starting with finding a paragraph on a specific Wikipedia page, next is the navi-\ngation to a hyperlynk found in that paragraph, this will later be a related article. The following\ntask is the formation of a question about the two articles, this formed query can’t be answered\nwith just the information of one and needs the two of them to be complete.   The following\nstep is the generation of a answer to the previous question this answer gets information from\nboth pages.",
      "tep is to identify the supporting facts, this is a crucial explainability step\nwhere the worker must select the specific sentences from the two articles that are needed to\nreason through and arrive to the final answer. The specific sentences found are the ground-truth\nsupporting facts. These steps form the generation pipeline of this method.\nThis method being specially develop with reasoning in mind, comes with two main types of\nreasoning Bridge-Entity, and Comparison Reasoning.",
      "Bridge-Entity  Reasoning  is  most  prevalent  question  in  HotPotQA  and  involves  a  bridge\nentity, meaning the two necessary documents are linked by a common person, place, or thing.\nTo  answer  the  question,  the  model  must  first  use  one  document  to  identify  this  bridge  and\nthen use the second document to find the final piece of information.",
      "As for the Comparison\nReasoning this requires the model to extract information and form various facts,  these facts\nneed then to be compared between themselves to find the final solution. This can be something\nlike ”when was the last public record of Astra systems publicized”.   This would require the\nmodel to find all the public records about Astra systems, then it would need to compare dates\nof all the documents, and only then would the final answer be evident.",
      "This dataset also comes with a evaluation framework specially designed to work with the\ndifferences found from more common datasets. The evaluation is measured in answer accuracy\nthat is the standard evaluation of wether the final answer is correct, this is done using a Exact\nMatch (EM) and F1 score.",
    ],
  },
  methodology: {
    title: "Methodology",
    content: [
  {
    "type": "image",
    "src": "/images/methodology/page_88_img_1.png",
    "alt": "Page 88 Image - Figure 25: Relationship between",
    "caption": "Figure 25: Relationship between"
  },
  "scores are also used, this evaluates wether the model correctly identified what was needed to\nreach the final answer. One important thing to note is that the paper argues that for the model to\nbe accurate it should get both the answer and the reasoning path correct. So a combined metric\nis proposed where a model only gets the full credit when success at both tasks is achieved, thus\na stricter and more meaningful evaluation of a model’s true reasoning capabilities is achieved.\n6    Methodology\nAs more powerful models hit the market with more expensive requirements, finding a model\nthat suits low-budget companies or entities is becoming harder. The problem occurs due to the\ngap in the performance of LMMs.  LMMs with more tokens will always tend to have greater\nperformance [8].  However, this performance could be increased with methods described pre-\nviously  like  HyDE  [40],  RAG  [35],  CoT  [67],  CAG  [43],  RAGCache  [44],  SKR  [52],  and\nContriever [49]. All of these methods aim to improve model performance, but they do so at the\ncost of increased computational overhead.  This overhead is variable, meaning that depending\non the method or combination of methods used, the system’s requirements and consequently its\nenergy consumption can change significantly. Since most of the studies are also on bigger mod-\nels a new approach will be taken to see if the results are similar to the bigger models. The key\npoints of the research are the efficiency and performance using some of the enhancing methods\ndescribed above. These will be paired with a small LLM meaning less than 36B tokens.\nFigure 25: Relationship between",
  "model size and monthly downloads[68]\n6.1    Overview of the Query Rewriting Flow\nThe use of Large Language Models (LLMs) in Information Retrieval (IR) systems has rev-\nolutionized the way people interact with and retrieve information.  Traditionally IR systems,\nsuch as search engines,  have evolved from term-based to neural network models,  which are\nuniquely suited to detect semantic nuance and contextual hints. Nevertheless, such systems still\npose challenges like query vagueness, data sparsity, and generation of responses that, although",
  "plausible, are actually incorrect.  LLMs, with their remarkable ability in language generation,\ncomprehension, and reasoning, offer a unmatched solution for the aforementioned challenges.\nLeveraging their huge pre-training on diverse text-based data, LLMs enhance various IR com-\nponents  like  query  rewriting,  retrieval,  reranking,  and  response  generation  and  enable  more\nprecise, contextual, and user-centric search experience as shown in Figure 26.\nBreakthroughs in cutting-edge large language models (LLMs) LLaMA have exhibited the\ncapability to accomplish demanding tasks and optimize information retrieval (IR) performance.\nThese models have not just the capacity to de-modularize user queries and retrieve relevant\ndocuments  but  also  render  lengthy  and  human-sounding  responses,  thereby  overcoming  the\nlimitations between traditional IR systems and the present user expectations.\nFigure 26: Traditional information retrieval architecture[68]\nThe proposed system will integrate a rewriter module that will be placed between the user\nquery q and the retriever similar to [45], this will",
  {
    "type": "image",
    "src": "/images/methodology/page_90_img_1.png",
    "alt": "Page 90 Image - Figure 27: Graphical representation of the system diagram[68]",
    "caption": "Figure 27: Graphical representation of the system diagram[68]"
  },
  "enable the injection of the retrieved documents\nas-well as the injection of instruction like CoT. This system works by first receiving the user\nquery,  which  will  be  processed  by  the  rewriter  module.   This  module  inserts  an  instruction\ninst that is used to determine the most suitable rewriting strategy to optimize the query.  The\ninstruction will prompt the LLM to evaluate which of the three approaches is the more suitable\nnormal response, RAG, or Chain-of-Thought.\nFigure 27: Graphical representation of the system diagram[68]\nIf the models concludes that it can answer directly then the model proceeds to generate the\nanswer without the need of a new generation, meaning only one hop. As for Chain-of-Thought,",
  "the approach works by injecting a command to try and force the model into thinking step by\nstep to obtain the answer. The final approach is Retrieval-Augmented Generation (RAG), which\nis also the most complex.  It begins by passing the query to a smaller model that generates an\nembedding representation of the query, aligned with a pre-embedded document collection.  A\nsimilarity comparison is then performed to retrieve the most relevant documents based on this\nembedding space.  Next, a rewriter is used to extract and pass only the most important parts\nof the retrieved documents, reducing the amount of irrelevant information passed to the model.\nFinally, a reranker prioritizes the most relevant documents so that they are presented first during\nthe generation process.   The last two aren’t necessarily needed so they can be turned off as\nneeded for more .\n6.2    Hardware and Software Environment\nThe experiments were co",
  "nducted on a high-performance workstation running windows 10,\nequipped with an Intel Core i7-13700KF processor, an NVIDIA RTX 4090 GPU, and 32 GB\nof DDR5 RAM operating at 6000 MHz. The software environment was managed using Python\n3.12 within a virtual environment (.venv), ensuring isolation and reproducibility of dependen-\ncies.   The  main  libraries  required  are  PyTorch  (with  CUDA  12.6  support)  and  the  Hugging\nFace Transformers library,  which was used to download and run the deepseek-ai/DeepSeek-\nR1-Distill-Llama-8B LLM. To monitor the system performance and power consumption during\nmodel inference and other system requirements,  HWInfo was employed.  Additional Python\nlibraries were installed as required to support the various aspects needed for the workflow. This\nsetup provides a robust and efficient platform for running and evaluating the proposed system.\n6.3    Rewriting Approaches\n6.3.1    Straight LLM\nThis approach uses the initial model response as the final answer.  It is designed for ques-\ntions that are too simple to require more complex methods.   From an efficiency standpoint,\nminimizing processing steps is desirable, and using the first response avoids redundant genera-\ntion. As the simplest approach, it does not enhance the model’s answer but provides a baseline\nfor comparison.\n6.3.2    Chain-of-Thought\nThe Chain-of-Thought (CoT) approach enhances model performance by encouraging it to\nreason through the logical steps of a query.  In some models, this can be achieved with simple\nprompts such as ”Please reason step by step, and put your final answer within a box.”, as seen\nwith DeepSeek models [69]. Other m",
  {
    "type": "image",
    "src": "/images/methodology/page_92_img_1.png",
    "alt": "Page 92 Image - Page 92 Image",
    "caption": "Page 92 Image"
  },
  "This approach has been widely adopted to improve and enhance the reasoning capabilities of\nLLMs. One of the reasons for this adoption comes from the simple requirements since it doesn’t\nneed much change on already-built systems. This method has shown significant improvements\nin certain tasks that require logical thinking and contextual understanding.  For instance, some\nbenchmarks often use both with and without Chain-of-Thought prompting to show the direct\nimpact of the on reasoning performance [70], [58].  The proposed system selects this option\nwhen the initial model response indicates that Chain-of-Thought reasoning is required.  This\nmethod involves two inference steps: the first allows the LLM to assess whether CoT is neces-\nsary, and the second passes a modified query containing the appropriate instruction to prompt\nthe model to reason through the problem.  Finally the model answer the instruction that now\ncontains a CoT instruction.\n6.3.3    RAG\nThis approach is used to retrieve documents or passages that are relevant to the user’s query.\nThis is done by embedding the user query into a vector , which is then compared to the vectors\nof stored documents.  Based on a top-K similarity ranking,  the most relevant documents are\nretrieved. These documents can then be refined by removing non-essential parts, aiming to make\nthe resulting content as concise and relevant as possible. Following refinement, the documents\nmay be reranked, as language models tend to focus more on the initial tokens in the input [71].\nSimilar to the Chain-of-Thought method, this approach also requires two hop",
  "s: The first hop\nacts as a reflection step to determine whether the LLM deems document retrieval necessary; The\nsecond hops consists on passing the query as well as the retrieved documents to the LL;\n6.3.4    Selecting the Approach\nThe system has three possible approaches to choose from: Retrieval R, CoT C, and Straight\nanswer S.  Since only one of these approaches can be selected for a given query, a selection\nmechanism is required. This selection is performed using the model (M) and the content of the\nquery (q), this is shown in Figure 29\nFigure 28: Analysis Prompt\nFigure 28 presents the analysis prompt responsible for this selection.  This instruction can\nbe divided into two parts.  The first part is responsible for the Straight Answer (S), it asks the",
  "model to respond directly to the query if it is confident it can do so correctly.  This approach\nis  not  only  the  fastest  as  well  as  the  simplest,  as  the  system  does  not  need  to  perform  any\nadditional steps to achieve the response. The second part of the prompt is responsible to induce\nthe model into deciding between Retrieval (R) or CoT (C). This is done by asking the model if\nit would benefit from retrieval of documents. If the response is ”Yes” then Retrieval process is\ntriggered, and all subsequent steps such as reranking and refining are also executed.  However,\nif the model’s answer is ’No’, the system interprets this as a lack of confidence in the initial\nresponse.  To improve the quality of the final output, the system then forces the model to use\nChain-of-Thought (CoT) reasoning.\nTo translate the model’s textual output into a definitive choice, the syst",
  {
    "type": "image",
    "src": "/images/methodology/page_94_img_1.png",
    "alt": "Page 94 Image - Page 94 Image",
    "caption": "Page 94 Image"
  },
  "em employs a so-\nphisticated post-processing and also voting mechanism rather than simply looking for a single\nkeyword. This implementation is a critical part that enables the system to use a decision-making\nprocess robust and resilient to multiple formatting variations. The process unfolds as follows:\n1.  Initial Cleaning:  The raw output from the model is first decoded and also normalized\nto standardize characters and spacings.  Any preliminary ”Chain-of-Thought” reasoning,\nwhich is normally enclosed in special tags like ” < /think > ”,  is stripped away to\nisolate the final answer.\n2.  Check for a Direct Answer(early exit): Before classifying the need for retrieval, the sys-\ntem first checks if the model already provided the final answer on it’s first analysis.  It\nspecifically looks for a ”\\\\boxed{...}” pattern containing a single alphabetic character\n(e.g., ”\\\\boxed{A}”). If this pattern is found, the system assumes it corresponds to strat-\negy S. The process then halts immediately, returning the response as the final output. The\n’method used’ is set to ’none’, indicating that no additional processing was necessary.\n3.  A Voting System For Decision Making:  Instead of a simple parse,  the system incurs a\nscoring mechanism to ”vote” on the best possible approach.   It initializes counters for\nboth R and C.\n4.  Identifying Strong Signals:  The code meticulously scans the output for high-confidence\nindicators.  A ”\\\\boxed{1}” pattern is considered a strong, explicit vote for R, adding a\nsignificant score of 100 points. Similarly, a ”\\\\boxed{2}” is a strong vote for C.\n5.  Identifying Secondary Signals: To enhance robustness and acc",
  "uracy, the system also looks\nfor secondary indicators. The presence of the word ”yes”(case-insensitive) in the analysis\nalso adds 100 points to the R counter.  This ensures the model’s intent is captured even\nwhen the model fails to use the precise boxed format.\n6.  Final Decision: After analyzing the entire output, the approach with the highest accumu-\nlated score is chosen as the ”suggested\nmethod”.  This multi-signal, voting-based mech-",
  "anism makes the classification resilient to minor formatting errors from the model, thus\nprioritizing a correct interpretation over strict adherence to a single output format.\n6.4    Dataset Augmentation\n6.5    Query-Answer Validation\nThe proposed validation framework is comprised of a three-stage process that is used to sys-\ntematically differentiate between high-quality and problematic queries.  This hybrid approach\nintegrates automated analysis with a human-in-the-loop component.\n6.5.1    Automated Preparation\nThe data is ingested and categorized using a python script. The script then splits the queries\ninto those that require retrieval and those that do not.   Depending on the configuration,  one\nof these two sets is selected for further processing.   This set is then divided into batches of\nn queries, a value that can be configured in the settings.  Each time the Alt key is pressed, a\nnew batch is compiled and copied to the clipboard, ready to be pasted into the LLM chat.  The\ngenerated batch already includes the appropriate instruction, tailored to the selected query type.\n6.5.2    AI-Powered Triage\nThe LLM analyzes the provided query and determines if the options are clear and the ground",
  {
    "type": "image",
    "src": "/images/methodology/page_96_img_1.png",
    "alt": "Page 96 Image - Page 96 Image",
    "caption": "Page 96 Image"
  },
  "truth is correct, paying special attention to the number of correct options.  If these prove to be\ncorrect then a Green Flag is assigned.  This flag contains an emoji so the human supervisor\ncan identified the queries easily.  In this workflow, a Green Flag is treated as a definitive pass,\nmeaning these queries are not further reviewed by the supervisor.\nA Red Flag, on the other hand, is assigned to any query that fails to meet the criteria for\nexample, due to ambiguous wording or other inconsistencies.  Unlike the Green Flag, a Red\nFlag does not automatically discard the query. Instead, it signals that the query requires human\nverification.  Although the model provides an explanation for why the Red Flag was assigned,\nthe final decision rests with the human supervisor.\n6.5.3    Human Verification\nThe human validator is instructed to only look at the Red Flags queries to maintain attention\nand reduce the number of queries a human needs to verify. The job of the validator falls into the\nverification of the LLM’s justification for the Red Flag, this verification can lead to the validator\ndoing some research on the query and the correctness of the expected output.  In case a query\nfails this verification it is deemed unusable and gets removed from the dataset, a new one is\nadded to it’s place that passes this validation.",
  "6.5.4    Dataset formation\nThe output of all the previous points are fully answerable queries that are free of ambiguity.\nThis is used for two distinct datasets,  the ARC-easy and the HotPotQA, the results are then\njoined to combine queries that require retrieval as well as those needing step-by-step reasoning.\nThi",
  "s new dataset is specially designed for systems that can decide what approach is required for\nthe best possible outcome. However due to the lack of complexity of the ARC-easy queries this\nis more suited towards weaker than state-of-the-art, more around 32B tokens and lower.\n6.6    Power Data Collection\n6.6.1    GPU\nTo compare the different components of the system, one important aspect is energy con-\nsumption.  However, collecting this data on Windows is not straightforward.  Due to hardware\nlimitations, all measurements will be performed using software tools that query system com-\nponents to report energy usage at a given moment.  Starting with GPU the power consumption\nis collected using nvidia-smi [72], this tool acts as bridge that queries the GPU driver directly\nand converts the retrieved data into a useful unit, these data points are collected every 15ms,\nhowever due to driver overhead the real gap is around 50ms.  The data collected according to\nNvidia NVML [73] is related to TBP or total board power, meaning that the consumption of\nVRAM and all the necessary components to support the GPU die itself are included in that\npower measurement.  This is important as the power consumption of VRAM is highly used by\nLLM’s during inference. This data is collected directly by a purpose-built library that monitors\nGPU power draw. The power measurements are added directly into the JSONL file, which also\ncontains the model’s response and all relevant metadata for later analysis.\n6.6.2    CPU\nThe other power consumption metric is the CPU package. This measures the power used by\nthe CPU chip itself, excluding any power consumed by supporting",
  {
    "type": "image",
    "src": "/images/methodology/page_98_img_1.png",
    "alt": "Page 98 Image - Page 98 Image",
    "caption": "Page 98 Image"
  },
  "components such as voltage\nregulator modules (VRMs), the chipset, and other peripherals. Although it would be interesting\nto measure the full system power draw, this requires specific hardware that was not available for\nthis project. This CPU package draw isn’t super easy to obtain in a Windows system so the use\nof a proprietary tool called HWiNFO this tool offers a logging feature that creates a CSV with\nall the collected data, this collection is done every 20ms theoretically however in reality there\nare a lot of times where it takes more than that, but on average it takes 31ms.\nBecause the CSV is generated by a third-party tool, it is only available at the end of the\nrun when logging is manually stopped.  To align energy data with query execution, a script is\nused to match each query’s start and end times (from the previously mentioned JSONL file)\nwith the corresponding timestamps in the CSV. All data points that fall within a query’s time",
  "window are extracted. Using these points and their timestamps, the trapezoidal rule is applied to\napproximate the energy consumption.  This method works by summing the areas of trapezoids\nunder the power curve, providing an estimate of the integral of power over time (watts × time).\nThis approach compensates for the irregular intervals in data retrieval by HWiNFO. The result\nis an estimate of energy consumption by the CPU package, expressed in watt-hours. This value\nis then added to the JSONL file, along with the total energy consumption calculated as the sum\nof the GPU’s Total Board Power (TBP) and the CPU package power.   GPU power usage is\nalready recorded in the JSONL, and the trapezoidal",
  "rule is applied in real time during inference\nto account for variations in the intervals between data points.\nFigure 29: Jsonl Data Structure\nIn Figure 29 is depicted the final JSONL structure.  The structure is devised into four main\nparts Query Information, Ground Truth, AI Prediction, and Performance & Metrics each being\nfinalized at different stages. The system initially gets a JSONL with just the Query Information\nand the Ground Truth. Part 2 is formed by the responses and metrics from the system solution.\n6.7    Evaluation Framework\nThe evaluation framework may vary depending on the specific domain being tested.  This\nis because some domains might require different metrics to understand the real capabilities of",
  "the model in a given task [74].  Another key point is the need to access each part individually\nas well as combined.  This is key in accessing the quality of the system and understand which\npoints could be improved for a better combined performance.One of the most important metrics\nacross all components of the system is efficiency, as it helps to assess how each part contributes\nto the overall energy consumption. What will be compared and obtained is the following:\n•  System answers to full dataset.\n•  Straight model answers to full dataset.\n•  Forced CoT answers to full dataset.\n•  Forced Retrieval answers to just full dataset.\nThe full dataset consists of 3000 queries, with 1312 originating from the ARC-Easy dataset\nand the remainder from the DragonBall dataset.  The ARC-Easy dataset was selected because\nit primarily contains simple reasoning queries that the model can answer without relying on\nexternal knowledge,",
  {
    "type": "image",
    "src": "/images/methodology/page_100_img_1.png",
    "alt": "Page 100 Image - Page 100 Image",
    "caption": "Page 100 Image"
  },
  "although a few questions do require more complex reasoning.  The Drag-\nonBall dataset, on the other hand, was chosen because it mostly includes queries that necessitate\nretrieval, with some also requiring advanced reasoning to be answered correctly.\nTogether, these datasets offer a comprehensive evaluation of the system’s capabilities.  Ad-\nditionally, if the model under study is a larger one, the ARC-Easy portion can be replaced by\nARC-Challenge, which features more complex queries that demand deeper reasoning than its\nsimpler counterpart.\n6.7.1    Retrieval Performance Metrics\nDue to the nature of this work, the quality of the retrieval itself will not be evaluated, as it\ndepends on the specific RAG method employed.  Instead, the evaluation will focus on whether\nretrieval occurred when it was necessary.  This will be represented as a binary outcome:  1 if\nretrieval was triggered, and 0 if not.  However, what needs to be evaluated is a direct compari-\nson between the energy consumption of the proposed system and that of a baseline that always\nperforms retrieval.  This comparison is important because the system requires two hops to de-\ncide whether to retrieve, whereas always retrieving eliminates the need for this decision-making\nstep. However, since the dataset includes questions both with and without the need for retrieval,\nan evaluation will be conducted to determine whether the system results in lower energy con-\nsumption.  This is based on the premise that retrieval is not necessary for every query, and the\nsystem may avoid unnecessary retrieval steps.  The quality and correctness of the answer will\nalso be evaluated. This is",
  {
    "type": "image",
    "src": "/images/methodology/page_101_img_1.png",
    "alt": "Page 101 Image - Page 101 Image",
    "caption": "Page 101 Image"
  },
  "important because, in cases where retrieval is not necessary, the sys-\ntem may still retrieve documents from the database that are not directly relevant to the query.\nAs a result, these retrieved passages may not contribute meaningfully to the answer.",
  "6.7.2    Straight Model\nThis approach will be evaluated in multiple parts.  The first aspect is whether the answer is\ncorrect specifically, whether the model’s response matches the ground truth option.  Next, the\nevaluation will check if the model correctly identified queries that should be answered without\nretrieval.   This part is linked to the previous one:  if the answer is correct without retrieval,\nthe classification is also considered correct.  Additionally, a comparison will be made between\nanswers generated with and without forced Chain-of-Thought (CoT) prompting to determine\nwhether the increased energy consumption associated with CoT leads to improved answers or\nif the same responses would have been provided without it.  This will be done to understand\nif the model is guessing correctly wether it can answer the question directly or not.  And will\nalso provide consumption metrics that will be compared in order to understand its efficiency.\nCoT will tend to be more accurate but it also requires more energy due to the thinking phase of\ngeneration.\n6.7.3    Chain-of-Thought Reasoning Performance Metrics\nThe  metrics  for  CoT  are  the  same  as  those  used  for  the  straight  model,  as  both  will  be\ndirectly compared.\n6.7.4    Automated Evaluation Script\nTo implement the evaluation metrics described, particularly for understanding the correct-\nness of the model’s answer",
  "s, an automated script was employed.  This script is responsible for\nprocessing the model’s output for each query, which is stored in a JSONL file format. The pri-\nmary goal is to systematically determine if the model’s final answer matches the ground truth,\nespecially for ARC queries which are multiple-choice questions.\nThe  core  to  this  evaluation  lies  in  a  multi-step  parsing  strategy  designed  to  intelligently\nextract the final answer from the model’s potentially complex and verbose output, similarly to\nhow a human user would read the output and understand which character is the one that the\nmodel chose. The process is as follow:\n1.  Definitive Answer Extraction:The script first searches the model’s prediction for the\nmost explicit answer format, such as ”\\\\boxed{B}”. This format is often used by models\nto clearly delineate the final answer, so finding it is the most reliable sign. However, since\nthis work is conducted using smaller models, they often do not follow patterns very well\nand may provide the answer surrounded by verbose context reflecting their reasoning.\n2.  Pattern-Based Fallback:  If the first pattern is not found, the script then looks for common\nnatural language phrases that indicate a final answer, like ”The answer is B” or ”Answer:\nB”.  It is designed to take the last match it finds, operating on the assumption that any\nreasoning or changes of mind would occur before the final declared answer.",
  "3.  Positional Fallback:  As a final strategy, if neither of the above patterns yields a result,\nthe script searches for all standalone capital letters (A, B, C, or D) within the response.\nSubsequently, the",
  {
    "type": "image",
    "src": "/images/methodology/page_103_img_1.png",
    "alt": "Page 103 Image - Page 103 Image",
    "caption": "Page 103 Image"
  },
  "system selects the final occurrence as the intended answer, assuming it\nreflects the model’s ultimate decision.  This servers as a robust fallback for cases where\nthe model might provide the final answer without any formatting.\nOnce one answer is extracted through this hierarchy of methods, it is compared directly with\nthe ”ground\ntruth” value from the JSONL. A new metric, ”correctanswerarc”, is added to the\ndata inside the metrics section, this then gets marked as ”true” if they match and ”false” if not.\nFurthermore, this script is also responsible for the automation of the retrieval performance\nmetric.  It checks if the ”references” inside the ”ground\ntruth” section contains any references\nthat indicate that retrieval was needed.  It then cross-references this with the ”methodused”\nvalue that represents the path the system chose.  If the model used ”retrieval” for a question\nthat required it, a ”retrievedcorrectly” metric is marked as ”true” on the same metrics section,\naligning with the binary evaluation approach mentioned previously.  This automated process\nensures a consistent and scalable way to apply the defined evaluation criteria across the entire\ndataset.\n6.7.5    Efficiency Metrics\nEfficiency is a metric that can be measure in various ways depending on the focus of the\nevaluation.  This could be power consumption, cost-effectiveness, or scalability, each of which\nplays a central role in the direction of this thesis. Power consumption will be measured as watts\nper query . This metric is important due to the multiple processing steps involved in generating\neach output. However, it will not reflect the total system power cons",
  {
    "type": "image",
    "src": "/images/methodology/page_104_img_1.png",
    "alt": "Page 104 Image - Page 104 Image",
    "caption": "Page 104 Image"
  },
  "umption, as only CPU and\nGPU usage will be measured due to hardware limitations.\nThe cost-effectiveness will be calculated by dividing the cost of the required hardware com-\nponents by the system’s performance.   This allows for a direct comparison between this ap-\nproach and more powerful alternatives.  Such comparisons can be conducted through a series\nof tests, similar to the benchmarks referenced in model comparison section.  Scalability will\nbe assessed by analyzing the requirements needed when using a larger model or when more\ndocuments and keywords are added to the system.  This will likely be the most difficult metric\nto evaluate directly, as I do not have access to more powerful hardware. However, I will attempt\nto estimate scalability based on data and findings from other researchers.\n6.8    Optimizing Query Classification through Iterative Prompt Refinement\nThe performance of a LLM is fundamentally linked to the clarity and quality of the instruc-\ntion provided. In this system, where the initial goal is to classify a user’s query into one of three\npaths retrieving external documents, reasoning step by step, or simply using the model’s first",
  "response as the answer (both relying on the model’s internal knowledge) the construction of the\ninstruction plays a crucial role.  By carefully designing this part of the process, we can guide\nand control the model’s decision-making behavior.  To determine the most effective approach\nfor this classification task, a series of instructions were developed and tested, evolving from a\nsimple open-ended prompt to a highly structured one that involves a fully rule based framework.\nThis sec",
  {
    "type": "image",
    "src": "/images/methodology/page_105_img_1.png",
    "alt": "Page 105 Image - Figure 30: Instruction V1",
    "caption": "Figure 30: Instruction V1"
  },
  {
    "type": "image",
    "src": "/images/methodology/page_105_img_2.png",
    "alt": "Page 105 Image - Figure 30: Instruction V1",
    "caption": "Figure 30: Instruction V1"
  },
  "tion will analyze the iterative refinement process of the instructions in detail, eval-\nuating the performance of each version. The evaluation on this section focuses on key metrics,\nsuch as: Routing Accuracy (the model’s ability to correctly chose ’retrieval’ or ’no-retrieval’),\nAnswer Accuracy (the correctness of the final response), and Efficiency (measured in energy\nconsumption) though this metric will be more thoroughly looked at at a later stage.  By exam-\nining the trade-offs between these factors at each stage, we can identify the best practices for\nguiding an LLM in a complex classification task.\n6.8.1    Instruction V1: A Simple Baseline",
  "The initial instruction, V1, was designed as a minimal baseline to assess the feasibility of the\nproposed approach. This instruction directly asks the model for a binary classification (”Would\nthis Query benefit from the retrieval of documents?”) Figure 30, with a heavy emphasis on the\noutput format rather than the decision-making logic.  This lack of explicit criteria forced the\nmodel to rely almost totally on its internal, pre-existing biases to interpret the query’s needs.",
  "As the performance data from the evaluation reveals, this approach was highly inconsistent\nand ultimately unreliable.   The model developed a strong bias against retrieving documents,\nleading to a significant imbalance in the classification.  This likely occurred due to the smaller\nmodel lacking sufficient internal knowledge, as it is less capable than state-of-the-art models.\nWhile the system was adept at identifying general knowledge questions (those",
  "not requiring\nretrieval),  it correctly avoided retrieval 98.17% of the time,  refer to Figure 31.   However,  it\nalmost completely failed at the inverse task, only correctly choosing retrieval 16.59% for the\nqueries that required it. As a result, the overall routing accuracy was limited to just 52.3%.",
  "questions that ideally required retrieval to answer correctly, the model incorrectly routed 1408\nof them to be answer without retrieving.  This fundamental failure to identify questions need-\ning external knowledge confirmed that a simple, unguided prompt is insufficient for creating\na reliable query-routing system.  While this appears to be true for the chosen model size, fur-\nther testing is necessary to determine whether this limitation persists in larger models or if they\nperform better on this task.\n6.8.2    Instruction V2: An Aggressive, Safety-First Heuristic",
  "To  counteract  the  significant  bias  against  retrieval  observed  on  the  first  approach  Figure\n30,  the  second  iteration  introduced  a  strong,  explicit  bias  towards  retrieval.   Instruction  V2\nFigure 32, framed the model as an ”expert query analyzer” with the primary goal of eliminating\nincorrect answer by trying to force document retrieval for any non-trivial query.  It established\nretrieval (’1-Yes’) as the default assumption,  permitting a direct answer (’2-No’) only if the\nquery met a very strict and narrow set of criteria: it had to involve exclusively ’globally famous\nentities’ and ask for a single, static, and universally known fac",
  {
    "type": "image",
    "src": "/images/methodology/page_107_img_1.png",
    "alt": "Page 107 Image - Page 107 Image",
    "caption": "Page 107 Image"
  },
  "t.  This ”safety-first” heuristic\nwas designed to try and minimize the risk of factual errors originating from the model’s internal\nknowledge.\nThis agressive change dramatically inverted the model’s behavior. The Retrieval Task Rout-\ning Accuracy skyrocketed from 16.59% to 98.22%, demonstrating that the model could now\nreliably identify questions that required external documents.   Out of 1688 such questions,  it\ncorrectly chose ”retrieval” for 1659 of them.\nHowever, this success came at significant cost to efficiency and accuracy on the opposite\ntask,  with  results  that  were  almost  predictably  inverse  to  those  of  the  first  instruction.   The\nmodel’s ability to recognize simple, general knowledge questions plummeted. This can be seen\non the General Knowledge Routing Accuracy that fell from 98.17% to a mere 26.07%.  The\nsystem was now incorrectly choosing to retrieve documents for the vast majority of the queries\nthat did not need it. This over-correction is properly showed on the decision matrix, where 970\nout of the 1312 No Retrieval queries, were wrongly sent down the retrieval path.\nAlthough the Overall Routing Accuracy improved to 66.7%, this aggressive heuristic proved\nto be an over-correction. While it successfully enforced the retrieval of necessary information,\nit failed to account for cases where retrieval was unnecessary.  This led to inefficient and often\nredundant processing for a large number of relatively simple queries that the base model could\nhave answered directly. This showed perfectly that while a strong default can steer the model’s\nbehavior, a purely aggressive approach is too rigid and fails to bala",
  {
    "type": "image",
    "src": "/images/methodology/page_108_img_1.png",
    "alt": "Page 108 Image - Figure 33.",
    "caption": "Figure 33."
  },
  "nce accuracy with efficiency.\nAll of this is evidenced by the results shown in Figure 33.",
  "6.8.3    Instruction V3: Introducing Balanced Criteria",
  "The third iteration, Instruction V3, tried to strike a balance between the aggressive retrieval\nstrategy of V2 and the passive approach of V1. The goal was to try and improve efficiency by re-\nducing unnecessary retrievals without sacrificing the accuracy gains made on complex queries.\nThe key refinement was the introduction of explicit, positive categories for non-retrieval (”2-\nNo”).  For the first time , the model was given clear examples of queries that were meant to be\nanswer directly, such as ”Universally Known Facts”, ”Creative Tasks”, and ”General Explana-\ntions”.\nThis structured, two-step process first checks for a simple case, and only then defaulting to\nretrieval on more complex queries.  This proved to be a significant step in the right direction.\nThe model was no longer forced into an overly aggressive default and was instead required to",
  "reason through its decision-making process to select the appropriate path.",
  "The results depicted in Figure 35 reflect this new found balance. The Retrieval Task Routing\nAccuracy remained exceptionally high at around 98%, indicating that the safety-first principle\nfor complex questions was successfully maintained. The model correctly identified 1655 out of\n1688 queries that required retrieval.\nCrucially, the opposite task had been the main weakness of the previous iteration, and this\nremained true in the current version, with results d",
  {
    "type": "image",
    "src": "/images/methodology/page_109_img_1.png",
    "alt": "Page 109 Image - Figure 36: Instruction V4",
    "caption": "Figure 36: Instruction V4"
  },
  "eteriorating further the General Knowledge\nRouting Accuracy dropped from 26.07% to 21.27% on the General Knowledge Routing Accu-\nracy. This can also be seen on the number of times that the model picked ”retrieval” 1033 of the\n1312 general knowledge questions that don’t require it.",
  "6.8.4    Instruction V4: A Shift to Profile-Based Classification",
  "The mixed results presented on the previous iteration highlighted a potential weakness in\nthe sequential, rules-based checklist approach.  This new instruction V4 represented a major\nconceptual shift, this time re-framing the task following a procedure to a more holistic classi-\nfication exercise.  Instead of the previous step-by-step process, the model was now tasked with\nmatching the user’s query to one of two detailed profiles:  ”Profile 1:  Retrieval Required” or\n”Profile 2: Direct Answer Sufficient”.\nThis new profile-based structure is more intuitive for the LLM, as it leverages a core strength\nof these types of models pattern-matching.   The profiles provided a clearer,  more organized",
  "framework, and critically introduced the ”Recent Events” as a trigger for retrieval, this was the\napproach chosen to try and remedy the LLM knowledge cut-off from more recent knowledge.\nThe decision rule, however, still maintained a cautious stance, instructing the model to default\nto the safety of ”1-Yes” in cases of doubt or ambiguity.",
  "This new approach proved to be a big breakthrough. The results show a dramatically more\nbalanced and effective system as showed in Figure 37.\nThe General Knowledge Routing Accuracy saw a massive cr",
  "ucial improvement jumping\nfrom the previous 21.27% to 69.36%.  For the first time, the model could correctly identify the\nmajority of the questions that did not require retrieval.\nThis improvement came with a small trade-off.  The Retrieval Task Routing Accuracy saw\na slight dip from the near perfect levels of V2/V3, but still remaining very good at 91.88%.\nAs a result of this new balance, the Overall Routing Accuracy increased to 82.0% the highest\nand most effective level achieved so far indicating that the instruction was finally moving in the\nright direction.\nThe success of this version is best captured in the ”Routing System Vs.  Baseline Model”\nanalysis for general knowledge questions. This version of the routing system achieved a 95.27%\nanswer accuracy,  which was 20.35 percentage points better than the baseline for just model\nanswering on its own without any guiding instruction.  By successfully re-framing the task to\nalign with the LLM’s natural capabilities, this instruction created a far more reliable and smart\nclassification system.",
  "6.8.5    Instruction V5: Final Refinement with a Guiding Principle",
  "Building on the successful profile-based structure of V4, the fifth version was a final re-\nfinement aimed at maximizing reliability.  The core structure of the two profiles remained un-\nchanged, but a critical addition was made to the Decision Rule.  This new rule introduced an\nexplicit guiding principle to resolve the possible ambiguity:  ”A slow but correct answer is al-\nways better than a fast but wrong one.”.\nThis principle served as a powerful tie-breaker, forcing the idea of accuracy on to th",
  "e model.\nIt explicitly stated that if there was any ambiguity, or if a query even touched on the character-",
  "istics from ”Profile 1” (like a specific name or date), it must default to the safety of retrieval.\nThis approach was designed to try and solidify the instruction’s focus on producing the most\ntrustworthy assessment possible, even at the cost of possible decrease in efficiency.",
  "Figure 39: Instruction V5 Results\nThe performance data shows that this refinement had a subtle but measurable impact, tuning\nthe model’s behavior as it was intended.",
  "The model became slightly more cautious.  The Retrieval Task Routing Accuracy slightly\nincreased from 91.88% to 92.0%, which means the model identified 1553 of the 1688 queries\nthat required retrieval.  This increased caution also resulted in a minor decrease in the General\nKnowledge Routing Accuracy, which shifted from 69.36% to 67.07%.\nThe model was now slightly more likely to send a simple query for retrieval if it contained\nany element of ambiguity.  This adjustment was made based on the reasoning that the model\nmight still produce a correct answer even when retrieval is not strictly necessary. However, the\ninverse failing to retrieve when it is required almost always results in incorrect answers.  As\na consequence of this shift, a slight dip in accuracy was observed, with the Overall Routing\nAccuracy decreasing to 81.1%.\nDespite the minor shifts in routing metrics, the final answer quality for general knowledge\nquestions remained identical to that of V4, with the routing system achieving a 95.27% . Simi-\nlarly to V4, this instruction resulted in a 20.35 percentage point impr",
  "ovement over the baseline.\nThis shows that V5 successfully reinforced the system’s reliability.",
  "6.8.6    Instruction V6: A Strategic Pivot to Efficiency",
  "The final iteration, V6, represented a deliberate reversal from the ”accuracy-first” principle\nthat guided the previous version.  The main goal was explicitly re-focused to ”AVOIDING un-\nnecessary document retrieval” and to ”reduce incorrect ’1-Yes’ classifications”. This instruction\nwas designed to test a high-efficiency approach, that prioritizes speed and resource conservation\nfor general knowledge questions.\nTo achieve this, the core logic was inverted.  Non-retrieval (”2-No”) was made to be the\ndefault path, and the model was instructed to choose this approach unless a ”clear and definite\n’retrieve trigger’” was present on the query.  The burden of proof was shifted:  instead of de-\nfaulting to the safer retrieval in cases of ambiguity, the model now required compelling explicit\nreason to engage the retrieval system.",
  "Figure 41: Instruction V6 Results\nThis strategic change had a profound and predictable impact on the system’s performance\neffectively trading accuracy for efficiency.",
  "The instruction’s goal was a success for the General Knowledge Routing Accuracy making\nit surge to its highest point across all the previous versions, reaching and impressive 90.47%\nas depicted in Figure 41. The system was now exceptionally skilled at identifying and directly\nanswering simple queries without using wasteful processing when not required.\nThis efficiency came at a significant and expected cost.  The Retrieval Task Routing Accu-\nracy fell sharply from",
  "92.00% to a measly 58.77%. By no longer erring on the side of caution,\nthe system failed to identify a large portion of queries that genuinely required the retrieval of\nexternal information.\nAs a result of this trade-off , the Overall Routing Accuracy dropped to 72.6%.\nInterestingly, despite the lower routing accuracy for complex queries, this approach achieved\nthe highest final answer accuracy on general knowledge questions with 97.18%.  This was a\n22.26 percentage points over the baseline answers. This outcome shows that by correctly routing\na very high volume of simple questions to the direct-answer paths, the system maximized the\nLLm’s ability to leverage its own knowledge effectively, this was also helped with the Chain-\nof-Thought instruction that improved the base model answer without requiring any external\ninformation.\nThis last experiment shows the high degree of control that prompt engineering provides on\nsuch a system and even on the LLM’s responses. V6 is not inherently better or worse than V5, it\nsimply optimized and constructed with a different objective in mind. The choice between these\ntwo mature instructions depends entirely on the desired system behavior. Which aligns with the\npurpose of this research prioritizing efficiency whenever the trade-off proves to be worthwhile.\nV5 is the ideal choice for a system where reliability and avoiding factual errors are paramount,\nwhile the V6 version is superior for a system where efficiency and speed in handling common\nqueries are the up most concern.\n6.9    Analysis of Energy Consumption and Efficiency\nA holistic view of the system’s performance is best captured by plotting th",
  "e total energy\nconsumed versus the number of correct answers that the system outputted.  The resulting scat-\nter plot reveals a fundamental trade-off inherent in the system’s operation:  achieving a higher\nnumber of correct answers of the provided dataset is directly correlated with increased in energy\nconsumption (see Figure42).",
  "Figure 42: Energy Costs vs. Correctness Scatterplot\nThis  is  clearly  lustrated  in  the  progression  from  the  baseline  models,  which  occupy  the\nlower-left quadrant of the graph which represents both the lowest energy consumption and the\nlowest correctness.  Although one part of the baseline achieved high correctness, this may be\npartly due to the way correctness was assigned to queries requiring retrieval. In this evaluation,\nif retrieval was triggered for a query that required it, the system marked the answer as correct\nregardless of whether the retrieved content actually led to a correct response.  This introduces\na significant limitation when interpreting the results.  If correctness had instead been evaluated\nbased on the accuracy of the retrieved content itself, the score would likely have been much\nlower and more in line with the other two baseline results. Another important factor is that, since\nthe retrievable documents come from Wikipedia, many of them coincidentally align with ARC-\nEasy queries. For example, in Query 3, the retrieved document happens to contain information\nthat, through reasoning, leads to the correct answer. This pattern appears in multiple ARC-style\nqueries and may inflate the apparent effectiveness of the baseline.  These two points boost the\nanswer correctness by a",
  "lot for this specific file thus should be looked at as a skewed result far\nfrom the truth.\nAt the upper-right , the system demonstrates its ability to improve both the model’s correct-\nness and the overall quality of the answers.  However, one outlier stands out: System Analysis\nV1.  This version reveals a particularly inefficient instruction, likely due to how open it was\nto interpretation.  As a result, the model engaged in excessive reflection while still failing to\nchoose the appropriate approach for each query type.  This led to a significantly lower number\nof correct answers compared to later, more refined instruction versions.",
  "An important takeaway is that the optimization process was not intended to reduce energy\nconsumption, but rather to maximize the productive use of that energy aiming to yield the most\naccurate results possible.\nFigure 43: Energy Costs vs. Correctness Scatterplot\nLooking further into the energy dynamics of this project, an analysis of the average energy\nper  query  reveals  a  striking  and  consistent  pattern,  incorrect  answers  are  consistently  more\nenergy-intensive than correct ones.  This suggests that incorrect answers are often the result of\ninefficient processing, such as retrieving irrelevant documents, pursuing flawed reasoning paths,\nor struggling to analyze conflicting information.  In contrast, correct answers appear to follow\na more direct and energetically efficient path. With one exception, the straight\nmodel baseline,\nwhere the correct answers consume slightly more energy.   This could be because,  when the\nmodel is more confident in its answer,  it tends to generate longer,  mor",
  "e detailed responses,\nwhich in turn require more energy than the simpler, shorter incorrect ones.",
  "Figure 44: Domain Average Energy per Correct Answer\nFigure 45: Domain Average Energy per Incorrect Answer\nFurther analysis of the results, now split by the query’s original dataset (domain), reveals\nanother clear efficiency trend: queries related to ’General Knowledge’ consistently require more\nenergy than those from the ’Science’ domain.  This pattern is still present for both correct and\nincorrect answers, as can be seen in both Figure 44, and 45.  For the more refined instructions\n(V2 through V6),  the energy cost for answering a general knowledge question is noticeably\nhigher than for a science question.  This disparity likely stems from the increased complexity\nof the general knowledge queries, which typically require retrieval to be answered.  This adds",
  "further  computational  overhead,  as  the  number  of  tokens  the  model  must  process  increases\ndue to the inclusion of retrieved documents alongside the instruction.  This occurs even after\norganizing the retrieved documents from most to least relevant, and removing any unnecessary\nexpressions that do not contribute to the quality of the response.   On the other hand,  in the\nScience domain,  the number of input tokens is always lower since no retrieval is performed\nwhen the system functions correctly, thereby reducing the total input tokens.\nIn summary, the energy consumption analysis provides a comprehensive, multi-dimensional\nview of the system’s efficiency.  It demonstrates that efficiency is not a single metric but a bal-\nance of multiple factors. The inherent com",
  "plexity of the query’s domain sets a baseline for en-\nergy consumption, with ”General Knowledge” questions proving to be more resource-intensive\ndue to the required retrieval process needed to answer them correctly. UGiven this baseline, the\neffectiveness of the instructional prompt plays a pivotal role in how efficiently the energy is uti-\nlized. Well-calibrated instructions help guide the model down more efficient pathways, leading\nto correct answers at a lower average energy cost. This demonstrates that query editing could be\na promising area of study for improving model efficiency. While also proving that ambiguity or\nflawed logic results in wasted energy on incorrect outputs.  Ultimately, this analyses reinforces\nthat the iterative refinement of prompts is not merely a quest for higher accuracy, but a method\nfor controlling the crucial trade-off between correctness and computational cost, while also al-\nlowing for the strategic selection of a system profile that best aligns with the desired balance of\nperformance and resource conservation.\n6.10    Detailed Energy Consumption Profiles\n6.10.1    Overall Energy Trends Across Instruction Versions\nA foundational analyses of the the system’s energy consumption begins with the average\nenergy consumed per query for each experimental version of the instructions,  as showed in\nFigure 46. This shows a clear high-level comparison of the computational cost associated with\neach iteration of the instructions against the baseline models.",
  "Figure 46: Average Energy Consumption per Query by File",
  ".  The introduction of the first routing instruction, V1, immediately results in a\nsignificant spike in energy consumption to nearly 2.0 Wh. This aligns with its characterization\nas a inefficient, open-ended prompt that likely caused extensive and unguided model processing.\nAs  the  instructions  were  refined  from  V2  to  V5,  the  average  energy  fluctuated  between\n1.45 and 1.9 Wh.   This demonstrates that the added complexity of the routing system,  even\nwhen optimized for accuracy,  carries a consistent energy overhead compared to the simpler\nbaseline approaches as expected since it requires the model to output two responses for just\none query.  Interestingly, Instruction V6, which was explicitly designed to enhance efficiency\nby trying to reduce the usage off unnecessary retrievals, results on the highest average energy\nconsumption at around 2.2 Wh.  misconception during the creation of the instruction, as it was\ninitially assumed that the retrieval process would be the most energy-intensive.  However, the\nresults show that although retrieval does contribute to energy consumption, it represents only a\nsmall portion of the overall cost.  Another mistake made during the creation of this instruction\nwas that it ended up heavily relying on an internal chain-of-though process to answer general\nknowledge queries, which resulted in higher energy consumption. However, it also led to faster\nresponse times contradicting some of the assumptions made during the instruction’s design.",
  "Figure 47: Total Energy Percentage Difference from Baseline\nThis increased energy overhead is clearly illustrated in Figure 47.  The graph shows that,\nc",
  "ompared to the baseline, the more advanced routing systems consistently increase total energy\nconsumption  by  over 40%.   This  underscores  a  critical  finding,  that  this  implementation  of\nan intelligent routing layer,  while substantially improving answer accuracy and reliability,  it\nalso presents a significantly and quantifiable trade-off in terms of computational and energy\ncost.  While this was already considered at the start of this endeavor, the main idea is to try\nand compete with much larger models that, on average, require significantly more energy and\ncomputational power. A deeper analysis of this will be carried out at a later stage.\n6.10.2    CPU vs. GPU: Deconstructing the Energy Cost\nTo try and understand the nature of the energy overhead introduced by the routing system,\nit is essential to deconstruct the total energy consumption into its primary hardware compo-\nnents, the Central Processing Unit (CPU) and the Graphics Processing Unit (GPU). The CPU\nnormally handles data processing, I/O operations, and logical orchestration, while the GPU is\nresponsible for parallel computations required by the model inference. The Figure 48 illustrates\nthis breakdown for each system version.",
  "Figure 48: Total Energy Consumption by File (CPU vs GPU)\nA clear pattern emerges from the data, the baseline ”straight\nmodel”, which relies almost ex-\nclusively on model inference, shows the lowest relative CPU energy consumption. On the other\nhand, all subsequent versions that incorporate either forced retrieval or CoT or the intelligent\nrouting system exhibit an increase in the proportion of energy consumed by the CPU.\nConversely,",
  "the  GPU  energy  consumption  scales  more  directly  with  the  complexity  and\nlength of the generation required from the LLM. The V6 instruction, which was designed to\nfavor internal Chain-of-Thought reasoning over retrieval, shows the highest energy consump-\ntion driven by a massive increase in GPU usage.  This shows that while avoiding CPU-heavy\nretrieval processes, version V6 shifts most of the burden to the GPU, requiring it to perform\nmore extensive and energy-demanding computations to generate the answers.  As previously\nobserved, this also resulted in poorer performance compared to earlier iterations.\nThis analysis reveals that the choice of instruction foes not just how much energy is con-\nsumed, but also where it is used.  A retrieval heavy strategy taxes the CPU, while a reasoning\nheavy strategy taxes the GPU. This distinction is critical for system optimization, as it shows\nhow different prompts and engineering strategies can create distinct hardware usage profiles.\n6.10.3    The Energetic Cost of Correcting Errors\nLooking beyond the general energy profiles, a more targeted analysis reveals the specific\nenergy consumption required for the system to add value that is, to correct an answer that the\nbaseline model would have gotten wrong. This scenario represents the core justification for this\nwhole approach.",
  "Figure 49:  General Knowledge:  Avg.  Energy when Straight Model is Incorrect & System is\nCorrect\nFigure 49 provides a quantitative analysis of the ’correction cost’ associated with ’General\nKnowledge’ queries.  When the baseline fails while consuming around 0.95 Wh, the various\nrouting systems successfully pro",
  "vided a correct answer, although this came with a significantly\nhigher energy cost, ranging from 1.5 Wh (V2) to a peak of over 2.1 Wh (V1 and V6). Showing\nthat overcoming the baseline’s knowledge gaps via retrieval is an intensive operation.  Though\nlike explained previously this lacks a better understanding since the evaluation of this domain\nis just that retrieval occurred, so the straight model never got a correct answer due to that fact.\nHowever, we can still compare them, and the V6 instruction stands out once again due to its high\nenergy cost.  This is primarily attributed to its reliance on a detailed chain-of-thought process\nwhich, while somewhat effective, is significantly more computationally demanding.",
  "Figure 50: Science: Avg. Energy when Straight Model is Incorrect & System is Correct\nA  similar  trend  is  observed  in  the  Science  domain,  as  shown  in  Figure  60.   Correcting\nthe baseline in this approach also required a substantial energy investment, with the V6 again\nshowing the highest consumption at nearly 2.5 Wh.  Thus reinforcing that the act of correcting\nthe responses regardless of the domain is inherently more energy demanding.\nFigure 51: Science: Avg. Energy when Straight Model is Incorrect & System is also Incorrect\nOn the other hand Figure 51 analyzes the scenario where both the baseline and the system\nfailed to produce a correct answer. This represents the least efficient use of energy, showing that",
  "the system uses additional resources only to arrive at the same incorrect outcome. It is notewor-\nthy that the energy consumed in these failure cases is often comparable to and sometimes even\nhigh",
  "er than the energy required to produce correct answers. As proof, the V6 system consumes\nover 2.6 Wh when it fails, more than it does for a successful answer which is around 2.5 Wh.\nThis might suggest that these incorrect answeres may result from the system pursuing particu-\nlarly complex, yet flawed, reasoning paths or even retrieving irrelevant information, leading to\nwasted resources.\n6.10.4    Energy Distribution and Consumption Predictability\nWhile the average energy consumption provides a useful high-level metric,  a deeper un-\nderstanding of the system efficiency is required to access its consistency and predictability.  A\nsystem with a low average cost is less desirable if it is prone to higher spikes.  The boxplot in\nFigure 52 provides valuable insight into all the versions by visually representing the distribution\nof energy consumption per query for each one.\nFigure 52: Distribution of Energy Consumption per Query by File\nThe baseline models,  particularly the ”straightmodel”,  presents the tightest distribution.\nThe small inter quartile range shows that most of the queries are processed using a very con-\nsistently and predictably amount of energy. This helps them in terms of reliability since from a\nresource planing perspective they are very predictable, though they show lower accuracy.\nOn the opposite side of the spectrum we have the initial routing Instruction V1, that demon-\nstrates extreme unpredictability.  It has by far the largest inter-quartile range and a long upper",
  "whisker, indicating a massive variance in energy consumption.  This proves that its ambiguity\nled to highly inefficient and erratic processing.\nT",
  "he following versions from V2 to V5 show a clear trend into an increasing predictabil-\nity. While they show a higher median energy cost than the baseline, their distributions become\ntighter and tighter as versions increase.  This shows the core benefit of the iterative refinement\nprocess,  reflecting precisely what was discussed previously as the instructions became more\nspecific and rule-based the model’s behavior became more constrained to the rules, therefore\nmore predictable.  The number of high energy outliers that caused an increase in system’s re-\nsource expenditure also decreased indicating a more robust and stable approach.\nLastly the V6 instruction, designed for efficiency presents a rather unique profile.  While\nits median energy is high, its distribution is relatively contained in comparison with V1, this\nsuggests  that  its  Chain-of-Thought  process,  though  energy  intensive,  is  being  applied  more\nconsistently.\nUltimately, this analysis underscores that effective prompt engineering does more than just\nimprove accuracy, as this proves that it enhances operational predictability.  A well designed\ninstruction set not only guides the model to the correct answer but also ensures that the model\ndoes so consistently while using a manageable level of resource consumption, which is a crit-\nical factor when picking and deploying such systems in the real world, especially in resource\nconstrained environments\n6.10.5    Overall Performance Quadrant:  Synthesizing Accuracy and Efficiency for ARC\nqueries\nThe culmination of this analysis is best visualized in the performance quadrant plot, which\nvisually presents each system’s fi",
  "nal correctness rate against its average energy cost for the ARC\nqueries.  The graph present in Figure 53, offers a clear overview of the trade-offs and situates\nthe performance of the 8B parameter model (DeepSeek-R1-Distill-Llama-8B) used as the base\nand all the routing systems routing systems against a crucial benchmark, a much larger 14B\nparameter model (DeepSeek-R1-Distill-Qwen-14B).",
  "Figure 53: Overall Performance Overview: Correctness vs. Energy Cost for ARC queries\nThe optimal position on this graph is at the top-left quadrant, which represents the highest\naccuracy with the lowest energy consumption.  The bottom-right represents the inverse so the\nworst possible outcome .  The baseline models establish two reference points.  The first is the\n”straightmodel” (8B), sits in the lower left quadrant, confirming it is a low accuracy but highly\nefficient  option.   On  the  other  hand  ,  at  the  upper-right  corner  sits  the  ”straight  model  14B\n8-bit” demonstrating the brute force approach as it achieves a very high correctness of nearly\n90%, however this comes with the cost of an enormous amount of energy at around 3.7 Wh per\nquery,making it by far the least efficient model\nThe iterative refinement of the routing instructions charts a clear journey toward the ideal\nquadrant.  The initial, poorly tuned instructions (V1,V2,V3) show modest accuracy gains over\nthe 8B baseline but at a notable energy cost, placing them in the lower-middle of the graph.\nThe major breakthrough occurs with instruction V4 and later V5. These versions represent\nan optimal sweet spot, achieving a substantial leap in terms of correctness to over 83%",
  "while\nstill maintaining an average cost below 1.8 Wh.  Most importantly, they deliver a significant\nportion of the accuracy gains of the 14B model while consuming less than half the energy.\nThe efficiency-focused V6 instruction pushes the accuracy to over 85%,  however it also\nincreased the energy cost, moving the results slightly to the right. While it is the most accurate\nof the routing instructions, it begins to approach a point of diminishing returns in the accuracy-",
  "efficiency trade off.\nTo conclude, this analysis of the quadrant provides the most complete validation of the re-\nsearch.  It shows that an intelligent routing layer with a combination of a carefully engineered\nprompts  are  not  just  incremental  improvements.   They  are  part  of  a  transformative  strategy\nof optimization.   By using carefully designed instructions to guide a smaller,  more efficient\n8B model, the system achieves a performance profile that rivals a modelnearlyt twice its size,\nbut at a fraction of the computational and energy costs.  This goes to show that architectural\nand instructional refinement can be a more sustainable and effective approach to high perfor-\nmance,rather than simply scaling up the model size.\n7    Experimental Consistency and Reproducibility\nTo guarantee that the performance and energy consumption results represented in this thesis\nare  both  valid  and  reliable,  a  strict  protocol  was  established  to  try  to  minimize  the  number\nof  variables  and  create  a  consistent  testing  environment  for  all  experiments.   The  following\nmeasures  were  systematically  implemented  to  ensure  that  the  results  sh",
  "own  can  be  directly\nattributed to the changes in the system’s instruction design and model performance.\nFirst, the test system was maintained in a controlled and isolated state. To prevent interfer-\nence from many background tasks associated with other programs, and also with the operating\nsystem tasks, such as automatic updates or network-related tasks, the machine’s internet con-\nnection was physically disabled for the duration of all test runs. To further improve repeatability,\nprior to initiating any experiment, all non-essential background applications and services were\nfully terminated.  The system was then left without any intervention after the scripts started up\nuntil they were finished and all the required data was collected. This ensured that the system’s\ncomputational resources were devoted exclusively to the experimental workload.\nAnother point taken to maintain the system repeatability was that a static software envi-\nronment  was  kept  during  the  entire  research  process.   The  main  components  of  the  system,\nincluding the Python interpreter, the PyCharm IDE, and the HWiNFO monitoring utility, were\nnot updated after the initial setup. This strategy is crucial to eliminate the risk of major software\nupdates introducing performance variations that could skew the results.\nFinally,  and  most  critically,  the  underlying  code  base  for  the  query-routing  system  and\nmodel inference remained identical across all comparative experiments.  When testing the ef-\nficacy of different instructions, the sole modification between each run was the content of the\nsystem instruction itself.  This strict isolation of the",
  "independent variable ensures that all mea-\nsured differences in answer accuracy, latency and energy consumption are direct consequences\nof the prompt engineering strategy, rather than unintended changes in the software that supports\nit.",
  "8    Challenges and Abandoned Approaches\nIn the course of this research, some promising strategies were explored but ultimately aban-\ndoned due to practical constraints, resource limitations, or conflicts with the project’s core ob-\njectives.  This section represents these methodological dead ends, as understanding what these\nwere and their causes could aid further research.\nOne of the initial strategies considered for enhancing the RAG component was the usage of\na hypothetical document similar to HyDE[40]. The technique, which involves generating a hy-\npothetical answer to a query to improve the semantic search for relevant documents, has shown\npromise in other projects. However, the original HyDE (Answer RQ2) paper recommended the\ngeneration of up to eight hypothetical documents per retrieval to achieve optimal performance.\nIn practice,  this approach proved to be prohibitively expensive for the presented framework.\nThe computational overhead of generating eight separate documents before the retrieval was\ndone and the formation of the final answer would have dramatically increased both energy con-\nsumption and latency,  with the latter representing a change that would make the framework\nbasically unusable with the current hardware. This would directly undermine the primary goal\nof creating a fast and efficient system for resource-constrained environments.\nAnother considered approach was the us"
]
  },,
      {
        type: 'image',
        src: '/images/methodology/page_88_img_1.png',
        alt: 'LLM in Information Retrieval',
        caption: 'LLM in Information Retrieval'
      },
      "Large Language Models have shown exceptional capabilities in understanding and generating human-like language. However, deploying these models in real-world, regulated environments presents substantial challenges. Organizations like banks, hospitals, and government offices are likely to handle sensitive information that should not exit their premise under strict data privacy laws.",
      {
        type: 'image',
        src: '/images/methodology/Figure 26 Traditional information retrieval architecture[68].png',
        alt: 'Figure 26: Traditional information retrieval architecture',
        caption: 'Figure 26: Traditional information retrieval architecture[68]'
      },
      // System Architecture section (items 4-5)
      "The system architecture consists of a rewriter module that processes incoming queries and determines the most appropriate routing strategy. The framework employs three distinct approaches: Straight LLM, Chain-of-Thought, and RAG.",
      {
        type: 'image',
        src: '/images/methodology/Figure 27 Graphical representation of the system diagram[68].png',
        alt: 'Figure 27: Graphical representation of the system diagram',
        caption: 'Figure 27: Graphical representation of the system diagram[68]'
      },
      "The three approaches work as follows: If the model concludes that it can answer directly then the model proceeds to generate the answer without the need of a new generation, meaning only one hop. As for Chain-of-Thought, the approach works by injecting a command to try and force the model into thinking step by step to obtain the answer. The final approach is Retrieval-Augmented Generation (RAG), which is also the most complex.",
      // Experimental Setup section (item 6)
      "The experimental setup involves hardware and software configurations optimized for energy-efficient inference. The system runs on a single-GPU workstation with specific power monitoring capabilities.",
      // Routing Mechanism section (items 7-10)
      "The routing mechanism employs an intelligent controller that analyzes each query and selects the most appropriate approach. The selection is based on iterative prompt refinement through six instruction designs.",
      {
        type: 'image',
        src: '/images/methodology/Figure 28 Analysis Prompt.png',
        alt: 'Figure 28: Analysis Prompt',
        caption: 'Figure 28: Analysis Prompt'
      },
      "The voting mechanism ensures robust decision extraction by aggregating multiple classification attempts. The validation framework includes automated preparation, AI-powered triage, and human verification steps.",
      {
        type: 'image',
        src: '/images/methodology/Figure 29 Jsonl Data Structure.png',
        alt: 'Figure 29: Jsonl Data Structure',
        caption: 'Figure 29: Jsonl Data Structure'
      },
      {
        type: 'image',
        src: '/images/methodology/page_90_img_1.png',
        alt: 'Page 90 Image',
        caption: 'Page 90'
      },
      // Instruction Design section (items 14-29)
      "The performance of an LLM in query classification depends heavily on the design of the instruction prompt. Through iterative refinement, six versions of instructions were developed, each improving upon the previous iteration.",
      // Instruction V1
      {
        type: 'image',
        src: '/images/methodology/Figure 30 Instruction V1.png',
        alt: 'Figure 30: Instruction V1',
        caption: 'Figure 30: Instruction V1 - A Simple Baseline'
      },
      "Instruction V1 serves as a simple baseline, providing basic routing instructions without complex heuristics.",
      {
        type: 'image',
        src: '/images/methodology/Figure 31 Instruction V1 Results.png',
        alt: 'Figure 31: Instruction V1 Results',
        caption: 'Figure 31: Instruction V1 Results'
      },
      // Instruction V2
      {
        type: 'image',
        src: '/images/methodology/Figure 32 Instruction V2.png',
        alt: 'Figure 32: Instruction V2',
        caption: 'Figure 32: Instruction V2 - An Aggressive, Safety-First Heuristic'
      },
      "Instruction V2 introduces an aggressive, safety-first heuristic that prioritizes retrieval when uncertainty is detected.",
      {
        type: 'image',
        src: '/images/methodology/Figure 33 Instruction V2 Results.png',
        alt: 'Figure 33: Instruction V2 Results',
        caption: 'Figure 33: Instruction V2 Results'
      },
      // Instruction V3
      {
        type: 'image',
        src: '/images/methodology/Figure 34 Instruction V3.png',
        alt: 'Figure 34: Instruction V3',
        caption: 'Figure 34: Instruction V3 - Introducing Balanced Criteria'
      },
      "Instruction V3 introduces balanced criteria, attempting to find a middle ground between efficiency and accuracy.",
      {
        type: 'image',
        src: '/images/methodology/Figure 35 Instruction V3 Results.png',
        alt: 'Figure 35: Instruction V3 Results',
        caption: 'Figure 35: Instruction V3 Results'
      },
      {
        type: 'image',
        src: '/images/methodology/page_92_img_1.png',
        alt: 'Page 92 Image',
        caption: 'Page 92'
      },
      // Instruction V4
      {
        type: 'image',
        src: '/images/methodology/Figure 36 Instruction V4.png',
        alt: 'Figure 36: Instruction V4',
        caption: 'Figure 36: Instruction V4 - A Shift to Profile-Based Classification'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 37 Instruction V4 Results.png',
        alt: 'Figure 37: Instruction V4 Results',
        caption: 'Figure 37: Instruction V4 Results'
      },
      "Instruction V4 represents a shift to profile-based classification, where queries are analyzed based on their characteristics and requirements.",
      {
        type: 'image',
        src: '/images/methodology/page_94_img_1.png',
        alt: 'Page 94 Image',
        caption: 'Page 94'
      },
      // Instruction V5
      {
        type: 'image',
        src: '/images/methodology/Figure 38 Instruction V5.png',
        alt: 'Figure 38: Instruction V5',
        caption: 'Figure 38: Instruction V5 - Final Refinement with a Guiding Principle'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 39 Instruction V5 Results.png',
        alt: 'Figure 39: Instruction V5 Results',
        caption: 'Figure 39: Instruction V5 Results'
      },
      "Instruction V5 incorporates a guiding principle that helps the model make more consistent routing decisions.",
      {
        type: 'image',
        src: '/images/methodology/page_96_img_1.png',
        alt: 'Page 96 Image',
        caption: 'Page 96'
      },
      // Instruction V6
      {
        type: 'image',
        src: '/images/methodology/Figure 40 Instruction V6.png',
        alt: 'Figure 40: Instruction V6',
        caption: 'Figure 40: Instruction V6 - A Strategic Pivot to Efficiency'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 41 Instruction V6 Results.png',
        alt: 'Figure 41: Instruction V6 Results',
        caption: 'Figure 41: Instruction V6 Results'
      },
      "Instruction V6 represents a strategic pivot to efficiency, optimizing for both accuracy and energy consumption.",
      {
        type: 'image',
        src: '/images/methodology/page_98_img_1.png',
        alt: 'Page 98 Image',
        caption: 'Page 98'
      },
      // Energy Analysis section (items 30+)
      "Energy consumption analysis reveals important trade-offs between different routing strategies and instruction designs. The framework tracks both CPU and GPU energy consumption to provide a comprehensive view of system efficiency.",
      {
        type: 'image',
        src: '/images/methodology/page_100_img_1.png',
        alt: 'Page 100 Image',
        caption: 'Page 100'
      },
      {
        type: 'image',
        src: '/images/methodology/page_101_img_1.png',
        alt: 'Page 101 Image',
        caption: 'Page 101'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 42 Energy Costs vs. Correctness Scatterplot.png',
        alt: 'Figure 42: Energy Costs vs. Correctness Scatterplot',
        caption: 'Figure 42: Energy Costs vs. Correctness Scatterplot'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 43 Energy Costs vs. Correctness Scatterplot.png',
        alt: 'Figure 43: Energy Costs vs. Correctness Scatterplot',
        caption: 'Figure 43: Energy Costs vs. Correctness Scatterplot'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 44 Domain Average Energy per Correct Answer.png',
        alt: 'Figure 44: Domain Average Energy per Correct Answer',
        caption: 'Figure 44: Domain Average Energy per Correct Answer'
      },
      {
        type: 'image',
        src: '/images/methodology/page_103_img_1.png',
        alt: 'Page 103 Image',
        caption: 'Page 103'
      },
      {
        type: 'image',
        src: '/images/methodology/page_104_img_1.png',
        alt: 'Page 104 Image',
        caption: 'Page 104'
      },
      {
        type: 'image',
        src: '/images/methodology/page_105_img_1.png',
        alt: 'Page 105 Image 1',
        caption: 'Page 105 Image 1'
      },
      {
        type: 'image',
        src: '/images/methodology/page_105_img_2.png',
        alt: 'Page 105 Image 2',
        caption: 'Page 105 Image 2'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 45 Domain Average Energy per Incorrect Answer.png',
        alt: 'Figure 45: Domain Average Energy per Incorrect Answer',
        caption: 'Figure 45: Domain Average Energy per Incorrect Answer'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 46 Average Energy Consumption per Query by File.png',
        alt: 'Figure 46: Average Energy Consumption per Query by File',
        caption: 'Figure 46: Average Energy Consumption per Query by File'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 47 Total Energy Percentage Difference from Baseline.png',
        alt: 'Figure 47: Total Energy Percentage Difference from Baseline',
        caption: 'Figure 47: Total Energy Percentage Difference from Baseline'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 48 Total Energy Consumption by File (CPU vs GPU).png',
        alt: 'Figure 48: Total Energy Consumption by File (CPU vs GPU)',
        caption: 'Figure 48: Total Energy Consumption by File (CPU vs GPU)'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 49 General Knowledge Avg. Energy when Straight Model is Incorrect & System is Correct.png',
        alt: 'Figure 49: General Knowledge Avg. Energy when Straight Model is Incorrect & System is Correct',
        caption: 'Figure 49: General Knowledge Avg. Energy when Straight Model is Incorrect & System is Correct'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 50 Science Avg. Energy when Straight Model is Incorrect & System is Correct.png',
        alt: 'Figure 50: Science Avg. Energy when Straight Model is Incorrect & System is Correct',
        caption: 'Figure 50: Science Avg. Energy when Straight Model is Incorrect & System is Correct'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 51 Science Avg. Energy when Straight Model is Incorrect & System is also Incorrect.png',
        alt: 'Figure 51: Science Avg. Energy when Straight Model is Incorrect & System is also Incorrect',
        caption: 'Figure 51: Science Avg. Energy when Straight Model is Incorrect & System is also Incorrect'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 52 Distribution of Energy Consumption per Query by File.png',
        alt: 'Figure 52: Distribution of Energy Consumption per Query by File',
        caption: 'Figure 52: Distribution of Energy Consumption per Query by File'
      },
      {
        type: 'image',
        src: '/images/methodology/Figure 53 Overall Performance Overview Correctness vs. Energy Cost for ARC queries.png',
        alt: 'Figure 53: Overall Performance Overview Correctness vs. Energy Cost for ARC queries',
        caption: 'Figure 53: Overall Performance Overview Correctness vs. Energy Cost for ARC queries'
      },
      {
        type: 'image',
        src: '/images/methodology/page_107_img_1.png',
        alt: 'Page 107 Image',
        caption: 'Page 107'
      },
      {
        type: 'image',
        src: '/images/methodology/page_108_img_1.png',
        alt: 'Page 108 Image',
        caption: 'Page 108'
      },
      {
        type: 'image',
        src: '/images/methodology/page_109_img_1.png',
        alt: 'Page 109 Image',
        caption: 'Page 109'
      },
    ],
  },
  results: {
    title: "Results",
    content: [
  "efficiency trade off.\nTo conclude, this analysis of the quadrant provides the most complete validation of the re-\nsearch.  It shows that an intelligent routing layer with a combination of a carefully engineered\nprompts  are  not  just  incremental  improvements.   They  are  part  of  a  transformative  strategy\nof optimization.   By using carefully designed instructions to guide a smaller,  more efficient\n8B model, the system achieves a performance profile that rivals a modelnearlyt twice its size,\nbut at a fraction of the computational and energy costs.  This goes to show that architectural\nand instructional refinement can be a more sustainable and effective approach to high perfor-\nmance,rather than simply scaling up the model size.\n7    Experimental Consistency and Reproducibility\nTo guarantee that the performance and energy consumption results represented in this thesis\nare  both  valid  and  reliable,  a  strict  protocol  was  established  to  try  to  minimize  the  number\nof  variables  and  create  a  consistent  testing  environment  for  all  experiments.   The  following\nmeasures  were  systematically  implemented  to  ensure  that  the  results  sh",
  "own  can  be  directly\nattributed to the changes in the system’s instruction design and model performance.\nFirst, the test system was maintained in a controlled and isolated state. To prevent interfer-\nence from many background tasks associated with other programs, and also with the operating\nsystem tasks, such as automatic updates or network-related tasks, the machine’s internet con-\nnection was physically disabled for the duration of all test runs. To further improve repeatability,\nprior to initiating any experiment, all non-essential background applications and services were\nfully terminated.  The system was then left without any intervention after the scripts started up\nuntil they were finished and all the required data was collected. This ensured that the system’s\ncomputational resources were devoted exclusively to the experimental workload.\nAnother point taken to maintain the system repeatability was that a static software envi-\nronment  was  kept  during  the  entire  research  process.   The  main  components  of  the  system,\nincluding the Python interpreter, the PyCharm IDE, and the HWiNFO monitoring utility, were\nnot updated after the initial setup. This strategy is crucial to eliminate the risk of major software\nupdates introducing performance variations that could skew the results.\nFinally,  and  most  critically,  the  underlying  code  base  for  the  query-routing  system  and\nmodel inference remained identical across all comparative experiments.  When testing the ef-\nficacy of different instructions, the sole modification between each run was the content of the\nsystem instruction itself.  This strict isolation of the",
  "independent variable ensures that all mea-\nsured differences in answer accuracy, latency and energy consumption are direct consequences\nof the prompt engineering strategy, rather than unintended changes in the software that supports\nit.",
  "Challenges and Abandoned Approaches\nIn the course of this research, some promising strategies were explored but ultimately aban-\ndoned due to practical constraints, resource limitations, or conflicts with the project’s core ob-\njectives.  This section represents these methodological dead ends, as understanding what these\nwere and their causes could aid further research.\nOne of the initial strategies considered for enhancing the RAG component was the usage of\na hypothetical document similar to HyDE[40]. The technique, which involves generating a hy-\npothetical answer to a query to improve the semantic search for relevant documents, has shown\npromise in other projects. However, the original HyDE (Answer RQ2) paper recommended the\ngeneration of up to eight hypothetical documents per retrieval to achieve optimal performance.\nIn practice,  this approach proved to be prohibitively expensive for the presented framework.\nThe computational overhead of generating eight separate documents before the retrieval was\ndone and the formation of the final answer would have dramatically increased both energy con-\nsumption and latency,  with the latter representing a change that would make the framework\nbasically unusable with the current hardware. This would directly undermine the primary goal\nof creating a fast and efficient system for resource-constrained environments.\nAnother considered approach was the us"
]
  },,
  conclusions: {
    title: "Conclusions",
    content: [
  "e of keyword injection to improve the model’s per-\nformance on specialized, domain-specific tasks.  The hypothesis was that by programmatically\ninserting key technical terms (e.g., medical terminology for healthcare implementations) into\nthe  prompt,  the  model  could  be  guided  toward  a  more  accurate  and  contextually  aware  re-\nsponse. This idea was ultimately abandoned due to the immense data curation effort this would\nneed.  To be effective, this strategy would necessitate the creation of a large, validated dataset\nmapping queries to the required keywords for each domain if various were to be involved in\ntesting.  Getting hold and verifying the accuracy of this much data would be a substantial re-\nsearch project in itself, and it felt outside of the scope of this project which was focused more\ntowards the general public.\nFinally, the scope of model comparison was limited by the hardware available for this re-\nsearch.  While the study successfully demonstrates the capabilities of an 8B parameter model\non a single GPU workstation, a more comprehensive analysis would have included benchmarks\nwith higher models as well, which would normally require enterprise-grade GPUs.  Such tests\nwere not conducted due to a lack of access to these powerful computational resources.  As a\nresult, the performance comparison remains focused on demonstrating the significant leap from\nsmaller models to the proposed SLM framework, rather than a broader spectrum of available\nopen-source models.",
  "9    Conclusion\nThis  thesis  confronted  the  significant  challenge  of  deploying  powerful  language  models\nwithin environments constrained by li",
  "mited computational resources, strict data privacy regu-\nlations, and tight budgets. The prohibitive cost and operational complexities of state-of-the-art\nLarge Language Models pose a significant barrier for small to medium enterprises and regulated\ninstitutions.   In  response,  this  research  proposed  and  evaluated  a  novel  framework  (Answer\nRQ1) centered on a compact, 8 billion parameter Small Language Model.  The core innova-\ntion of this work is a dynamic, adaptive query routing system that intelligently triages incoming\nqueries, selecting the most efficient and effective path be it a direct answer, a reasoning-intensive\nchain of thought, or knowledge augmentation using retrieval-augmented generation.\nThe results of this study show that architectural control and sophisticated prompt engineer-\ning can substantially bridge the performance gap between small, quantized models and their\nlarger, more resource-intensive counterparts.  Through iterative refinement of the controller’s\ninstruction design, particularly with profile-based prompts, (Answer RQ3) the system achieved\nan  accuracy  exceeding 85%  on  reasoning-focused  science  questions.   Critically,  this  perfor-\nmance was achieved with significantly greater energy efficiency than would be possible using\nlarger models. The detailed energy profiling revealed a direct correlation between incorrect an-\nswers and higher energy consumption, and also showed how different instructions strategically\nshift the computational load between CPU-intensive retrieval and GPU-intensive generation.\nThus confirming that a ”smarter, not bigger” approach is a viable path forward.\nThe impli",
  "cations of this research are both practical and strategic.   It provides a tangible\nblueprint for developing high-quality, cost-effective, and secure question-answering system that\ncan operate on-premise on a single GPU workstation.  This work (Answer RQ4) democratizes\naccess to advanced AI capabilities, enabling organizations without massive computational in-\nfrastructure to leverage the power of language models.  Furthermore, it also contributes to the\ngrowing field of sustainable AI by demonstrating that performance and efficiency are not mu-\ntually exclusive.\n10    Future work\nThe framework developed in this thesis successfully demonstrates that an intelligently con-\ntrolled Small Language Model can achieve high performance in resource-constrained environ-\nments. This research also opens up several compelling points for future investigation that could\nfurther enhance the robustness, efficiency, and applicability of this approach.\nFirst a critical area for future research is the system’s resilience to irrelevant or misleading\ninformation  within  its  knowledge  base.   The  current  experiments  utilized  a  corpus  that  was\nsometimes relevant to the ARC dataset.  A future study could involve another dataset that has\nno relevant information to the real world,  this would split the realities of this dataset versus",
  "the ARC one, this way when retrieval occurred for ARC it wouldn’t improve the answer of the\nsystem. Though this is part of the advantage of a system like this, as it will always aim for the\nbest possible result.\nSecond, a novel and promising research direction based on the extracted results. A promis-\ning direction",
  "would be to explore the relationship between energy consumption and model hal-\nlucination. During this work, a correlation was observed between incorrect answers and higher\nenergy usage.  This suggests the possibility of identifying a computational signature for hal-\nlucination.  This could involve analyzing power draw and processing patterns to determine if\nnon-factual or fabricated responses can be detected in real-time based on their energy profile.\nIf a reliable correlation is established, this could lead to the development of a mechanism that\nflags potential hallucinations as they are being generated, allowing the system to intervene and\nreroute the query for correction, thereby improving the model’s trustworthiness.\nFinally, the adaptive routing principles pioneered here for SMLs could be extended to ad-\ndress known inefficiencies in much larger models.  State-of-the-art LLMs, despite their power,\ncan sometimes enter unproductive reasoning loops, repeatedly processing the same logic with-\nout reaching a conclusion, which wastes significant computational resources.  A future imple-\nmentation could adapt the controller to monitor the reasoning paths of an LLM. By detecting\nmajor semantic repetition or a lack of progress, the system could intervene to break the loop,\nperhaps by injecting new information via RAG or rewriting the prompt.  This would not only\nprevent wasted computation and energy while also improving the reliability of LLMs in com-\nplex, multi-step reasoning tasks positioning this framework as a valuable tool for optimizing\nboth small and large language models.",
  "Figure 55: Avg. Energy of Analysis Instructions that were also INCORRECT when Baseline Failed.",
  "Figure 56: Correctness Comparison between system versus a 14B Model.",
  "Figure 58: Average CPU Energy Consumption by Domain.",
  "Figure 59: Avg. Energy of Analysis Models that were CORRECT when Baseline Failed.",
  "Figure 60: Science: Avg. Energy when Straight Model is Incorrect & System is Correct.",
  "Figure 61: Average CPU Energy Consumption by Method.",
  "Figure 62: Efficiency Score: Energy Cost of a Correct Answer for the system versus the 14B Model at the science domain.",
  "Figure 63: Average GPU Energy Consumption by Method.",
  "Figure 65: Science Domain: Avg. Energy when Baseline is Incorrect & System is also Incorrect.",
  "Figure 67: Average Energy Consumption in Science Domain by Correctness.",
  "Figure 68: Average Energy for CORRECT Answers in Science Domain (with Counts).",
  "Figure 69: Average Energy for CORRECT Answers in Science Domain.",
  "Figure 70: Average Energy for INCORRECT Answers in Science Domain (with Counts).",
  "References\n[1]    Wenpeng Yin et al. Comparative Study of CNN and RNN for Natural Language Process-\ning. arXiv:1702.01923 [cs]. Feb. 2017. DOI: 10.48550/arXiv.1702.01923. URL:\nhttp://arxiv.org/abs/1702.01923 (visited on 01/09/2025).\n[2]    Jeffrey  L.  Elman.  Finding  Structure  in  Time.  1990.  DOI: https : / / doi . org /\n10 . 1207 / s15516709cog1402 \\ _1.  eprint: https : / / onlinelibrary .\nwiley.com/doi",
  "/pdf/10.1207/s15516709cog1402_1.  URL: https://\nonlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1.\n[3]    Neural Computation. “Long short-term memory”. In: Neural Comput 9 (2016), pp. 1735–\n1780.  URL: https : / / interactiveaudiolab . github . io / teaching /\ncasa/HorchreiterSchmidhuber_LSTM.pdf (visited on 01/09/2025).\n[4]    Kyunghyun Cho et al. On the Properties of Neural Machine Translation: Encoder-Decoder\nApproaches. arXiv:1409.1259 [cs]. Oct. 2014. DOI: 10.48550/arXiv.1409.1259.\nURL: http://arxiv.org/abs/1409.1259 (visited on 01/09/2025).\n[5]    Ashish  Vaswani  et  al.  “Attention  is  All  you  Need”.  In:  Advances  in  Neural  Informa-\ntion  Processing  Systems.  Vol.  30.  Curran  Associates,  Inc.,  2017.  URL: https : / /\nproceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html (visited on 01/08/2025).\n[6]    Sofia  Serrano  and Noah  A. Smith.  Is  Attention Interpretable?  arXiv:1906.03731 [cs].\nJune 2019.  DOI: 10.48550/arXiv.1906.03731.  URL: http://arxiv.org/\nabs/1906.03731 (visited on 01/09/2025).\n[7]    [2006.16362] Multi-Head Attention: Collaborate Instead of Concatenate. URL: https:\n//arxiv.org/abs/2006.16362 (visited on 01/10/2025).\n[8]    Katikapalli  Subramanyam  Kalyan,  Ajit  Rajasekharan,  and  Sivanesan  Sangeetha.  AM-\nMUS : A Survey of Transformer-based Pretrained Models in Natural Language Process-\ning.  arXiv:2108.05542  [cs].  Aug.  2021.  DOI: 10.48550/arXiv.2108.05542.\nURL: http://arxiv.org/abs/2108.05542 (visited on 01/11/2025).\n[9]    Tom B. Brown et al. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs].",
  "5.  URL: http://arxiv.org/\nabs/2005.14165 (visited on 01/11/2025).\n[10]    Wei Zeng et al. PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language\nModels with Auto-parallel Computation. arXiv:2104.12369 [cs]. Apr. 2021.  DOI: 10.\n48550/arXiv.2104.12369.  URL: http://arxiv.org/abs/2104.12369\n(visited on 01/11/2025).",
  "[11]    Dmitry Lepikhin  et al.  GShard:  Scaling Giant Models  with Conditional Computation\nand Automatic Sharding. arXiv:2006.16668 [cs]. June 2020. DOI: 10.48550/arXiv.\n2006 . 16668.  URL: http : / / arxiv . org / abs / 2006 . 16668  (visited  on\n01/11/2025).\n[12]    Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs]. May 2019.  DOI: 10.48550/arXiv.1810.\n04805. URL: http://arxiv.org/abs/1810.04805 (visited on 01/08/2025).\n[13]    Microsoft. microsoft/Phi-3-mini-4k-instruct at main. Sept. 2024. URL: https://huggingface.\nco / microsoft / Phi - 3 - mini - 4k - instruct / tree / main  (visited  on\n01/13/2025).\n[14]    Yanshu Wang et al. Art and Science of Quantizing Large-Scale Models: A Comprehen-\nsive Overview. arXiv:2409.11650 [cs]. Sept. 2024.  DOI: 10.48550/arXiv.2409.\n11650. URL: http://arxiv.org/abs/2409.11650 (visited on 01/08/2025).\n[15]    Zhewei Yao et al. ZeroQuant-V2: Exploring Post-training Quantization in LLMs from\nComprehensive Study to Low Rank Compensation. arXiv:2303.08302 [cs]. May 2023.\nDOI: 10.48550/arXiv.2303.08302.  URL: http://arxiv.org/abs/\n2303.08302 (visited on 01/08/2025).\n[16]    Wenxiao Wang et al. Model Compression and Efficient Inference for Large Language",
  "4. DOI: 10.48550/arXiv.2402.\n09748. URL: http://arxiv.org/abs/2402.09748 (visited on 01/08/2025).\n[17]    Gunho  Park  et  al.  LUT-GEMM:  Quantized  Matrix  Multiplication  based  on  LUTs  for\nEfficient Inference in Large-Scale Generative Language Models. arXiv:2206.09557 [cs].\nApr. 2024.  DOI: 10.48550/arXiv.2206.09557.  URL: http://arxiv.org/\nabs/2206.09557 (visited on 01/08/2025).\n[18]    Sehoon Kim et al. SqueezeLLM: Dense-and-Sparse Quantization. arXiv:2306.07629 [cs].\nJune 2024.  DOI: 10.48550/arXiv.2306.07629.  URL: http://arxiv.org/\nabs/2306.07629 (visited on 01/08/2025).\n[19]    Elias  Frantar  et  al.  GPTQ:  Accurate  Post-Training  Quantization  for  Generative  Pre-\ntrained Transformers. arXiv:2210.17323 [cs]. Mar. 2023.  DOI: 10.48550/arXiv.\n2210 . 17323.  URL: http : / / arxiv . org / abs / 2210 . 17323  (visited  on\n01/08/2025).\n[20]    Elias Frantar et al. “OPTQ: Accurate Quantization for Generative Pre-trained Transform-\ners”. en. In:  URL: https://openreview.net/forum?id=tcbBPnfwxS (vis-\nited on 01/24/2025).",
  "[21]    Elias Frantar and Dan Alistarh. “Optimal Brain Compression: A Framework for Accurate\nPost-Training Quantization and Pruning”. en. In: (). URL: https://proceedings.\nneurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-\nAbstract-Conference.html (visited on 01/08/2025).\n[22]    Hanlin Tang et al. EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs.\narXiv:2403.02775 [cs]. Mar. 2024.  DOI: 10.48550/arXiv.2403.02775.  URL:\nhttp://arxiv.org/abs/2403.02775 (visited on 01/08/2025).\n[23]    Zhewei Yao et al. “ZeroQuant: Efficient and Affordable Post-Training Quantization fo",
  "r\nLarge-Scale Transformers”. en. In: ().  URL: https://proceedings.neurips.\ncc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-\nAbstract-Conference.html (visited on 01/08/2025).\n[24]    Wei Huang et al. BiLLM: Pushing the Limit of Post-Training Quantization for LLMs.\narXiv:2402.04291 [cs]. May 2024.  DOI: 10.48550/arXiv.2402.04291.  URL:\nhttp://arxiv.org/abs/2402.04291 (visited on 01/08/2025).\n[25]    Chee-Yong Chan and Yannis E. Ioannidis. Bitmap index design and evaluation. Seattle,\nWashington, USA, 1998. DOI: 10.1145/276304.276336. URL: https://doi.\norg/10.1145/276304.276336.\n[26]    Zefan Li et al. “ICCV 2017 Open Access Repository”. In: URL: https://openaccess.\nthecvf.com/content_iccv_2017/html/Li_Performance_Guaranteed_\nNetwork_ICCV_2017_paper.html (visited on 01/08/2025).\n[27]    Wei Huang et al. An empirical study of LLaMA3 quantization: from LLMs to MLLMs.\nDec. 2024. DOI: 10.1007/s44267-024-00070-x. URL: https://doi.org/\n10.1007/s44267-024-00070-x.\n[28]    Saleh Ashkboos et al. QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. arXiv:2404.00456\n[cs]. Oct. 2024.  DOI: 10.48550/arXiv.2404.00456.  URL: http://arxiv.\norg/abs/2404.00456 (visited on 01/08/2025).\n[29]    Guangxuan Xiao et al. SmoothQuant: Accurate and Efficient Post-Training Quantization\nfor Large Language Models. Ed. by Andreas Krause et al. July 2023.  URL: https:\n//proceedings.mlr.press/v202/xiao23c.html.\n[30]    Tim Dettmers et al. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\narXiv:2208.07339 [cs]. Nov. 2022.  DOI: 10.48550/arXiv.2208.07339.  URL:\nhttp://arxiv.org/abs/2208.07339 (visited on 01/14/2025).\n[31]    Aohan Zeng e",
  "t al. GLM-130B: An Open Bilingual Pre-trained Model. arXiv:2210.02414\n[cs]. Oct. 2023.  DOI: 10.48550/arXiv.2210.02414.  URL: http://arxiv.\norg/abs/2210.02414 (visited on 01/08/2025).",
  "[32]    Susan Zhang et al. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068\n[cs]. June 2022.  DOI: 10.48550/arXiv.2205.01068.  URL: http://arxiv.\norg/abs/2205.01068 (visited on 01/08/2025).\n[33]    Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971\n[cs]. Feb. 2023.  DOI: 10.48550/arXiv.2302.13971.  URL: http://arxiv.\norg/abs/2302.13971 (visited on 01/08/2025).\n[34]    Fabio Petroni et al. Language Models as Knowledge Bases? arXiv:1909.01066 [cs]. Sept.\n2019. DOI: 10.48550/arXiv.1909.01066. URL: http://arxiv.org/abs/\n1909.01066 (visited on 01/08/2025).\n[35]    Patrick  Lewis  et  al.  “Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[36]    Vladimir Karpukhin et al. Dense Passage Retrieval for Open-Domain Question Answer-\ning.  arXiv:2004.04906  [cs].  Sept.  2020.  DOI: 10.48550/arXiv.2004.04906.\nURL: http://arxiv.org/abs/2004.04906 (visited on 01/08/2025).\n[37]    Mike Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Lan-\nguage Generation, Translation, and Comprehension. arXiv:1910.13461 [cs] version: 1.\nOct. 2019.  DOI: 10.48550/arXiv.1910.13461.  URL: http://arxiv.org/\nabs/1910.13461 (visited on 01/08/202",
  "5).\n[38]    Fabing Duan, Franc ̧ois Chapeau-Blondeau, and Derek Abbott. “Optimized injection of\nnoise in activation functions to improve generalization of neural networks”. In: Chaos,\nSolitons & Fractals 178  (Jan.  2024), p.  114363.  ISSN:  0960-0779.  DOI: 10.1016/\nj . chaos . 2023 . 114363.  URL: https : / / www . sciencedirect . com /\nscience/article/pii/S0960077923012651 (visited on 01/08/2025).\n[39]    Patrick  Lewis  et  al.  “Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[40]    Luyu Gao et al. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv:2212.10496\n[cs]. Dec. 2022.  DOI: 10.48550/arXiv.2212.10496.  URL: http://arxiv.\norg/abs/2212.10496 (visited on 01/08/2025).",
  "[41]    Luyu Gao and Jamie Callan. Unsupervised Corpus Aware Language Model Pre-training\nfor  Dense  Passage  Retrieval.  arXiv:2108.05540  [cs].  Aug.  2021.  DOI: 10.48550/\narXiv.2108.05540.  URL: http://arxiv.org/abs/2108.05540 (visited\non 01/08/2025).\n[42]    Long Ouyang et al. “Training language models to follow instructions with human feed-\nback”. en. In: (). URL: https://proceedings.neurips.cc/paper_files/\npaper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-\nConference.html (visited on 01/08/2025).\n[43]    Brian J. Chan et al. Don’t Do RAG: When Cache-Augmented Generation is All You Need\nfor Knowledge Tasks. arXiv:2412.15605 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.\n2412 .",
  "15605.  URL: http : / / arxiv . org / abs / 2412 . 15605  (visited  on\n01/15/2025).\n[44]    Chao Jin et al. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Gen-\neration. arXiv:2404.12457 [cs]. Apr. 2024.  DOI: 10.48550/arXiv.2404.12457.\nURL: http://arxiv.org/abs/2404.12457 (visited on 01/15/2025).\n[45]    Zhuowan Li et al. “Retrieval Augmented Generation or Long-Context LLMs? A Com-\nprehensive Study and Hybrid Approach”. In: Proceedings of the 2024 Conference on Em-\npirical Methods in Natural Language Processing: Industry Track. Ed. by Franck Dernon-\ncourt, Daniel Preot ̧iuc-Pietro, and Anastasia Shimorina. Miami, Florida, US: Association\nfor Computational Linguistics, Nov. 2024, pp. 881–893. DOI: 10.18653/v1/2024.\nemnlp-industry.66.  URL: https://aclanthology.org/2024.emnlp-\nindustry.66/ (visited on 01/15/2025).\n[46]    Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\narXiv:2403.05530 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.2403.05530.  URL:\nhttp://arxiv.org/abs/2403.05530.\n[47]    Woosuk Kwon et al. “Efficient Memory Management for Large Language Model Serving\nwith  PagedAttention”.  In:  Proceedings  of  the  29th  Symposium  on  Operating  Systems\nPrinciples. SOSP ’23. New York, NY, USA: Association for Computing Machinery, Oct.\n2023, pp. 611–626. ISBN: 979-8-4007-0229-7. DOI: 10.1145/3600006.3613165.\nURL: https://dl.acm.org/doi/10.1145/3600006.3613165 (visited on\n01/16/2025).\n[48]    Conglong Li et al. “Improving Approximate Nearest Neighbor Search through Learned\nAdaptive Early Termination”. In: Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data. SIG",
  "MOD ’20. New York, NY, USA: Association\nfor Computing Machinery, May 2020, pp. 2539–2554.  ISBN: 978-1-4503-6735-6.  DOI:\n10.1145/3318464.3380600. URL: https://dl.acm.org/doi/10.1145/\n3318464.3380600 (visited on 01/19/2025).",
  "[49]    Gautier Izacard et al. Unsupervised Dense Information Retrieval with Contrastive Learn-\ning. arXiv:2112.09118 [cs]. Aug. 2022. DOI: 10.48550/arXiv.2112.09118. URL:\nhttp://arxiv.org/abs/2112.09118 (visited on 01/20/2025).\n[50]    Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent Retrieval for Weakly Su-\npervised  Open  Domain  Question  Answering.  arXiv:1906.00300  [cs].  June  2019.  DOI:\n10.48550/arXiv.1906.00300.  URL: http://arxiv.org/abs/1906.\n00300 (visited on 01/20/2025).\n[51]    Kaiming He et al. Momentum Contrast for Unsupervised Visual Representation Learn-\ning. arXiv:1911.05722 [cs]. Mar. 2020. DOI: 10.48550/arXiv.1911.05722. URL:\nhttp://arxiv.org/abs/1911.05722 (visited on 01/21/2025).\n[52]    Yile Wang et al. “Self-Knowledge Guided Retrieval Augmentation for Large Language\nModels”. In: Findings of the Association for Computational Linguistics: EMNLP 2023.\nEd. by Houda Bouamor, Juan Pino, and Kalika Bali. Singapore: Association for Com-\nputational Linguistics, Dec. 2023, pp. 10303–10315.  DOI: 10.18653/v1/2023.\nfindings-emnlp.691. URL: https://aclanthology.org/2023.findings-\nemnlp.691/ (visited on 05/05/2025).\n[53]    Evelyn Fix and J. L. Hodges. “Discriminatory Analysis. Nonparametric Discrimination:\nConsistency Properties”. In: International Statistical Review / Revue Internationale de\nStatistique 57.3 (1989), pp. 238–247. ISSN: 03067734, 17515823. URL: http://www.\njstor.org/st",
  "able/1403797 (visited on 06/17/2025).\n[54]    LLM Explorer. LLM Explorer: A Curated Large Language Model Directory. LLM List.\n41870 Open-Source Language Models. en.  URL: https://llm.extractum.io\n(visited on 01/24/2025).\n[55]    Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. arXiv:2009.03300\n[cs]. Jan. 2021.  DOI: 10.48550/arXiv.2009.03300.  URL: http://arxiv.\norg/abs/2009.03300 (visited on 01/08/2025).\n[56]    Yubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Un-\nderstanding Benchmark. arXiv:2406.01574 [cs]. Nov. 2024. DOI: 10.48550/arXiv.\n2406 . 01574.  URL: http : / / arxiv . org / abs / 2406 . 01574  (visited  on\n01/08/2025).\n[57]    David Rein et al. GPQA: A Graduate-Level Google-Proof Q&A Benchmark. arXiv:2311.12022\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.12022.  URL: http://arxiv.\norg/abs/2311.12022 (visited on 01/08/2025).\n[58]    Zayne Sprague et al. MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\nReasoning.  arXiv:2310.16049  [cs].  Mar.  2024.  DOI: 10 . 48550 / arXiv . 2310 .\n16049. URL: http://arxiv.org/abs/2310.16049 (visited on 01/08/2025).",
  "[59]    Jeffrey Zhou et al. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.07911.  URL: http://arxiv.\norg/abs/2311.07911 (visited on 01/08/2025).\n[60]    Peter Clark et al. Think you have Solved Question Answering? Try ARC, the AI2 Reason-\ning Challenge. arXiv:1803.05457 [cs]. Mar. 2018.  DOI: 10.48550/arXiv.1803.\n05457. URL: http://arxiv.org/abs/1803.05457 (visited on 01/24/2025).\n[61]    Rowan Zellers et al. HellaSwag: Can a Machine",
  "[cs]. May 2019.  DOI: 10.48550/arXiv.1905.07830.  URL: http://arxiv.\norg/abs/1905.07830 (visited on 01/08/2025).\n[62]    Stephanie  Lin,  Jacob  Hilton,  and  Owain  Evans.  TruthfulQA:  Measuring  How  Models\nMimic Human Falsehoods. arXiv:2109.07958 [cs]. May 2022. DOI: 10.48550/arXiv.\n2109 . 07958.  URL: http : / / arxiv . org / abs / 2109 . 07958  (visited  on\n01/08/2025).\n[63]    Karl Cobbe et al. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168\n[cs]. Nov. 2021.  DOI: 10.48550/arXiv.2110.14168.  URL: http://arxiv.\norg/abs/2110.14168 (visited on 01/08/2025).\n[64]    Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset.\narXiv:2103.03874 [cs]. Nov. 2021.  DOI: 10.48550/arXiv.2103.03874.  URL:\nhttp://arxiv.org/abs/2103.03874 (visited on 01/08/2025).\n[65]    Kunlun  Zhu  et  al.  RAGEval:  Scenario  Specific  RAG  Evaluation  Dataset  Generation\nFramework.  arXiv:2408.01262  [cs].  Mar.  2025.  DOI: 10.48550/arXiv.2408.\n01262. URL: http://arxiv.org/abs/2408.01262 (visited on 03/28/2025).\n[66]    Zhilin Yang et al. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question An-\nswering. arXiv:1809.09600 [cs]. Sept. 2018. DOI: 10.48550/arXiv.1809.09600.\nURL: http://arxiv.org/abs/1809.09600 (visited on 07/29/2025).\n[67]    Rolf Jagerman et al. Query Expansion by Prompting Large Language Models. arXiv:2305.03653\n[cs]. May 2023.  DOI: 10.48550/arXiv.2305.03653.  URL: http://arxiv.\norg/abs/2305.03653 (visited on 01/22/2025).\n[68]    Lihu Chen and Ga\n ̈\nel Varoquaux. What is the Role of Small Models in the LLM Era: A\nSurvey. arXiv:2409.06857 [cs].",
  "Dec. 2024.  DOI: 10.48550/arXiv.2409.06857.\nURL: http://arxiv.org/abs/2409.06857 (visited on 01/08/2025).\n[69]    DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning. en. arXiv:2501.12948 [cs]. Jan. 2025.  DOI: 10.48550/arXiv.\n2501 . 12948.  URL: http : / / arxiv . org / abs / 2501 . 12948  (visited  on\n06/20/2025).",
  "[70]    Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can\nSolve  Them.  arXiv:2210.09261  [cs].  Oct.  2022.  DOI: 10.48550/arXiv.2210.\n09261. URL: http://arxiv.org/abs/2210.09261 (visited on 01/08/2025).\n[71]    Xinyi Wu et al. On the Emergence of Position Bias in Transformers. arXiv:2502.01951\n[cs]. June 2025.  DOI: 10.48550/arXiv.2502.01951.  URL: http://arxiv.\norg/abs/2502.01951 (visited on 06/20/2025).\n[72]    docs.nvidia.com/deploy/nvidia-smi/index.html. URL: https://docs.nvidia.com/\ndeploy/nvidia-smi/index.html (visited on 07/02/2025).\n[73]    NVML Device Queries. en-us. cppModule.  URL: https://docs.nvidia.com/\ndeploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_\n1g7ef7dff0ff14238d08a19ad7fb23fc87 (visited on 07/02/2025).\n[74]    Alireza Salemi and Hamed Zamani. “Evaluating Retrieval Quality in Retrieval-Augmented\nGeneration”. In: Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearch  and  Development  in  Information  Retrieval.  SIGIR  ’24.  New  York,  NY,  USA:\nAssociation for Computing Machinery, July 2024, pp. 2395–2400.  ISBN: 979-8-4007-\n0431-4.  DOI: 10.1145/3626772.3657957.  URL: https://dl.acm.org/\ndoi/10.1145/3626772.3657957 (visited on 01/23/2025)."
]
  },,
      {
        type: 'image',
        src: '/images/methodology/page_120_img_1.png',
        alt: 'Figure 55: Avg. Energy of Analysis Instructions that were also INCORRECT when Baseline Failed',
        caption: 'Figure 55: Avg. Energy of Analysis Instructions that were also INCORRECT when Baseline Failed'
      },
      {
        type: 'image',
        src: '/images/methodology/page_121_img_1.png',
        alt: 'Figure 56: Correctness Comparison between system versus a 14B Model',
        caption: 'Figure 56: Correctness Comparison between system versus a 14B Model'
      },
      {
        type: 'image',
        src: '/images/methodology/page_122_img_1.png',
        alt: 'Figure 57: Average Energy Consumption by Domain',
        caption: 'Figure 57: Average Energy Consumption by Domain'
      },
      {
        type: 'image',
        src: '/images/methodology/page_123_img_1.png',
        alt: 'Figure 58: Average CPU Energy Consumption by Domain',
        caption: 'Figure 58: Average CPU Energy Consumption by Domain'
      },
      {
        type: 'image',
        src: '/images/methodology/page_124_img_1.png',
        alt: 'Figure 59: Avg. Energy of Analysis Models that were CORRECT when Baseline Failed',
        caption: 'Figure 59: Avg. Energy of Analysis Models that were CORRECT when Baseline Failed'
      },
      {
        type: 'image',
        src: '/images/methodology/page_125_img_1.png',
        alt: 'Figure 60: Science: Avg. Energy when Straight Model is Incorrect & System is Correct',
        caption: 'Figure 60: Science: Avg. Energy when Straight Model is Incorrect & System is Correct'
      },
      {
        type: 'image',
        src: '/images/methodology/page_126_img_1.png',
        alt: 'Figure 61: Average CPU Energy Consumption by Method',
        caption: 'Figure 61: Average CPU Energy Consumption by Method'
      },
      {
        type: 'image',
        src: '/images/methodology/page_127_img_1.png',
        alt: 'Figure 62: Efficiency Score: Energy Cost of a Correct Answer for the system versus the 14B Model at the science domain',
        caption: 'Figure 62: Efficiency Score: Energy Cost of a Correct Answer for the system versus the 14B Model at the science domain'
      },
      {
        type: 'image',
        src: '/images/methodology/page_128_img_1.png',
        alt: 'Figure 63: Average GPU Energy Consumption by Method',
        caption: 'Figure 63: Average GPU Energy Consumption by Method'
      },
      {
        type: 'image',
        src: '/images/methodology/page_129_img_1.png',
        alt: 'Figure 64: Average Energy Consumption by Method',
        caption: 'Figure 64: Average Energy Consumption by Method'
      },
      {
        type: 'image',
        src: '/images/methodology/page_130_img_1.png',
        alt: 'Figure 65: Science Domain: Avg. Energy when Baseline is Incorrect & System is also Incorrect',
        caption: 'Figure 65: Science Domain: Avg. Energy when Baseline is Incorrect & System is also Incorrect'
      },
      {
        type: 'image',
        src: '/images/methodology/page_131_img_1.png',
        alt: 'Figure 66: Total Energy Consumption by Answer',
        caption: 'Figure 66: Total Energy Consumption by Answer'
      },
      {
        type: 'image',
        src: '/images/methodology/page_132_img_1.png',
        alt: 'Figure 67: Average Energy Consumption in Science Domain by Correctness',
        caption: 'Figure 67: Average Energy Consumption in Science Domain by Correctness'
      },
      {
        type: 'image',
        src: '/images/methodology/page_133_img_1.png',
        alt: 'Figure 68: Average Energy for CORRECT Answers in Science Domain (with Counts)',
        caption: 'Figure 68: Average Energy for CORRECT Answers in Science Domain (with Counts)'
      },
      {
        type: 'image',
        src: '/images/methodology/page_134_img_1.png',
        alt: 'Figure 69: Average Energy for CORRECT Answers in Science Domain',
        caption: 'Figure 69: Average Energy for CORRECT Answers in Science Domain'
      },
      {
        type: 'image',
        src: '/images/methodology/page_135_img_1.png',
        alt: 'Figure 70: Average Energy for INCORRECT Answers in Science Domain (with Counts)',
        caption: 'Figure 70: Average Energy for INCORRECT Answers in Science Domain (with Counts)'
      },
      "References\n[1]    Wenpeng Yin et al. Comparative Study of CNN and RNN for Natural Language Process-\ning. arXiv:1702.01923 [cs]. Feb. 2017. DOI: 10.48550/arXiv.1702.01923. URL:\nhttp://arxiv.org/abs/1702.01923 (visited on 01/09/2025).\n[2]    Jeffrey  L.  Elman.  Finding  Structure  in  Time.  1990.  DOI: https : / / doi . org /\n10 . 1207 / s15516709cog1402 \\ _1.  eprint: https : / / onlinelibrary .\nwiley.com/doi",
      "/pdf/10.1207/s15516709cog1402_1.  URL: https://\nonlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1.\n[3]    Neural Computation. “Long short-term memory”. In: Neural Comput 9 (2016), pp. 1735–\n1780.  URL: https : / / interactiveaudiolab . github . io / teaching /\ncasa/HorchreiterSchmidhuber_LSTM.pdf (visited on 01/09/2025).\n[4]    Kyunghyun Cho et al. On the Properties of Neural Machine Translation: Encoder-Decoder\nApproaches. arXiv:1409.1259 [cs]. Oct. 2014. DOI: 10.48550/arXiv.1409.1259.",
      "URL: http://arxiv.org/abs/1409.1259 (visited on 01/09/2025).\n[5]    Ashish  Vaswani  et  al.  “Attention  is  All  you  Need”.  In:  Advances  in  Neural  Informa-\ntion  Processing  Systems.  Vol.  30.  Curran  Associates,  Inc.,  2017.  URL: https : / /\nproceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html (visited on 01/08/2025).\n[6]    Sofia  Serrano  and Noah  A. Smith.  Is  Attention Interpretable?  arXiv:1906.03731 [cs].\nJune 2019.  DOI: 10.48550/arXiv.",
      "1906.03731.  URL: http://arxiv.org/\nabs/1906.03731 (visited on 01/09/2025).\n[7]    [2006.16362] Multi-Head Attention: Collaborate Instead of Concatenate. URL: https:\n//arxiv.org/abs/2006.16362 (visited on 01/10/2025).\n[8]    Katikapalli  Subramanyam  Kalyan,  Ajit  Rajasekharan,  and  Sivanesan  Sangeetha.  AM-\nMUS : A Survey of Transformer-based Pretrained Models in Natural Language Process-\ning.  arXiv:2108.05542  [cs].  Aug.  2021.  DOI: 10.48550/arXiv.2108.05542.\nURL: http://arxiv.",
      "org/abs/2108.05542 (visited on 01/11/2025).\n[9]    Tom B. Brown et al. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs].\nJuly 2020.  DOI: 10.48550/arXiv.2005.",
      "5.  URL: http://arxiv.org/\nabs/2005.14165 (visited on 01/11/2025).\n[10]    Wei Zeng et al. PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language\nModels with Auto-parallel Computation. arXiv:2104.12369 [cs]. Apr. 2021.  DOI: 10.\n48550/arXiv.2104.12369.  URL: http://arxiv.org/abs/2104.12369\n(visited on 01/11/2025).\n128",
      "[11]    Dmitry Lepikhin  et al.  GShard:  Scaling Giant Models  with Conditional Computation\nand Automatic Sharding. arXiv:2006.16668 [cs]. June 2020. DOI: 10.48550/arXiv.\n2006 . 16668.  URL: http : / / arxiv . org / abs / 2006 . 16668  (visited  on\n01/11/2025).\n[12]    Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs]. May 2019.  DOI: 10.48550/arXiv.1810.\n04805. URL: http://arxiv.org/abs/1810.",
      "04805 (visited on 01/08/2025).\n[13]    Microsoft. microsoft/Phi-3-mini-4k-instruct at main. Sept. 2024. URL: https://huggingface.\nco / microsoft / Phi - 3 - mini - 4k - instruct / tree / main  (visited  on\n01/13/2025).\n[14]    Yanshu Wang et al. Art and Science of Quantizing Large-Scale Models: A Comprehen-\nsive Overview. arXiv:2409.11650 [cs]. Sept. 2024.  DOI: 10.48550/arXiv.2409.\n11650. URL: http://arxiv.org/abs/2409.11650 (visited on 01/08/2025).\n[15]    Zhewei Yao et al.",
      "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from\nComprehensive Study to Low Rank Compensation. arXiv:2303.08302 [cs]. May 2023.\nDOI: 10.48550/arXiv.2303.08302.  URL: http://arxiv.org/abs/\n2303.08302 (visited on 01/08/2025).\n[16]    Wenxiao Wang et al. Model Compression and Efficient Inference for Large Language\nModels: A Survey. arXiv:2402.09748 [cs]. Feb.",
      "4. DOI: 10.48550/arXiv.2402.\n09748. URL: http://arxiv.org/abs/2402.09748 (visited on 01/08/2025).\n[17]    Gunho  Park  et  al.  LUT-GEMM:  Quantized  Matrix  Multiplication  based  on  LUTs  for\nEfficient Inference in Large-Scale Generative Language Models. arXiv:2206.09557 [cs].\nApr. 2024.  DOI: 10.48550/arXiv.2206.09557.  URL: http://arxiv.org/\nabs/2206.09557 (visited on 01/08/2025).\n[18]    Sehoon Kim et al. SqueezeLLM: Dense-and-Sparse Quantization. arXiv:2306.07629 [cs].\nJune 2024.",
      "DOI: 10.48550/arXiv.2306.07629.  URL: http://arxiv.org/\nabs/2306.07629 (visited on 01/08/2025).\n[19]    Elias  Frantar  et  al.  GPTQ:  Accurate  Post-Training  Quantization  for  Generative  Pre-\ntrained Transformers. arXiv:2210.17323 [cs]. Mar. 2023.  DOI: 10.48550/arXiv.\n2210 . 17323.  URL: http : / / arxiv . org / abs / 2210 . 17323  (visited  on\n01/08/2025).\n[20]    Elias Frantar et al. “OPTQ: Accurate Quantization for Generative Pre-trained Transform-\ners”. en.",
      "In:  URL: https://openreview.net/forum?id=tcbBPnfwxS (vis-\nited on 01/24/2025).",
      "[21]    Elias Frantar and Dan Alistarh. “Optimal Brain Compression: A Framework for Accurate\nPost-Training Quantization and Pruning”. en. In: (). URL: https://proceedings.\nneurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-\nAbstract-Conference.html (visited on 01/08/2025).\n[22]    Hanlin Tang et al. EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs.\narXiv:2403.02775 [cs]. Mar. 2024.  DOI: 10.48550/arXiv.2403.02775.  URL:\nhttp://arxiv.org/abs/2403.",
      "02775 (visited on 01/08/2025).\n[23]    Zhewei Yao et al.",
      "r\nLarge-Scale Transformers”. en. In: ().  URL: https://proceedings.neurips.\ncc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-\nAbstract-Conference.html (visited on 01/08/2025).\n[24]    Wei Huang et al. BiLLM: Pushing the Limit of Post-Training Quantization for LLMs.\narXiv:2402.04291 [cs]. May 2024.  DOI: 10.48550/arXiv.2402.04291.  URL:\nhttp://arxiv.org/abs/2402.04291 (visited on 01/08/2025).\n[25]    Chee-Yong Chan and Yannis E. Ioannidis. Bitmap index design and evaluation.",
      "Seattle,\nWashington, USA, 1998. DOI: 10.1145/276304.276336. URL: https://doi.\norg/10.1145/276304.276336.\n[26]    Zefan Li et al. “ICCV 2017 Open Access Repository”. In: URL: https://openaccess.\nthecvf.com/content_iccv_2017/html/Li_Performance_Guaranteed_\nNetwork_ICCV_2017_paper.html (visited on 01/08/2025).\n[27]    Wei Huang et al. An empirical study of LLaMA3 quantization: from LLMs to MLLMs.\nDec. 2024. DOI: 10.1007/s44267-024-00070-x. URL: https://doi.org/\n10.1007/s44267-024-00070-x.",
      "[28]    Saleh Ashkboos et al. QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. arXiv:2404.00456\n[cs]. Oct. 2024.  DOI: 10.48550/arXiv.2404.00456.  URL: http://arxiv.\norg/abs/2404.00456 (visited on 01/08/2025).\n[29]    Guangxuan Xiao et al. SmoothQuant: Accurate and Efficient Post-Training Quantization\nfor Large Language Models. Ed. by Andreas Krause et al. July 2023.  URL: https:\n//proceedings.mlr.press/v202/xiao23c.html.\n[30]    Tim Dettmers et al. LLM.",
      "int8(): 8-bit Matrix Multiplication for Transformers at Scale.\narXiv:2208.07339 [cs]. Nov. 2022.  DOI: 10.48550/arXiv.2208.07339.  URL:\nhttp://arxiv.org/abs/2208.07339 (visited on 01/14/2025).",
      "t al. GLM-130B: An Open Bilingual Pre-trained Model. arXiv:2210.02414\n[cs]. Oct. 2023.  DOI: 10.48550/arXiv.2210.02414.  URL: http://arxiv.\norg/abs/2210.02414 (visited on 01/08/2025).\n130",
      "[32]    Susan Zhang et al. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068\n[cs]. June 2022.  DOI: 10.48550/arXiv.2205.01068.  URL: http://arxiv.\norg/abs/2205.01068 (visited on 01/08/2025).\n[33]    Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971\n[cs]. Feb. 2023.  DOI: 10.48550/arXiv.2302.13971.  URL: http://arxiv.\norg/abs/2302.13971 (visited on 01/08/2025).\n[34]    Fabio Petroni et al. Language Models as Knowledge Bases? arXiv:1909.",
      "01066 [cs]. Sept.\n2019. DOI: 10.48550/arXiv.1909.01066. URL: http://arxiv.org/abs/\n1909.01066 (visited on 01/08/2025).\n[35]    Patrick  Lewis  et  al.  “Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[36]    Vladimir Karpukhin et al.",
      "Dense Passage Retrieval for Open-Domain Question Answer-\ning.  arXiv:2004.04906  [cs].  Sept.  2020.  DOI: 10.48550/arXiv.2004.04906.\nURL: http://arxiv.org/abs/2004.04906 (visited on 01/08/2025).\n[37]    Mike Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Lan-\nguage Generation, Translation, and Comprehension. arXiv:1910.13461 [cs] version: 1.\nOct. 2019.  DOI: 10.48550/arXiv.1910.13461.  URL: http://arxiv.org/\nabs/1910.",
      "5).\n[38]    Fabing Duan, Franc ̧ois Chapeau-Blondeau, and Derek Abbott. “Optimized injection of\nnoise in activation functions to improve generalization of neural networks”. In: Chaos,\nSolitons & Fractals 178  (Jan.  2024), p.  114363.  ISSN:  0960-0779.  DOI: 10.1016/\nj . chaos . 2023 . 114363.  URL: https : / / www . sciencedirect . com /\nscience/article/pii/S0960077923012651 (visited on 01/08/2025).\n[39]    Patrick  Lewis  et  al.",
      "“Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[40]    Luyu Gao et al. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv:2212.10496\n[cs]. Dec. 2022.  DOI: 10.48550/arXiv.2212.10496.  URL: http://arxiv.\norg/abs/2212.",
      "10496 (visited on 01/08/2025).",
      "[41]    Luyu Gao and Jamie Callan. Unsupervised Corpus Aware Language Model Pre-training\nfor  Dense  Passage  Retrieval.  arXiv:2108.05540  [cs].  Aug.  2021.  DOI: 10.48550/\narXiv.2108.05540.  URL: http://arxiv.org/abs/2108.05540 (visited\non 01/08/2025).\n[42]    Long Ouyang et al. “Training language models to follow instructions with human feed-\nback”. en. In: (). URL: https://proceedings.neurips.cc/paper_files/\npaper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-\nConference.",
      "html (visited on 01/08/2025).\n[43]    Brian J. Chan et al. Don’t Do RAG: When Cache-Augmented Generation is All You Need\nfor Knowledge Tasks. arXiv:2412.15605 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.\n2412 .",
      "15605.  URL: http : / / arxiv . org / abs / 2412 . 15605  (visited  on\n01/15/2025).\n[44]    Chao Jin et al. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Gen-\neration. arXiv:2404.12457 [cs]. Apr. 2024.  DOI: 10.48550/arXiv.2404.12457.\nURL: http://arxiv.org/abs/2404.12457 (visited on 01/15/2025).\n[45]    Zhuowan Li et al. “Retrieval Augmented Generation or Long-Context LLMs? A Com-\nprehensive Study and Hybrid Approach”.",
      "In: Proceedings of the 2024 Conference on Em-\npirical Methods in Natural Language Processing: Industry Track. Ed. by Franck Dernon-\ncourt, Daniel Preot ̧iuc-Pietro, and Anastasia Shimorina. Miami, Florida, US: Association\nfor Computational Linguistics, Nov. 2024, pp. 881–893. DOI: 10.18653/v1/2024.\nemnlp-industry.66.  URL: https://aclanthology.org/2024.emnlp-\nindustry.66/ (visited on 01/15/2025).\n[46]    Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
      "arXiv:2403.05530 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.2403.05530.  URL:\nhttp://arxiv.org/abs/2403.05530.\n[47]    Woosuk Kwon et al. “Efficient Memory Management for Large Language Model Serving\nwith  PagedAttention”.  In:  Proceedings  of  the  29th  Symposium  on  Operating  Systems\nPrinciples. SOSP ’23. New York, NY, USA: Association for Computing Machinery, Oct.\n2023, pp. 611–626. ISBN: 979-8-4007-0229-7. DOI: 10.1145/3600006.3613165.\nURL: https://dl.acm.org/doi/10.1145/3600006.",
      "3613165 (visited on\n01/16/2025).\n[48]    Conglong Li et al. “Improving Approximate Nearest Neighbor Search through Learned\nAdaptive Early Termination”. In: Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data.",
      "MOD ’20. New York, NY, USA: Association\nfor Computing Machinery, May 2020, pp. 2539–2554.  ISBN: 978-1-4503-6735-6.  DOI:\n10.1145/3318464.3380600. URL: https://dl.acm.org/doi/10.1145/\n3318464.3380600 (visited on 01/19/2025).\n132",
      "[49]    Gautier Izacard et al. Unsupervised Dense Information Retrieval with Contrastive Learn-\ning. arXiv:2112.09118 [cs]. Aug. 2022. DOI: 10.48550/arXiv.2112.09118. URL:\nhttp://arxiv.org/abs/2112.09118 (visited on 01/20/2025).\n[50]    Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent Retrieval for Weakly Su-\npervised  Open  Domain  Question  Answering.  arXiv:1906.00300  [cs].  June  2019.  DOI:\n10.48550/arXiv.1906.00300.  URL: http://arxiv.org/abs/1906.\n00300 (visited on 01/20/2025).",
      "[51]    Kaiming He et al. Momentum Contrast for Unsupervised Visual Representation Learn-\ning. arXiv:1911.05722 [cs]. Mar. 2020. DOI: 10.48550/arXiv.1911.05722. URL:\nhttp://arxiv.org/abs/1911.05722 (visited on 01/21/2025).\n[52]    Yile Wang et al. “Self-Knowledge Guided Retrieval Augmentation for Large Language\nModels”. In: Findings of the Association for Computational Linguistics: EMNLP 2023.\nEd. by Houda Bouamor, Juan Pino, and Kalika Bali.",
      "Singapore: Association for Com-\nputational Linguistics, Dec. 2023, pp. 10303–10315.  DOI: 10.18653/v1/2023.\nfindings-emnlp.691. URL: https://aclanthology.org/2023.findings-\nemnlp.691/ (visited on 05/05/2025).\n[53]    Evelyn Fix and J. L. Hodges. “Discriminatory Analysis. Nonparametric Discrimination:\nConsistency Properties”. In: International Statistical Review / Revue Internationale de\nStatistique 57.3 (1989), pp. 238–247. ISSN: 03067734, 17515823. URL: http://www.\njstor.",
      "able/1403797 (visited on 06/17/2025).\n[54]    LLM Explorer. LLM Explorer: A Curated Large Language Model Directory. LLM List.\n41870 Open-Source Language Models. en.  URL: https://llm.extractum.io\n(visited on 01/24/2025).\n[55]    Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. arXiv:2009.03300\n[cs]. Jan. 2021.  DOI: 10.48550/arXiv.2009.03300.  URL: http://arxiv.\norg/abs/2009.03300 (visited on 01/08/2025).\n[56]    Yubo Wang et al.",
      "MMLU-Pro: A More Robust and Challenging Multi-Task Language Un-\nderstanding Benchmark. arXiv:2406.01574 [cs]. Nov. 2024. DOI: 10.48550/arXiv.\n2406 . 01574.  URL: http : / / arxiv . org / abs / 2406 . 01574  (visited  on\n01/08/2025).\n[57]    David Rein et al. GPQA: A Graduate-Level Google-Proof Q&A Benchmark. arXiv:2311.12022\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.12022.  URL: http://arxiv.\norg/abs/2311.12022 (visited on 01/08/2025).\n[58]    Zayne Sprague et al.",
      "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\nReasoning.  arXiv:2310.16049  [cs].  Mar.  2024.  DOI: 10 . 48550 / arXiv . 2310 .\n16049. URL: http://arxiv.org/abs/2310.16049 (visited on 01/08/2025).",
      "[59]    Jeffrey Zhou et al. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.07911.  URL: http://arxiv.\norg/abs/2311.07911 (visited on 01/08/2025).\n[60]    Peter Clark et al. Think you have Solved Question Answering? Try ARC, the AI2 Reason-\ning Challenge. arXiv:1803.05457 [cs]. Mar. 2018.  DOI: 10.48550/arXiv.1803.\n05457. URL: http://arxiv.org/abs/1803.05457 (visited on 01/24/2025).\n[61]    Rowan Zellers et al.",
      "Really Finish Your Sentence? arXiv:1905.07830\n[cs]. May 2019.  DOI: 10.48550/arXiv.1905.07830.  URL: http://arxiv.\norg/abs/1905.07830 (visited on 01/08/2025).\n[62]    Stephanie  Lin,  Jacob  Hilton,  and  Owain  Evans.  TruthfulQA:  Measuring  How  Models\nMimic Human Falsehoods. arXiv:2109.07958 [cs]. May 2022. DOI: 10.48550/arXiv.\n2109 . 07958.  URL: http : / / arxiv . org / abs / 2109 . 07958  (visited  on\n01/08/2025).\n[63]    Karl Cobbe et al. Training Verifiers to Solve Math Word Problems.",
      "arXiv:2110.14168\n[cs]. Nov. 2021.  DOI: 10.48550/arXiv.2110.14168.  URL: http://arxiv.\norg/abs/2110.14168 (visited on 01/08/2025).\n[64]    Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset.\narXiv:2103.03874 [cs]. Nov. 2021.  DOI: 10.48550/arXiv.2103.03874.  URL:\nhttp://arxiv.org/abs/2103.03874 (visited on 01/08/2025).\n[65]    Kunlun  Zhu  et  al.  RAGEval:  Scenario  Specific  RAG  Evaluation  Dataset  Generation\nFramework.  arXiv:2408.01262  [cs].  Mar.  2025.",
      "DOI: 10.48550/arXiv.2408.\n01262. URL: http://arxiv.org/abs/2408.01262 (visited on 03/28/2025).\n[66]    Zhilin Yang et al. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question An-\nswering. arXiv:1809.09600 [cs]. Sept. 2018. DOI: 10.48550/arXiv.1809.09600.\nURL: http://arxiv.org/abs/1809.09600 (visited on 07/29/2025).\n[67]    Rolf Jagerman et al. Query Expansion by Prompting Large Language Models. arXiv:2305.03653\n[cs]. May 2023.  DOI: 10.48550/arXiv.2305.03653.  URL: http://arxiv.",
      "org/abs/2305.03653 (visited on 01/22/2025).\n[68]    Lihu Chen and Ga\n ̈\nel Varoquaux. What is the Role of Small Models in the LLM Era: A\nSurvey. arXiv:2409.06857 [cs].",
      "Dec. 2024.  DOI: 10.48550/arXiv.2409.06857.\nURL: http://arxiv.org/abs/2409.06857 (visited on 01/08/2025).\n[69]    DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning. en. arXiv:2501.12948 [cs]. Jan. 2025.  DOI: 10.48550/arXiv.\n2501 . 12948.  URL: http : / / arxiv . org / abs / 2501 . 12948  (visited  on\n06/20/2025).\n134",
      "[70]    Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can\nSolve  Them.  arXiv:2210.09261  [cs].  Oct.  2022.  DOI: 10.48550/arXiv.2210.\n09261. URL: http://arxiv.org/abs/2210.09261 (visited on 01/08/2025).\n[71]    Xinyi Wu et al. On the Emergence of Position Bias in Transformers. arXiv:2502.01951\n[cs]. June 2025.  DOI: 10.48550/arXiv.2502.01951.  URL: http://arxiv.\norg/abs/2502.01951 (visited on 06/20/2025).\n[72]    docs.nvidia.com/deploy/nvidia-smi/index.html.",
      "URL: https://docs.nvidia.com/\ndeploy/nvidia-smi/index.html (visited on 07/02/2025).\n[73]    NVML Device Queries. en-us. cppModule.  URL: https://docs.nvidia.com/\ndeploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_\n1g7ef7dff0ff14238d08a19ad7fb23fc87 (visited on 07/02/2025).\n[74]    Alireza Salemi and Hamed Zamani. “Evaluating Retrieval Quality in Retrieval-Augmented\nGeneration”.",
      "In: Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearch  and  Development  in  Information  Retrieval.  SIGIR  ’24.  New  York,  NY,  USA:\nAssociation for Computing Machinery, July 2024, pp. 2395–2400.  ISBN: 979-8-4007-\n0431-4.  DOI: 10.1145/3626772.3657957.  URL: https://dl.acm.org/\ndoi/10.1145/3626772.3657957 (visited on 01/23/2025).",
    ],
  },
  downloads: {
    title: "Downloads",
    thesisPdf: "/Tese_fixed_references.pdf",
    presentationPptx: "/Final_Presentation.pptx",
    frameworkGitHub: "https://git.mainet.uk/Jose-Ribeir/An-Adaptive-Query-Routing-Framework.git",
  },
  contact: {
    title: "Contact",
    name: "José Pedro Farinha Ribeiro",
    email: "josepfribeiro@live.com.pt",
    university: "IADE - Faculdade de Design, Tecnologia e Comunicação",
    department: "Mestre em Creative Computing and Artificial Intelligence",
  },
};
