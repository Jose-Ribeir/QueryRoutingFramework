[
  "e of keyword injection to improve the model’s per-\nformance on specialized, domain-specific tasks.  The hypothesis was that by programmatically\ninserting key technical terms (e.g., medical terminology for healthcare implementations) into\nthe  prompt,  the  model  could  be  guided  toward  a  more  accurate  and  contextually  aware  re-\nsponse. This idea was ultimately abandoned due to the immense data curation effort this would\nneed.  To be effective, this strategy would necessitate the creation of a large, validated dataset\nmapping queries to the required keywords for each domain if various were to be involved in\ntesting.  Getting hold and verifying the accuracy of this much data would be a substantial re-\nsearch project in itself, and it felt outside of the scope of this project which was focused more\ntowards the general public.\nFinally, the scope of model comparison was limited by the hardware available for this re-\nsearch.  While the study successfully demonstrates the capabilities of an 8B parameter model\non a single GPU workstation, a more comprehensive analysis would have included benchmarks\nwith higher models as well, which would normally require enterprise-grade GPUs.  Such tests\nwere not conducted due to a lack of access to these powerful computational resources.  As a\nresult, the performance comparison remains focused on demonstrating the significant leap from\nsmaller models to the proposed SLM framework, rather than a broader spectrum of available\nopen-source models.",
  "9    Conclusion\nThis  thesis  confronted  the  significant  challenge  of  deploying  powerful  language  models\nwithin environments constrained by li",
  "mited computational resources, strict data privacy regu-\nlations, and tight budgets. The prohibitive cost and operational complexities of state-of-the-art\nLarge Language Models pose a significant barrier for small to medium enterprises and regulated\ninstitutions.   In  response,  this  research  proposed  and  evaluated  a  novel  framework  (Answer\nRQ1) centered on a compact, 8 billion parameter Small Language Model.  The core innova-\ntion of this work is a dynamic, adaptive query routing system that intelligently triages incoming\nqueries, selecting the most efficient and effective path be it a direct answer, a reasoning-intensive\nchain of thought, or knowledge augmentation using retrieval-augmented generation.\nThe results of this study show that architectural control and sophisticated prompt engineer-\ning can substantially bridge the performance gap between small, quantized models and their\nlarger, more resource-intensive counterparts.  Through iterative refinement of the controller’s\ninstruction design, particularly with profile-based prompts, (Answer RQ3) the system achieved\nan  accuracy  exceeding 85%  on  reasoning-focused  science  questions.   Critically,  this  perfor-\nmance was achieved with significantly greater energy efficiency than would be possible using\nlarger models. The detailed energy profiling revealed a direct correlation between incorrect an-\nswers and higher energy consumption, and also showed how different instructions strategically\nshift the computational load between CPU-intensive retrieval and GPU-intensive generation.\nThus confirming that a ”smarter, not bigger” approach is a viable path forward.\nThe impli",
  "cations of this research are both practical and strategic.   It provides a tangible\nblueprint for developing high-quality, cost-effective, and secure question-answering system that\ncan operate on-premise on a single GPU workstation.  This work (Answer RQ4) democratizes\naccess to advanced AI capabilities, enabling organizations without massive computational in-\nfrastructure to leverage the power of language models.  Furthermore, it also contributes to the\ngrowing field of sustainable AI by demonstrating that performance and efficiency are not mu-\ntually exclusive.\n10    Future work\nThe framework developed in this thesis successfully demonstrates that an intelligently con-\ntrolled Small Language Model can achieve high performance in resource-constrained environ-\nments. This research also opens up several compelling points for future investigation that could\nfurther enhance the robustness, efficiency, and applicability of this approach.\nFirst a critical area for future research is the system’s resilience to irrelevant or misleading\ninformation  within  its  knowledge  base.   The  current  experiments  utilized  a  corpus  that  was\nsometimes relevant to the ARC dataset.  A future study could involve another dataset that has\nno relevant information to the real world,  this would split the realities of this dataset versus",
  "the ARC one, this way when retrieval occurred for ARC it wouldn’t improve the answer of the\nsystem. Though this is part of the advantage of a system like this, as it will always aim for the\nbest possible result.\nSecond, a novel and promising research direction based on the extracted results. A promis-\ning direction",
  "would be to explore the relationship between energy consumption and model hal-\nlucination. During this work, a correlation was observed between incorrect answers and higher\nenergy usage.  This suggests the possibility of identifying a computational signature for hal-\nlucination.  This could involve analyzing power draw and processing patterns to determine if\nnon-factual or fabricated responses can be detected in real-time based on their energy profile.\nIf a reliable correlation is established, this could lead to the development of a mechanism that\nflags potential hallucinations as they are being generated, allowing the system to intervene and\nreroute the query for correction, thereby improving the model’s trustworthiness.\nFinally, the adaptive routing principles pioneered here for SMLs could be extended to ad-\ndress known inefficiencies in much larger models.  State-of-the-art LLMs, despite their power,\ncan sometimes enter unproductive reasoning loops, repeatedly processing the same logic with-\nout reaching a conclusion, which wastes significant computational resources.  A future imple-\nmentation could adapt the controller to monitor the reasoning paths of an LLM. By detecting\nmajor semantic repetition or a lack of progress, the system could intervene to break the loop,\nperhaps by injecting new information via RAG or rewriting the prompt.  This would not only\nprevent wasted computation and energy while also improving the reliability of LLMs in com-\nplex, multi-step reasoning tasks positioning this framework as a valuable tool for optimizing\nboth small and large language models.",
  "Figure 55: Avg. Energy of Analysis Instructions that were also INCORRECT when Baseline Failed.",
  "Figure 56: Correctness Comparison between system versus a 14B Model.",
  "Figure 58: Average CPU Energy Consumption by Domain.",
  "Figure 59: Avg. Energy of Analysis Models that were CORRECT when Baseline Failed.",
  "Figure 60: Science: Avg. Energy when Straight Model is Incorrect & System is Correct.",
  "Figure 61: Average CPU Energy Consumption by Method.",
  "Figure 62: Efficiency Score: Energy Cost of a Correct Answer for the system versus the 14B Model at the science domain.",
  "Figure 63: Average GPU Energy Consumption by Method.",
  "Figure 65: Science Domain: Avg. Energy when Baseline is Incorrect & System is also Incorrect.",
  "Figure 67: Average Energy Consumption in Science Domain by Correctness.",
  "Figure 68: Average Energy for CORRECT Answers in Science Domain (with Counts).",
  "Figure 69: Average Energy for CORRECT Answers in Science Domain.",
  "Figure 70: Average Energy for INCORRECT Answers in Science Domain (with Counts).",
  "References\n[1]    Wenpeng Yin et al. Comparative Study of CNN and RNN for Natural Language Process-\ning. arXiv:1702.01923 [cs]. Feb. 2017. DOI: 10.48550/arXiv.1702.01923. URL:\nhttp://arxiv.org/abs/1702.01923 (visited on 01/09/2025).\n[2]    Jeffrey  L.  Elman.  Finding  Structure  in  Time.  1990.  DOI: https : / / doi . org /\n10 . 1207 / s15516709cog1402 \\ _1.  eprint: https : / / onlinelibrary .\nwiley.com/doi",
  "/pdf/10.1207/s15516709cog1402_1.  URL: https://\nonlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1.\n[3]    Neural Computation. “Long short-term memory”. In: Neural Comput 9 (2016), pp. 1735–\n1780.  URL: https : / / interactiveaudiolab . github . io / teaching /\ncasa/HorchreiterSchmidhuber_LSTM.pdf (visited on 01/09/2025).\n[4]    Kyunghyun Cho et al. On the Properties of Neural Machine Translation: Encoder-Decoder\nApproaches. arXiv:1409.1259 [cs]. Oct. 2014. DOI: 10.48550/arXiv.1409.1259.\nURL: http://arxiv.org/abs/1409.1259 (visited on 01/09/2025).\n[5]    Ashish  Vaswani  et  al.  “Attention  is  All  you  Need”.  In:  Advances  in  Neural  Informa-\ntion  Processing  Systems.  Vol.  30.  Curran  Associates,  Inc.,  2017.  URL: https : / /\nproceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html (visited on 01/08/2025).\n[6]    Sofia  Serrano  and Noah  A. Smith.  Is  Attention Interpretable?  arXiv:1906.03731 [cs].\nJune 2019.  DOI: 10.48550/arXiv.1906.03731.  URL: http://arxiv.org/\nabs/1906.03731 (visited on 01/09/2025).\n[7]    [2006.16362] Multi-Head Attention: Collaborate Instead of Concatenate. URL: https:\n//arxiv.org/abs/2006.16362 (visited on 01/10/2025).\n[8]    Katikapalli  Subramanyam  Kalyan,  Ajit  Rajasekharan,  and  Sivanesan  Sangeetha.  AM-\nMUS : A Survey of Transformer-based Pretrained Models in Natural Language Process-\ning.  arXiv:2108.05542  [cs].  Aug.  2021.  DOI: 10.48550/arXiv.2108.05542.\nURL: http://arxiv.org/abs/2108.05542 (visited on 01/11/2025).\n[9]    Tom B. Brown et al. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs].",
  "5.  URL: http://arxiv.org/\nabs/2005.14165 (visited on 01/11/2025).\n[10]    Wei Zeng et al. PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language\nModels with Auto-parallel Computation. arXiv:2104.12369 [cs]. Apr. 2021.  DOI: 10.\n48550/arXiv.2104.12369.  URL: http://arxiv.org/abs/2104.12369\n(visited on 01/11/2025).",
  "[11]    Dmitry Lepikhin  et al.  GShard:  Scaling Giant Models  with Conditional Computation\nand Automatic Sharding. arXiv:2006.16668 [cs]. June 2020. DOI: 10.48550/arXiv.\n2006 . 16668.  URL: http : / / arxiv . org / abs / 2006 . 16668  (visited  on\n01/11/2025).\n[12]    Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs]. May 2019.  DOI: 10.48550/arXiv.1810.\n04805. URL: http://arxiv.org/abs/1810.04805 (visited on 01/08/2025).\n[13]    Microsoft. microsoft/Phi-3-mini-4k-instruct at main. Sept. 2024. URL: https://huggingface.\nco / microsoft / Phi - 3 - mini - 4k - instruct / tree / main  (visited  on\n01/13/2025).\n[14]    Yanshu Wang et al. Art and Science of Quantizing Large-Scale Models: A Comprehen-\nsive Overview. arXiv:2409.11650 [cs]. Sept. 2024.  DOI: 10.48550/arXiv.2409.\n11650. URL: http://arxiv.org/abs/2409.11650 (visited on 01/08/2025).\n[15]    Zhewei Yao et al. ZeroQuant-V2: Exploring Post-training Quantization in LLMs from\nComprehensive Study to Low Rank Compensation. arXiv:2303.08302 [cs]. May 2023.\nDOI: 10.48550/arXiv.2303.08302.  URL: http://arxiv.org/abs/\n2303.08302 (visited on 01/08/2025).\n[16]    Wenxiao Wang et al. Model Compression and Efficient Inference for Large Language",
  "4. DOI: 10.48550/arXiv.2402.\n09748. URL: http://arxiv.org/abs/2402.09748 (visited on 01/08/2025).\n[17]    Gunho  Park  et  al.  LUT-GEMM:  Quantized  Matrix  Multiplication  based  on  LUTs  for\nEfficient Inference in Large-Scale Generative Language Models. arXiv:2206.09557 [cs].\nApr. 2024.  DOI: 10.48550/arXiv.2206.09557.  URL: http://arxiv.org/\nabs/2206.09557 (visited on 01/08/2025).\n[18]    Sehoon Kim et al. SqueezeLLM: Dense-and-Sparse Quantization. arXiv:2306.07629 [cs].\nJune 2024.  DOI: 10.48550/arXiv.2306.07629.  URL: http://arxiv.org/\nabs/2306.07629 (visited on 01/08/2025).\n[19]    Elias  Frantar  et  al.  GPTQ:  Accurate  Post-Training  Quantization  for  Generative  Pre-\ntrained Transformers. arXiv:2210.17323 [cs]. Mar. 2023.  DOI: 10.48550/arXiv.\n2210 . 17323.  URL: http : / / arxiv . org / abs / 2210 . 17323  (visited  on\n01/08/2025).\n[20]    Elias Frantar et al. “OPTQ: Accurate Quantization for Generative Pre-trained Transform-\ners”. en. In:  URL: https://openreview.net/forum?id=tcbBPnfwxS (vis-\nited on 01/24/2025).",
  "[21]    Elias Frantar and Dan Alistarh. “Optimal Brain Compression: A Framework for Accurate\nPost-Training Quantization and Pruning”. en. In: (). URL: https://proceedings.\nneurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-\nAbstract-Conference.html (visited on 01/08/2025).\n[22]    Hanlin Tang et al. EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs.\narXiv:2403.02775 [cs]. Mar. 2024.  DOI: 10.48550/arXiv.2403.02775.  URL:\nhttp://arxiv.org/abs/2403.02775 (visited on 01/08/2025).\n[23]    Zhewei Yao et al. “ZeroQuant: Efficient and Affordable Post-Training Quantization fo",
  "r\nLarge-Scale Transformers”. en. In: ().  URL: https://proceedings.neurips.\ncc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-\nAbstract-Conference.html (visited on 01/08/2025).\n[24]    Wei Huang et al. BiLLM: Pushing the Limit of Post-Training Quantization for LLMs.\narXiv:2402.04291 [cs]. May 2024.  DOI: 10.48550/arXiv.2402.04291.  URL:\nhttp://arxiv.org/abs/2402.04291 (visited on 01/08/2025).\n[25]    Chee-Yong Chan and Yannis E. Ioannidis. Bitmap index design and evaluation. Seattle,\nWashington, USA, 1998. DOI: 10.1145/276304.276336. URL: https://doi.\norg/10.1145/276304.276336.\n[26]    Zefan Li et al. “ICCV 2017 Open Access Repository”. In: URL: https://openaccess.\nthecvf.com/content_iccv_2017/html/Li_Performance_Guaranteed_\nNetwork_ICCV_2017_paper.html (visited on 01/08/2025).\n[27]    Wei Huang et al. An empirical study of LLaMA3 quantization: from LLMs to MLLMs.\nDec. 2024. DOI: 10.1007/s44267-024-00070-x. URL: https://doi.org/\n10.1007/s44267-024-00070-x.\n[28]    Saleh Ashkboos et al. QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. arXiv:2404.00456\n[cs]. Oct. 2024.  DOI: 10.48550/arXiv.2404.00456.  URL: http://arxiv.\norg/abs/2404.00456 (visited on 01/08/2025).\n[29]    Guangxuan Xiao et al. SmoothQuant: Accurate and Efficient Post-Training Quantization\nfor Large Language Models. Ed. by Andreas Krause et al. July 2023.  URL: https:\n//proceedings.mlr.press/v202/xiao23c.html.\n[30]    Tim Dettmers et al. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\narXiv:2208.07339 [cs]. Nov. 2022.  DOI: 10.48550/arXiv.2208.07339.  URL:\nhttp://arxiv.org/abs/2208.07339 (visited on 01/14/2025).\n[31]    Aohan Zeng e",
  "t al. GLM-130B: An Open Bilingual Pre-trained Model. arXiv:2210.02414\n[cs]. Oct. 2023.  DOI: 10.48550/arXiv.2210.02414.  URL: http://arxiv.\norg/abs/2210.02414 (visited on 01/08/2025).",
  "[32]    Susan Zhang et al. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068\n[cs]. June 2022.  DOI: 10.48550/arXiv.2205.01068.  URL: http://arxiv.\norg/abs/2205.01068 (visited on 01/08/2025).\n[33]    Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971\n[cs]. Feb. 2023.  DOI: 10.48550/arXiv.2302.13971.  URL: http://arxiv.\norg/abs/2302.13971 (visited on 01/08/2025).\n[34]    Fabio Petroni et al. Language Models as Knowledge Bases? arXiv:1909.01066 [cs]. Sept.\n2019. DOI: 10.48550/arXiv.1909.01066. URL: http://arxiv.org/abs/\n1909.01066 (visited on 01/08/2025).\n[35]    Patrick  Lewis  et  al.  “Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[36]    Vladimir Karpukhin et al. Dense Passage Retrieval for Open-Domain Question Answer-\ning.  arXiv:2004.04906  [cs].  Sept.  2020.  DOI: 10.48550/arXiv.2004.04906.\nURL: http://arxiv.org/abs/2004.04906 (visited on 01/08/2025).\n[37]    Mike Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Lan-\nguage Generation, Translation, and Comprehension. arXiv:1910.13461 [cs] version: 1.\nOct. 2019.  DOI: 10.48550/arXiv.1910.13461.  URL: http://arxiv.org/\nabs/1910.13461 (visited on 01/08/202",
  "5).\n[38]    Fabing Duan, Franc ̧ois Chapeau-Blondeau, and Derek Abbott. “Optimized injection of\nnoise in activation functions to improve generalization of neural networks”. In: Chaos,\nSolitons & Fractals 178  (Jan.  2024), p.  114363.  ISSN:  0960-0779.  DOI: 10.1016/\nj . chaos . 2023 . 114363.  URL: https : / / www . sciencedirect . com /\nscience/article/pii/S0960077923012651 (visited on 01/08/2025).\n[39]    Patrick  Lewis  et  al.  “Retrieval-Augmented  Generation  for  Knowledge-Intensive  NLP\nTasks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 9459–9474.  URL: https://proceedings.neurips.cc/\npaper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.\nhtml (visited on 01/08/2025).\n[40]    Luyu Gao et al. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv:2212.10496\n[cs]. Dec. 2022.  DOI: 10.48550/arXiv.2212.10496.  URL: http://arxiv.\norg/abs/2212.10496 (visited on 01/08/2025).",
  "[41]    Luyu Gao and Jamie Callan. Unsupervised Corpus Aware Language Model Pre-training\nfor  Dense  Passage  Retrieval.  arXiv:2108.05540  [cs].  Aug.  2021.  DOI: 10.48550/\narXiv.2108.05540.  URL: http://arxiv.org/abs/2108.05540 (visited\non 01/08/2025).\n[42]    Long Ouyang et al. “Training language models to follow instructions with human feed-\nback”. en. In: (). URL: https://proceedings.neurips.cc/paper_files/\npaper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-\nConference.html (visited on 01/08/2025).\n[43]    Brian J. Chan et al. Don’t Do RAG: When Cache-Augmented Generation is All You Need\nfor Knowledge Tasks. arXiv:2412.15605 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.\n2412 .",
  "15605.  URL: http : / / arxiv . org / abs / 2412 . 15605  (visited  on\n01/15/2025).\n[44]    Chao Jin et al. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Gen-\neration. arXiv:2404.12457 [cs]. Apr. 2024.  DOI: 10.48550/arXiv.2404.12457.\nURL: http://arxiv.org/abs/2404.12457 (visited on 01/15/2025).\n[45]    Zhuowan Li et al. “Retrieval Augmented Generation or Long-Context LLMs? A Com-\nprehensive Study and Hybrid Approach”. In: Proceedings of the 2024 Conference on Em-\npirical Methods in Natural Language Processing: Industry Track. Ed. by Franck Dernon-\ncourt, Daniel Preot ̧iuc-Pietro, and Anastasia Shimorina. Miami, Florida, US: Association\nfor Computational Linguistics, Nov. 2024, pp. 881–893. DOI: 10.18653/v1/2024.\nemnlp-industry.66.  URL: https://aclanthology.org/2024.emnlp-\nindustry.66/ (visited on 01/15/2025).\n[46]    Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\narXiv:2403.05530 [cs]. Dec. 2024.  DOI: 10.48550/arXiv.2403.05530.  URL:\nhttp://arxiv.org/abs/2403.05530.\n[47]    Woosuk Kwon et al. “Efficient Memory Management for Large Language Model Serving\nwith  PagedAttention”.  In:  Proceedings  of  the  29th  Symposium  on  Operating  Systems\nPrinciples. SOSP ’23. New York, NY, USA: Association for Computing Machinery, Oct.\n2023, pp. 611–626. ISBN: 979-8-4007-0229-7. DOI: 10.1145/3600006.3613165.\nURL: https://dl.acm.org/doi/10.1145/3600006.3613165 (visited on\n01/16/2025).\n[48]    Conglong Li et al. “Improving Approximate Nearest Neighbor Search through Learned\nAdaptive Early Termination”. In: Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data. SIG",
  "MOD ’20. New York, NY, USA: Association\nfor Computing Machinery, May 2020, pp. 2539–2554.  ISBN: 978-1-4503-6735-6.  DOI:\n10.1145/3318464.3380600. URL: https://dl.acm.org/doi/10.1145/\n3318464.3380600 (visited on 01/19/2025).",
  "[49]    Gautier Izacard et al. Unsupervised Dense Information Retrieval with Contrastive Learn-\ning. arXiv:2112.09118 [cs]. Aug. 2022. DOI: 10.48550/arXiv.2112.09118. URL:\nhttp://arxiv.org/abs/2112.09118 (visited on 01/20/2025).\n[50]    Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent Retrieval for Weakly Su-\npervised  Open  Domain  Question  Answering.  arXiv:1906.00300  [cs].  June  2019.  DOI:\n10.48550/arXiv.1906.00300.  URL: http://arxiv.org/abs/1906.\n00300 (visited on 01/20/2025).\n[51]    Kaiming He et al. Momentum Contrast for Unsupervised Visual Representation Learn-\ning. arXiv:1911.05722 [cs]. Mar. 2020. DOI: 10.48550/arXiv.1911.05722. URL:\nhttp://arxiv.org/abs/1911.05722 (visited on 01/21/2025).\n[52]    Yile Wang et al. “Self-Knowledge Guided Retrieval Augmentation for Large Language\nModels”. In: Findings of the Association for Computational Linguistics: EMNLP 2023.\nEd. by Houda Bouamor, Juan Pino, and Kalika Bali. Singapore: Association for Com-\nputational Linguistics, Dec. 2023, pp. 10303–10315.  DOI: 10.18653/v1/2023.\nfindings-emnlp.691. URL: https://aclanthology.org/2023.findings-\nemnlp.691/ (visited on 05/05/2025).\n[53]    Evelyn Fix and J. L. Hodges. “Discriminatory Analysis. Nonparametric Discrimination:\nConsistency Properties”. In: International Statistical Review / Revue Internationale de\nStatistique 57.3 (1989), pp. 238–247. ISSN: 03067734, 17515823. URL: http://www.\njstor.org/st",
  "able/1403797 (visited on 06/17/2025).\n[54]    LLM Explorer. LLM Explorer: A Curated Large Language Model Directory. LLM List.\n41870 Open-Source Language Models. en.  URL: https://llm.extractum.io\n(visited on 01/24/2025).\n[55]    Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. arXiv:2009.03300\n[cs]. Jan. 2021.  DOI: 10.48550/arXiv.2009.03300.  URL: http://arxiv.\norg/abs/2009.03300 (visited on 01/08/2025).\n[56]    Yubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Un-\nderstanding Benchmark. arXiv:2406.01574 [cs]. Nov. 2024. DOI: 10.48550/arXiv.\n2406 . 01574.  URL: http : / / arxiv . org / abs / 2406 . 01574  (visited  on\n01/08/2025).\n[57]    David Rein et al. GPQA: A Graduate-Level Google-Proof Q&A Benchmark. arXiv:2311.12022\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.12022.  URL: http://arxiv.\norg/abs/2311.12022 (visited on 01/08/2025).\n[58]    Zayne Sprague et al. MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\nReasoning.  arXiv:2310.16049  [cs].  Mar.  2024.  DOI: 10 . 48550 / arXiv . 2310 .\n16049. URL: http://arxiv.org/abs/2310.16049 (visited on 01/08/2025).",
  "[59]    Jeffrey Zhou et al. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911\n[cs]. Nov. 2023.  DOI: 10.48550/arXiv.2311.07911.  URL: http://arxiv.\norg/abs/2311.07911 (visited on 01/08/2025).\n[60]    Peter Clark et al. Think you have Solved Question Answering? Try ARC, the AI2 Reason-\ning Challenge. arXiv:1803.05457 [cs]. Mar. 2018.  DOI: 10.48550/arXiv.1803.\n05457. URL: http://arxiv.org/abs/1803.05457 (visited on 01/24/2025).\n[61]    Rowan Zellers et al. HellaSwag: Can a Machine",
  "[cs]. May 2019.  DOI: 10.48550/arXiv.1905.07830.  URL: http://arxiv.\norg/abs/1905.07830 (visited on 01/08/2025).\n[62]    Stephanie  Lin,  Jacob  Hilton,  and  Owain  Evans.  TruthfulQA:  Measuring  How  Models\nMimic Human Falsehoods. arXiv:2109.07958 [cs]. May 2022. DOI: 10.48550/arXiv.\n2109 . 07958.  URL: http : / / arxiv . org / abs / 2109 . 07958  (visited  on\n01/08/2025).\n[63]    Karl Cobbe et al. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168\n[cs]. Nov. 2021.  DOI: 10.48550/arXiv.2110.14168.  URL: http://arxiv.\norg/abs/2110.14168 (visited on 01/08/2025).\n[64]    Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset.\narXiv:2103.03874 [cs]. Nov. 2021.  DOI: 10.48550/arXiv.2103.03874.  URL:\nhttp://arxiv.org/abs/2103.03874 (visited on 01/08/2025).\n[65]    Kunlun  Zhu  et  al.  RAGEval:  Scenario  Specific  RAG  Evaluation  Dataset  Generation\nFramework.  arXiv:2408.01262  [cs].  Mar.  2025.  DOI: 10.48550/arXiv.2408.\n01262. URL: http://arxiv.org/abs/2408.01262 (visited on 03/28/2025).\n[66]    Zhilin Yang et al. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question An-\nswering. arXiv:1809.09600 [cs]. Sept. 2018. DOI: 10.48550/arXiv.1809.09600.\nURL: http://arxiv.org/abs/1809.09600 (visited on 07/29/2025).\n[67]    Rolf Jagerman et al. Query Expansion by Prompting Large Language Models. arXiv:2305.03653\n[cs]. May 2023.  DOI: 10.48550/arXiv.2305.03653.  URL: http://arxiv.\norg/abs/2305.03653 (visited on 01/22/2025).\n[68]    Lihu Chen and Ga\n ̈\nel Varoquaux. What is the Role of Small Models in the LLM Era: A\nSurvey. arXiv:2409.06857 [cs].",
  "Dec. 2024.  DOI: 10.48550/arXiv.2409.06857.\nURL: http://arxiv.org/abs/2409.06857 (visited on 01/08/2025).\n[69]    DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning. en. arXiv:2501.12948 [cs]. Jan. 2025.  DOI: 10.48550/arXiv.\n2501 . 12948.  URL: http : / / arxiv . org / abs / 2501 . 12948  (visited  on\n06/20/2025).",
  "[70]    Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can\nSolve  Them.  arXiv:2210.09261  [cs].  Oct.  2022.  DOI: 10.48550/arXiv.2210.\n09261. URL: http://arxiv.org/abs/2210.09261 (visited on 01/08/2025).\n[71]    Xinyi Wu et al. On the Emergence of Position Bias in Transformers. arXiv:2502.01951\n[cs]. June 2025.  DOI: 10.48550/arXiv.2502.01951.  URL: http://arxiv.\norg/abs/2502.01951 (visited on 06/20/2025).\n[72]    docs.nvidia.com/deploy/nvidia-smi/index.html. URL: https://docs.nvidia.com/\ndeploy/nvidia-smi/index.html (visited on 07/02/2025).\n[73]    NVML Device Queries. en-us. cppModule.  URL: https://docs.nvidia.com/\ndeploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_\n1g7ef7dff0ff14238d08a19ad7fb23fc87 (visited on 07/02/2025).\n[74]    Alireza Salemi and Hamed Zamani. “Evaluating Retrieval Quality in Retrieval-Augmented\nGeneration”. In: Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearch  and  Development  in  Information  Retrieval.  SIGIR  ’24.  New  York,  NY,  USA:\nAssociation for Computing Machinery, July 2024, pp. 2395–2400.  ISBN: 979-8-4007-\n0431-4.  DOI: 10.1145/3626772.3657957.  URL: https://dl.acm.org/\ndoi/10.1145/3626772.3657957 (visited on 01/23/2025)."
]