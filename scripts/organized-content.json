[
  {
    "pageNumber": 1,
    "imageNumber": 2,
    "type": "page",
    "folderName": "Page_1_Image_2",
    "precedingText": "2025 \n<colocar o ano \nem que \n as provas se irão \nrealizar> \nJosé Pedro Farinha \nRibeiro \nAn Adaptive Query-Routing Framework for \nOptimizing Small Language Models in Resource-\nConstrained Environments",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_1_image_2.png"
  },
  {
    "pageNumber": 2,
    "imageNumber": 3,
    "type": "page",
    "folderName": "Page_2_Image_3",
    "precedingText": "MOD-195.IADEV02;  07-07-2023",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_2_image_3.png"
  },
  {
    "pageNumber": 3,
    "imageNumber": 1,
    "type": "figure",
    "folderName": "Figure 1 Transformer model architecture [5]",
    "precedingText": "Table 2: Maximum path lengths, per-layer complexity and minimum number of sequential\noperations for different layer types. [5]",
    "caption": "Figure 1: Transformer model architecture [5]",
    "imagePath": "/images/figure_1_transformer_model_architecture__5_.png"
  },
  {
    "pageNumber": 3,
    "imageNumber": 5,
    "type": "page",
    "folderName": "Page_3_Image_5",
    "precedingText": "2025 \n<colocar o ano \nem que \n as provas se irão \nrealizar> \nJosé Pedro Farinha \nRibeiro \nAn Adaptive Query-Routing Framework for \nOptimizing Small Language Models in Resource-\nConstrained Enviornments  \nDissertação apresentada ao IADE - Faculdade de \nDesign, Tecnologia e Comunicação da Universidade \nEuropeia, para cumprimento dos requisitos necessários à \nobtenção do grau de Mestre em Creative Computing and \nArtificial Intelligence realizada sob a orientação \ncientífica da Doutora Cláudia Sofia Sevivas Ribeiro, \nprofessora auxiliar da Universidade Europeia.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_3_image_5.png"
  },
  {
    "pageNumber": 4,
    "imageNumber": 6,
    "type": "page",
    "folderName": "Page_4_Image_6",
    "precedingText": "MOD-195.IADEV02;  07-07-2023 \ntexto Apoio financeiro da FCT ou outro... \n(se aplicável)",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_4_image_6.png"
  },
  {
    "pageNumber": 5,
    "imageNumber": 7,
    "type": "page",
    "folderName": "Page_5_Image_7",
    "precedingText": "MOD-195.IADEV02;  07-07-2023 \nI want to express my deepest gratitude to my grandfather, \nLeonel Pires Ribeiro, for not only giving me a place to stay \nbut also for helping with all the expenses. I cherished every \nconversation we had throughout most of college and deeply \nappreciate all the support he provided. \n    A sincere thanks to my advisor, Prof. Cláudia Sofia \nSevivas Ribeiro, for the valuable insights shared throughout \nthe thesis and for the many constructive discussions along \nthe way. \n    A heartfelt thank you to my wonderful girlfriend, \nBárbara Höller, for taking care of most of the household \ntasks while I focused on completing this thesis. \n    None of this work would have been possible without the \nsupport of my parents, Lídia Ribeiro and Nuno Ribeiro, \nwho provided the resources needed to run all the required \nmodels and processes including the high electricity costs, \nkindly and unquestioningly covered by my grandmother, \nMaria Ribeiro. \n    And finally, the biggest thanks go to my other \ngrandmother, Maria Lopes Marçal, for all the support she \ngave me throughout my academic journey and for all the \nboxes of vegetables which, according to her, gave me my \nbeautiful eyes. \n(opcional) \nAcknowledgements",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_5_image_7.png"
  },
  {
    "pageNumber": 6,
    "imageNumber": 2,
    "type": "figure",
    "folderName": "Figure 2 RN18 squared error. [21]",
    "precedingText": "Transformer models introduced many changes to try to solve all of the problems in the\nprevious models. Starting with the Self-attention Mechanism which lowers the complexity\nfor short sequences. This is due to the fact that the computational complexity is O(n2 ∗d)\nwhich is more efficient than Recurrent layers O(n ∗d2) when the sequence n is smaller than\nthe dimensionality d. When parallelization is used it requires only O(1) sequential operations\nbut on the recurrent layers they need O(n) sequential operations due to their sequential nature.\nDue to the short path length between any two positions in the input and output sequences the\nsignals can travel between all pairs of positions, this proved to be a big improvement for leaning\nlong-range dependencies. Moreover which can be useful to improve the global context since it\ncan attend to all positions in the sequence at once. Transformers self-attention can dynamically\n18\nadjust based on the input sequence, this can also be modified to make the model focus on just\nthe local context.\nThe ability to interpret attention mechanisms is a significant advantage for understanding\nhow the model works. Papers such as [6] investigate what the model focuses on, which in turn\nprovides insights into its decision-making process.\nMulti-Head Attention this mechanism instead of performing a single attention function with\ndmodel-dimensional keys, values and queries they linearly project the queries, keys and values\nh times with different learned linear projections to dk(Queries), dk(Keys) and dv(V alues) di-\nmensions. The queries represent the search The way Multi-Head Attention computes attention\nindependently for each head allows the model to focus on different types of relationships and\nfeatures in parallel capturing more information, this also makes it possible to capture informa-\ntion that with a single head would normally be lost like complex relationships [7].\nMulti-Head Attention extends the standard attention mechanism by applying multiple atten-\ntion functions in parallel. Instead of performing a single attention operation with dmodel dimen-\nsional keys, values, and queries, the mechanism linearly projects the queries, keys, and values\nh times using different learned linear transformations into dimensions dk(Queries), dk(Keys)\nand dv(V alues). These projections allow each head to compute attention independently, en-\nabling the model to focus on different types of relationships and features simultaneously. This\nparallelization captures a richer set of dependencies and patterns, including complex relation-\nships that might otherwise be lost with a single attention head [7].\nBecause each head has smaller subspaces instead of one high-dimensional space this reduces\nthe computational complexity and improves the optimization process by scaling the dot products\nto prevent their values from becoming too large.\nIn natural languages, word order plays a crucial role in conveying meaning and context.\nTherefore, it is essential for models to incorporate positional information. Unlike recurrent or\nconvolutional models, the Transformer does not have an inherent mechanism to capture token\norder. To address this, positional encoding is introduced to inject information about the relative\nor absolute position of tokens within the input sequence. To make sure the the weight adapts to\nthe size of the query it uses the wave length of sin and cos that increases exponentially with the\ndimension of the index i, this grants that the positional encoding adjusts to compensate for any\ndimension length:\nPE(pos, 2i) = sin\n\u0012\npos\n\u0013\n[5]\n2i\ndmodel\n10000\nPE(pos, 2i + 1) = cos\n\u0012\npos\n\u0013\n[5]\n2i\ndmodel\n10000\nAs a benefit to how the model treats the positional encoding, it is able to adapt to sequences\nlonger than the ones found in training.\nFeed-Forward network consists of two linear transformations with a ReLU activation be-\n19\ntween them:\nFFN(x) = max(0, xW1 + b1)W2 + b2\n[5]\nReLU or rectified linear unit is used to add non-linearity to the network allowing it to learn more\ncomplex functions. The non linearity comes from the activation function max(0, x), which\nwhen the input x is positive it passes through without any change, on the other hand when\nthe input is negative it outputs 0. With linear transformations being the same across different\npositions, they use different parameters from layer to layer, for example if the input and output\ndimensionality is set to 512 and the inner-layer for 2048 this means that w1 will transform from\n512 to 2048 and then w2 will transform the 2048 into 512. ”Another way of describing this is\nas two convolutions with kernel size 1.”[5].\nThe evolution of the Transformer models came with GPT and BERT [8]. GPT and BERT\nwere the first T-PTLMs developed based on transformer decoder and encoder layers respec-\ntively, these models where the basis for the discovery that performance of T-PTLMs could be\nincreased just by increasing the size of the model which triggered the development of mod-\nels like GPT-3 (175B)[9], PANGU- (200B)[10] and even a model with 1.6 trillions of tokens\nnamed Switch-Transformers [11]. Although performance is not strictly linear and depends on\nmany factors, the number of tokens plays a significant role. This was only made possible in part\ndue to the parallelization ability of the Transformer model. Bert[12] differs from the original\nTransformer model due to its bidirectional architecture contrary to the original model which\nwas unidirectional. Bert uses its bidirectional architecture (left-to-right, right-to-left) to access\ncontext from both directions simultaneously.\nAnother important feature introduced during pretraining is Masked Language Modeling\n(MLM), in which 15% of the tokens in the input are randomly masked and the model learns to\npredict them using bidirectional context. Additionally, Next Sentence Prediction (NSP) is used\nto train the model to determine whether two sentences logically follow each other.\nAnother improvement to the model was the input representation, BERT adds special to-\nkens [CLS] and [SEP], these are used by the model in conjunction with segment embeddings\nto handle sentence pairs, this allowed BERT to handle both single sentence and sentence pair\ntasks. GPT differs significantly in its training approach by combining both supervised and un-\nsupervised learning. In the unsupervised phase, the model is trained on a large text corpus (e.g.,\nBooksCorpus) using standard language modeling. This is followed by a fine-tuning phase using\nsupervised learning on specific downstream tasks. This two-stage process allows the model to\nfirst learn general language patterns and then adapt to more specialized tasks. GPT also uses\nonly a Transformer decoder architecture, consisting of 12 layers of masked self-attention. Be-\ncause it relies solely on a decoder, the model can only attend to previous tokens in the sequence,\nmaking it well-suited for auto-regressive language modeling. This model construction makes it\nsuitable to need minimal architecture changes when adapting to different tasks.\n20\n5.2\nQuantization\nQuantization is one of the most important techniques used to improve the performance and\nefficiency of large language models (LLMs), which are increasingly applied across a wide range\nof domains from customer service to scientific research. However, as models grow in size and\ncomputational demands increase, a major challenge arises: the hardware required to run these\nmodels becomes a limiting factor. High-performance hardware can be extremely expensive and\nenergy-intensive. While recent advancements in hardware have enabled the development of\nmore powerful AI models, the cost and accessibility of such infrastructure remain significant\nbarriers to widespread adoption. To give an example, Microsoft’s Phi-3-mini-4k-instruct model\n[13] requires 512 H100-80G GPUs to be run consecutively for 10 days with each costing around\nC30,000. Although such needs refer to training, which is done only once, running this kind of\nmodel still proves to be a computationally heavy task.\nThe most frequently used method to improve on this challenge is quantization, which re-\nduces the computational and memory requirements of the machine learning model. It achieves\nthis by converting the model weights and, in some cases, the activations from high-precision\n32FP to a lower precision representation such as INT8. This reduces the memory consumption\nof the model on the GPU and can accelerate computation because integer operations consume\nfewer resources compared to floating-point operations.\nThe two major methods of quantization will be discussed in more detail in subsequent sec-\ntions: Post-Training and Quantization-Aware Training.\n5.2.1\nPost-Training Quantization (PTQ)\nTThis is the most commonly used quantization method because it does not require access\nto the model’s training process. The quantization is applied after the model has already been\ntrained, making it especially useful when training resources or data are unavailable. This type\nof quantization has been proven not to be as accurate at lower bit levels and there is a tendency\nof degradation if quantized to lower than 8-bits.[14] However since this method is less resource\nintensive, it has attracted more attention with a remarkable surge in post-training quantization\nmethods in the recent years.\nThe simplest approach is also the least efficient, as it directly quantizes 16-bit values to\n8-bit using row-wise symmetric quantization. While this method is straightforward, it typi-\ncally results in only negligible degradation in perplexity. However, this breaks down with 4-bit\nquantization as it witnesses a significant drop in perplexity [15]. To improve the quantization\nperformance for low-bit applications, ZeroQuant-V2 [15] proposed Low-Rank Compensation\n(LoRC) method. This method approximates the error E between the original weight matrix W\nand the quantized weight matrix ˆW using storage-efficient low-rank matrix ˜E so that ˆW + ˜E\nwould be a better approximation of the original weight W[16].\nLater research by LUT-GEMM[17] and SqueezeLLM[18] showed that non-uniform weight\n21\ndistributions could achieve even lower bit-width quantization. This is due to the weight distri-\nbution after training being nonuniform so it makes sense for the weight distribution not being\nquantized uniformly. This is done by allowing the quantization process to allocate more pre-\ncision to the ranges of weights that are more densely populated while leaving larger intervals\nfor less frequent weight ranges. Building upon these methodologies OPTQ[19] emerged as an\nadvancement in quantization for big LLM’s. Making it possible to run OPT-175B on just a\nsingle Nvidia A100 GPU or only two of the more cost-effective A6000. OPTQ also provided\ngreater results in the extreme quantization regime where models were quantized all the way\ndown to 2 bits, or even ternary values. The OPTQ [20] algorithm improved on Arbitrary Order\nInsight, prior to this method the norm was to quantize the weights in a greedy order [21] this\nmeans that the weight picked for the next quantization was picked based on minimum quan-\ntization error, this performs well but compared to arbitrary order quantization only offered a\nnegligible improvement in large, heavily-parameterized layers. The likely reasons for the lack\nof improvement were that large individual quantization errors were balanced out overall, and\nthat these errors occurred later in the quantization process when fewer weights remained to\nbe quantized, leaving less opportunity for adjustment. But with the OPTQ approach instead\nof quantizing the weights row-by-row, this method aimed to quantize the weights in all rows\nsimultaneously and in the same order. This can be shown by how the unquantized weights F\nand the inverse layer hessian (H−1\nF ) depend only on the input activations (XF) and not on the\nweights themselves, this proves that the quantization of a column affected all rows uniformly.\nColumns within blocks are quantized recursively and at each step, unquantized weights are\nupdated based on the quantized weights.",
    "caption": "Figure 2: RN18 squared error. [21]",
    "imagePath": "/images/figure_2_rn18_squared_error___21_.png"
  },
  {
    "pageNumber": 6,
    "imageNumber": 8,
    "type": "page",
    "folderName": "Page_6_Image_8",
    "precedingText": "MOD-195.IADEV02;  07-07-2023",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_6_image_8.png"
  },
  {
    "pageNumber": 9,
    "imageNumber": 3,
    "type": "figure",
    "folderName": "Figure 3 The Hessian metrics (sensitivity) and mag",
    "precedingText": "Table 7: Outlier fraction distribution in different layer index in BLOOM-7B under 3-sigma\nthreshold [22]\nOn the outlier distribution, it was explored the distribution along different modules and\nlayers, and it showed that the fraction of the outliers share different patterns in different modules\nand layers, refer to Tables 6 and 7.\nAnother characteristic found was that the FFN.2 module showed a significantly higher frac-\ntion of outliers, however there was no pattern along the layer index.\nOn the quantization range, it was observed that the dynamic quantization range of different\noptimization steps and concluded that the range decreased fast in the early stages of training\nmeaning a smaller quantization range facilitating more precise quantization of parameters. The\nstudy also revealed that after a certain number of steps, the quantization range became stable\nmeaning that the optimal range had already been achieved. In deep neural networks, not all\nweights have the same influence on the model’s performance some contribute more significantly\nthan others [22]. This implies that relying solely on the magnitude of the weights is insufficient\nto fully capture the impact of each element on the model’s behavior. A good benchmark to\ndetect parameter sensitivity is the Hessian metric. This occurs due to the fact of the Hessian\nmatrix being leveraged to assess the salience of parameters in each under-binarized layer. The\noptimized computation process to derive weight sensitivity is given by:\nsi =\nw2\ni\n[H−1]2\nii\n[24]\n26\nThe H represents the Hessian matrix of each layer and the wi which represents the original\nvalue of each element. The si is then used as a criterion for assessing the weight significance of\nthe element also used as a feature indicator for a structured selection.\nStructural search selection can be implemented using unstructured selection, allowing the\nmodel to cover all salient weights. However, this approach requires an additional 1-bit bitmap\nindex [25], which increases the average bit-width. This proves to be inefficient, especially\nfor the Hessian outlier weights that are only less than 1% of the total. According to [24] the\nmajority of the weights that are sensitive Hessian values are predominantly concentrated in\nspecific columns or rows.",
    "caption": "Figure 3: The Hessian metrics (sensitivity) and magnitude (value) of weights in LLMs. The\nweights of different layers in LLMs are characterized by bell-shaped distribution, accompanied\nby a few salient values.[24]",
    "imagePath": "/images/figure_3_the_hessian_metrics__sensitivity__and_mag.png"
  },
  {
    "pageNumber": 12,
    "imageNumber": 4,
    "type": "figure",
    "folderName": "Figure 4 Illustration of salient weight binarizati",
    "precedingText": "This pattern is due to the convergence effects inherent in multi-head self-attention mecha-\nnism of the models, thus needing a structured approach to select salient weights, reducing the\nadditional bit-map. The approach described in [24] is to employ a per-channel or per row type of\nbinarization, they determine salience through a per-column segmentation on the whole matrix.",
    "caption": "Figure 4: Illustration of salient weight binarization. The B1 binarized from salient weight is\nmade into a residual with the original value and then binarized again to obtain B2.[24]",
    "imagePath": "/images/figure_4_illustration_of_salient_weight_binarizati.png"
  },
  {
    "pageNumber": 15,
    "imageNumber": 5,
    "type": "figure",
    "folderName": "Figure 5 Comparison of activations and weights in",
    "precedingText": "Table 8: Quantized LLaMA3-8B performance[27]\n29\n5.2.4\nWeight + Activation Quantization\nAlthough weights are already extremely hard to quantize, incorporating the activations into\nthe quantization pipeline adds another degree of complexity, especially for large language mod-\nels. Compared to weights, activations have some unique challenges since they are dynamic, and\ntheir range and statistics are unknown until runtime. The LLMs also have a unique problem,\nnot broadly seen for other transformer-based models: the systematic outlier activations. These\noutliers if clipped during quantization can cause significant degradation in performance and\nrequire special attention if model accuracy is to be preserved [16].",
    "caption": "Figure 5: Comparison of activations and weights in LLAMA2-7B and OPT-13B models.",
    "imagePath": "/images/figure_5_comparison_of_activations_and_weights_in.png"
  },
  {
    "pageNumber": 15,
    "imageNumber": 5,
    "type": "figure",
    "folderName": "Figure 5 Comparison of activations and weights in_2",
    "caption": "Figure 5: Comparison of activations and weights in LLAMA2-7B and OPT-13B models.",
    "imagePath": "/images/figure_5_comparison_of_activations_and_weights_in_2.png"
  },
  {
    "pageNumber": 18,
    "imageNumber": 6,
    "type": "figure",
    "folderName": "Figure 6 Finding the sweet spot for the migration",
    "precedingText": "(a) The distributions of activations at the input to the\nFFN block in LLAMA2-7B model[28]\n(b) Magnitude of the input activations and\nweights of a linear layer in OPT-13B[29]\nAnother problem that makes it hard to quantize is the significant variations in value range\nacross different channels, which can be troublesome for the quatization algorithm. However, a\nstrong motivation for undertaking this complexity is the efficiency gained by quantizing both\nweights and activations to low-bit data types on specific hardware. This efficiency is demon-\nstrated by the performance of SmoothQuant, as shown in Figure 8.\n5.2.5\nMixed Precision Quantization\nRPTQ, or Reorder-based Post-Training Quantization, differs from per-tensor quantization\ntechniques in that it does not apply the same quantization parameters uniformly across the entire\ntensor. This distinction is important, as uniform quantization can sometimes lead to suboptimal\nresults.\nOne key problem consists of the range of quantization being too wide to cover a large value\nrange, as this can cause increased quantization errors in channels with smaller value ranges. On\n30\nthe other hand if the quantization range is too narrow it could lead to truncation of the outliers\nresulting in quantization errors.\nFor example if one channel as a range of -100 to -50 and another 80 to 100 when trying to\ncover their ranges by quantizing them form -100 to 100 this will result in a significant quanti-\nzation error for both channels.\nTo address this researchers have proposed several methods one of them being LLM.int8 [30]\nwhich utilizes mixed-precision quantization by using high-precision data types (FP16) to quan-\ntize the outliers in activations and low precision data types (INT8) for the remaining values. As\nexplained above this improves model performance preventing errors caused by the quantization\nof a wide range of values. Another method for quantizing the activation SmoothQuant [29]\nsolved the problem by introducing a process that is meant to ”smooth” the input activation by\ndividing it by a per-channel smoothing factor s ∈R, Ci. To keep the mathematical equivalence\nof a linear layer the weights are scaled accordingly in the reversed direction:\nY = (X diag(s)−1) · (diag(s)W) = ˆX ˆW\n[29]\nThe next point of SmoothQuant is called Migrate Quantization Difficulty and the idea is to con-\ntrol the trade-off between the quantization difficulty of activations and weights by redistributing\ntheir values scale, meaning migrating the difficulty from activation to weights.\nThe idea works by choosing a per-channel smoothing factor s such that ˆX = X diag(s)−1\nso it’s easier to quantize. To reduce quantization error, the effective quantization bits for all\nchannels should be increased. This maximizes the total effective quantization bits when all\nchannels share the same maximum magnitude, making the optimal choice the scale factor sj =\nmax(|Xj|), j = 1, 2, . . . , Ci where j corresponds to the j-th input channel. This choice grants\nthat after the division, all the channels will have the same maximum value, which makes it easy\nto quantize. However, this formula shifts all the quantization difficulty to the weights. As a\nresult, the quantization errors tend to be larger in the weights, leading to significant accuracy\ndegradation shown in Figure 6.",
    "caption": "Figure 6: Finding the sweet spot for the migration strength[29]",
    "imagePath": "/images/figure_6_finding_the_sweet_spot_for_the_migration.png"
  },
  {
    "pageNumber": 21,
    "imageNumber": 7,
    "type": "figure",
    "folderName": "Figure 7 Main idea of SmoothQuant when α is 0.5. T",
    "precedingText": "However there is a possibility off also pushing all of the quantization difficulty from the\n31\nweights to the activations by choosing sj =\n1\nmax(|Wj|). Similarly the model performance will\ndegrade heavily due to the activation quantization errors this introduces, therefore there is a\nneed to split all of the quantization difficulty between weights and activations so they are both\neasier to quantize.\nSmoothQuant achieves this by introducing a hyper-parameter migration strength, depicted\nin Figure 6, to control how much difficulty will be migrated from activation to weights, this is\ndone using:\nsj =\nmax(|Xj|)α\nmax(|Wj|)1−α\n[29]",
    "caption": "Figure 7: Main idea of SmoothQuant when α is 0.5. The smoothing factor s is obtained on cal-\nibration samples and the entire transformation is performed offline. At runtime, the activations\nare smooth without scaling.[29]",
    "imagePath": "/images/figure_7_main_idea_of_smoothquant_when___is_0_5__t.png"
  },
  {
    "pageNumber": 24,
    "imageNumber": 8,
    "type": "figure",
    "folderName": "Figure 8 RAG implementation overview[35]",
    "precedingText": "This formula ensures that the weights and activations at the corresponding channel share a\nsimilar maximum value, thus sharing the same difficulty[29]. Figure 7 illustrates the smoothing\ntransformation when α = 0.5, this works for models where the activation outliers aren’t very\nsignificant, on models where the outliers are more significant (e.g. GLM-130B[31] which hap-\npens to have ∼30% outliers), a larger α can be picked to migrate more quantization difficulty\nto the weights(like 0.7).\nThe performance results for this method are very promising on models like OPT-175B [32]\nshow an efficient quantization at INT8 quantization level since the method can match the FP16\naccuracy on all evaluation datasets. This also proved to be true for LLaMA [33]. Although the\noutliers in this model tend to be less severe, SmoothQuant still performed well, with an average\nperformance drop of only 0.4%. The PyTorch implementation also proved effective, achieving\na 1.51× speedup and a 1.96× memory reduction for OPT models.\n5.2.6\nQuantization-Aware Training (QAT)\nLLM-QAT is an advanced method for Quantization-Aware training (QAT) specifically de-\nsigned for LLMs[14]. This method as been proven to be accurate to quantization levels as low as\n4-bits. This method helps in keeping the origianl output distribution and allows the quantization\nof weights, activations, and the key-value cache. There are three core components, Symetric\nMinMax Quantization, Student-Teacher Framework and Data Generation Process. Symmet-\n32\nric MinMax Quantization is a method used to preserve the outliers in large language models\n(LLMs) and maintain their performance.\nXi\nQ =\n\u0014Xi\nR\nα\n\u0015\n,\nα = max(|XR|)\n2N−1 −1 ,\n[14]\nIn the function above the XQ represents the quantized values, XR represents the full preci-\nsion and the α is the scaling factor this is a general quantization formula and it can be applied\nto both weights and activations, but the method to quantize differs depending on the target. In\nthe case of weights, per-channel quantization is used, meaning that at each channel (or filter)\nin the weight tensor it will be quantized independently. This approach allows the quantization\nprocess to adapt accordingly to the specific range of values in every channel, preserving more\ninformation and reducing the possible quantization error. As for activations and the KV cache,\na per-token quantization is applied. In this case, the quantization is performed independently\nfor each token, allowing the method to account for the diverse ranges of activation values across\nthe multiple tokens. This distinction ensures that the quantization process is purposely selected\nto the specific characteristics of weights, activations, and KV cache, optimizing the trade-off\nbetween efficiency and accuracy for each case.\nThen LLM-QAT uses the student-teacher model framework to ensure that the quantized\nmodel retains the performance of the full-precision model. This works by having the teacher\nmodel the full-precision version guide the student, which is the newly quantized model. The\nguidance is provided through cross-entropy-based logits distillation.\nn\nX\nLCE = −1\nX\ni=1\npT\nc (Xi) log(pS\nc (Xi))\n[14]\n(2)\nn\nc\nOn Equation 2 the i represents the i-th sample in the batch , c denotes the number of classes\n(vocabulary size), and T and S are the Teacher and the Student models, respectively. The\nnext-token data generation is based on the full-precision model and is proposed as a method to\nsynthesize a distribution similar to the pre-training data. This data is generated by the teacher\nmodel, which begins with a random start token and iteratively generates subsequent tokens\nuntil it reaches the end of the sentence or the maximum sequence length. The LLM-QAT\nintroduced a hybrid approach to ensure the generated data is diverse and accurate, on the hybrid\napproach only the first few tokens are deterministically selected with the top-1 strategy the\nrest are stochastically sampled from the full precision SoftMax output distribution. Lastly, the\ngenerated data is then used as input for fine-tuning the quantized model, where the teacher\nmodel’s predictions serve as labels to guide training thus achieving a performance close to the\noriginal model yet quantized to a specified level.\n33\n5.3\nRetrieval-Augmented Generation (RAG)\nLarge language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [34] this is accomplished without access to any outside data [5] this comes with\nsome pros and cons. On the pros side, there’s the ability of the model to capture a lot of data\nand compress it, on the other hand, this comes with the downside of not being able to expand\nthe model knowledge or even revise their memory. Another downside is the hallucinations that\nsometimes are produced by this models. These limitations can be addressed using a method pro-\nposed by [35], known as Retrieval-Augmented Generation (RAG). This method consists of four\nmain components: a query encoder (q), a retriever (pn), a document indexer, and a generator\n(p0).",
    "caption": "Figure 8: RAG implementation overview[35]",
    "imagePath": "/images/figure_8_rag_implementation_overview_35_.png"
  },
  {
    "pageNumber": 26,
    "imageNumber": 9,
    "type": "page",
    "folderName": "Page_26_Image_9",
    "precedingText": "MOD-195.IADEV02;  07-07-2023 \nAI; RAG; Quantization; HyDE; Large Language Models; \nInstruction Optimization. \npalavras-chave \nresumo \nÀ medida que os custos computacionais e financeiros dos \nmodelos de linguagem de grande escala (LLMs) de última \ngeração continuam a aumentar, a sua implementação torna-\nse mais difícil para organizações com recursos limitados. \nUma vez que métodos de melhoria como a Retrieval \nAugmented Generation (RAG), Chain-of-Thought (CoT), \nHyDE e técnicas relacionadas melhoram a qualidade, mas \nimplicam custos variáveis. Este trabalho apresenta uma \nestrutura dinâmica de encaminhamento de consultas, na \nqual um LLM compacto (8 mil milhões de parâmetros) é \nemparelhado com um controlador adaptativo que seleciona \numa de três vias por consulta: resposta direta, CoT ou \nRAG. Para tal, o controlador baseia-se no refinamento \niterativo de prompts, progredindo através de seis designs de \ninstrução que evoluem de heurísticas baseadas em formato \npara uma classificação baseada em perfil, e emprega um \npós-processador do tipo votação para garantir uma extração \nde decisão robusta. A estrutura proposta é avaliada em \ntermos de precisão de encaminhamento, correção da \nresposta de ponta a ponta e perfil energético detalhado \n(CPU e GPU), utilizando um conjunto de dados compósito \nque combina conhecimento geral com forte dependência de \nrecuperação de informação e questões de ciência focadas \nem raciocínio (itens ao estilo ARC-Easy e HotPotQA), num \ncomputador com uma única GPU. Os resultados mostram \nque os prompts baseados em perfil podem melhorar o \nequilíbrio do encaminhamento: as versões mais \ndesenvolvidas atingem uma precisão de resposta superior a \n85% em consultas do tipo ARC, mantendo-se muito mais \neficientes em termos energéticos do que modelos maiores. \nAlém disso, as análises demonstram que as respostas \nincorretas consomem mais energia e que o design da \ninstrução transfere o consumo de energia entre a \nrecuperação de informação (RAG), intensiva em CPU, e o \nraciocínio, intensivo em GPU. Consequentemente, os \nnossos resultados indicam que o controlo arquitetónico e a \nengenharia de prompts podem diminuir a diferença de \ndesempenho entre modelos de pequena e média dimensão, \nenquanto alcançam ganhos de eficiência significativos e \nfornecem um caminho prático para sistemas de \nRecuperação de Informação (IR) e de Pergunta-Resposta \n- i - \n(QA) de alta qualidade, sob fortes restrições de recursos e \nrequisitos de segurança de dados. \n- ii - \n- iii - \nAI; RAG; Quantization; HyDE; Large Language Models; \nInstruction Optimization. \nKeywords \nAs the computational and financial costs of state-of-the-art \nlarge language models (LLMs) continue to grow, deploying \nthem becomes harder for resource-constrained \norganizations as improvement methods such as Retrieval-\nAugmented Generation (RAG), Chain-of-Thought (CoT), \nHyDE, and related techniques enhance quality but incur \nvariable overheads. This work presents a dynamic query-\nrouting framework, in which a compact LLM (8B \nparameters) is paired with an adaptive controller that selects \nfrom three routes per query: direct answer, CoT or, RAG. \nTherefore the controller builds on iterative prompt \nrefinement, proceeding through six instruction designs that \nevolve from format-driven heuristics to profile-based \nclassification, and employs a voting-style post-processor to \nensure robust decision extraction. The proposed framework \nis evaluated on routing accuracy, end-to-end answer \ncorrectness, and detailed energy profiling (CPU and GPU) \nusing a composite dataset that combines retrieval-heavy \ngeneral-knowledge and reasoning-focused science \nquestions (ARC-Easy and HotPotQA-style items), on a \nsingle-GPU workstation. Results show that profile-based \nprompts can improve routing balance: mature versions \nreach 85%+ answer accuracy on ARC-style queries while \nremaining much more energy efficient than larger models. \nMoreover, analyses show that incorrect answers consume \nmore energy, and that instruction design shifts the energy \nburden between CPU-heavy retrieval and GPU-heavy \nreasoning. Consequently our results indicate that \narchitectural control and prompt engineering can close the \nperformance gap between small and mid-sized models \nwhile achieving significant efficiency gains and providing a \npractical path to high-quality IR and QA systems under \ntight resource constraints and data security requirements. \nabstract \n- iv - \n- v - \nContents\n1\nAbstract\n2\n2\nResumo\n3\n3\nAcknowledgments\n11\n4\nIntroduction\n15\n4.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2\nAims and Research Questions\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.3\nDocument Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n5\nState-of-the-Art\n17\n5.1\nTransformers and Language Models . . . . . . . . . . . . . . . . . . . . . . .\n17\n5.2\nQuantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n5.2.1\nPost-Training Quantization (PTQ) . . . . . . . . . . . . . . . . . . . .\n21\n5.2.2\nWeight-Only Quantization . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5.2.3\nNon Uniform Weight Quantization . . . . . . . . . . . . . . . . . . . .\n24\n5.2.4\nWeight + Activation Quantization . . . . . . . . . . . . . . . . . . . .\n30\n5.2.5\nMixed Precision Quantization . . . . . . . . . . . . . . . . . . . . . .\n30\n5.2.6\nQuantization-Aware Training (QAT) . . . . . . . . . . . . . . . . . . .\n32\n5.3\nRetrieval-Augmented Generation (RAG) . . . . . . . . . . . . . . . . . . . . .\n34\n5.3.1\nHypothetical Document Embeddings (HyDE) . . . . . . . . . . . . . .\n36\n5.3.2\nCache Augmented Generation . . . . . . . . . . . . . . . . . . . . . .\n39\n5.3.3\nHybrid approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.4\nChallenges and Applications of Quantization in RAG . . . . . . . . . . . . . .\n48\n5.5\nRetrieval Methods to enhance HyDE . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.5.1\nContriever . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.6\nSelf-Knowledge Guided Retrieval Augmentation\n. . . . . . . . . . . . . . . .\n50\n5.6.1\nCollecting Self-Knowledge . . . . . . . . . . . . . . . . . . . . . . . .\n51\n5.6.2\nEliciting Self-Knowledge of LLMs\n. . . . . . . . . . . . . . . . . . .\n51\n5.6.3\nUsing Self-Knowledge for Adaptive Retrieval Augmentation\n. . . . .\n54\n5.7\nModel Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.7.1\nMMLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.7.2\nMMLU-Pro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.7.3\nGPQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n5.7.4\nMUSR\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n5.7.5\nBBH\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n5.7.6\nIFEval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n5.7.7\nARC\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n4\n5.7.8\nHellaSwag\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n5.7.9\nThrutfulQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n5.7.10 WinoGrande\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n5.7.11 GSM8K . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n5.7.12 Math Lvl5\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n5.7.13 RAGEval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n5.7.14 HotPotQA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n6\nMethodology\n64\n6.1\nOverview of the Query Rewriting Flow\n. . . . . . . . . . . . . . . . . . . . .\n64\n6.2\nHardware and Software Environment . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3\nRewriting Approaches\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3.1\nStraight LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3.2\nChain-of-Thought\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3.3\nRAG\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n6.3.4\nSelecting the Approach . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n6.4\nDataset Augmentation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.5\nQuery-Answer Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.5.1\nAutomated Preparation . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.5.2\nAI-Powered Triage . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.5.3\nHuman Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.5.4\nDataset formation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.6\nPower Data Collection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.6.1\nGPU\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.6.2\nCPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.7\nEvaluation Framework\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n6.7.1\nRetrieval Performance Metrics . . . . . . . . . . . . . . . . . . . . . .\n72\n6.7.2\nStraight Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n6.7.3\nChain-of-Thought Reasoning Performance Metrics . . . . . . . . . . .\n73\n6.7.4\nAutomated Evaluation Script . . . . . . . . . . . . . . . . . . . . . . .\n73\n6.7.5\nEfficiency Metrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n6.8\nOptimizing Query Classification through Iterative Prompt Refinement . . . . .\n74\n6.8.1\nInstruction V1: A Simple Baseline . . . . . . . . . . . . . . . . . . . .\n75\n6.8.2\nInstruction V2: An Aggressive, Safety-First Heuristic . . . . . . . . . .\n77\n6.8.3\nInstruction V3: Introducing Balanced Criteria . . . . . . . . . . . . . .\n80\n6.8.4\nInstruction V4: A Shift to Profile-Based Classification . . . . . . . . .\n84\n6.8.5\nInstruction V5: Final Refinement with a Guiding Principle . . . . . . .\n88\n6.8.6\nInstruction V6: A Strategic Pivot to Efficiency\n. . . . . . . . . . . . .\n92\n6.9\nAnalysis of Energy Consumption and Efficiency . . . . . . . . . . . . . . . . .\n94\n5\n6.10 Detailed Energy Consumption Profiles . . . . . . . . . . . . . . . . . . . . . .\n98\n6.10.1 Overall Energy Trends Across Instruction Versions . . . . . . . . . . .\n98\n6.10.2 CPU vs. GPU: Deconstructing the Energy Cost . . . . . . . . . . . . . 100\n6.10.3 The Energetic Cost of Correcting Errors . . . . . . . . . . . . . . . . . 101\n6.10.4 Energy Distribution and Consumption Predictability\n. . . . . . . . . . 104\n6.10.5 Overall Performance Quadrant: Synthesizing Accuracy and Efficiency\nfor ARC queries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n7\nExperimental Consistency and Reproducibility\n107\n8\nChallenges and Abandoned Approaches\n108\n9\nConclusion\n109\n10 Future work\n109\n11 Images\n111\n6\nList of Figures\n1\nTransformer model architecture [5] . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2\nRN18 squared error. [21] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3\nThe Hessian metrics (sensitivity) and magnitude (value) of weights in LLMs.\nThe weights of different layers in LLMs are characterized by bell-shaped dis-\ntribution, accompanied by a few salient values.[24] . . . . . . . . . . . . . . .\n27\n4\nIllustration of salient weight binarization. The B1 binarized from salient weight\nis made into a residual with the original value and then binarized again to obtain\nB2.[24]\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5\nComparison of activations and weights in LLAMA2-7B and OPT-13B models.\n30\n6\nFinding the sweet spot for the migration strength[29] . . . . . . . . . . . . . .\n31\n7\nMain idea of SmoothQuant when α is 0.5. The smoothing factor s is obtained\non calibration samples and the entire transformation is performed offline. At\nruntime, the activations are smooth without scaling.[29] . . . . . . . . . . . . .\n32\n8\nRAG implementation overview[35] . . . . . . . . . . . . . . . . . . . . . . . .\n34\n9\nAn illustration of the HyDE model.[40]\n. . . . . . . . . . . . . . . . . . . . .\n36\n10\nComparison RAG on the top and CAG on the botom[43] . . . . . . . . . . . .\n39\n11\nComparison between performance and costs on multiple models using LC,\nRAG and Self-Route[45] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n12\nDistribution of the difference of prediction scores between RAG and LC[45] . .\n41\n13\nTrade-off curves between (a) model performance and (b) token percentage as a\nfunction of k.[45] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n14\nRAGCache Overview[44] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n15\nKnowledge Tree[44]\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n16\nCost estimation PGDSF[44]\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n17\nCache aware Reordering[44] . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n18\nSpeculative Pipelining[44] . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n19\nOptimal speculative pipelining strategy [44] . . . . . . . . . . . . . . . . . . .\n48\n20\nThe SKR Pipeline and its component interactions.[52] . . . . . . . . . . . . . .\n50\n21\nDirect Prompting [52] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n22\nIn-Context Learning [52] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n23\nk-nearest-neighbor to understand model knowledge [52]\n. . . . . . . . . . . .\n53\n24\nRAGEval System: 1 summarizing a schema containing specific knowledge\nfrom seed documents. 2 filling in factual information based on this schema\nto generate diverse configurations. 3 generating documents according to the\nconfigurations. 4 creating evaluation data composed of questions, answers, and\nreferences derived from the configurations and documents.[65] . . . . . . . . .\n62\n25\nRelationship between model size and monthly downloads[68] . . . . . . . . . .\n64\n7\n26\nTraditional information retrieval architecture[68]\n. . . . . . . . . . . . . . . .\n65\n27\nGraphical representation of the system diagram[68] . . . . . . . . . . . . . . .\n65\n28\nAnalysis Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n29\nJsonl Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n30\nInstruction V1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n31\nInstruction V1 Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n32\nInstruction V2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n33\nInstruction V2 Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n34\nInstruction V3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n35\nInstruction V3 Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n36\nInstruction V4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n37\nInstruction V4 Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n38\nInstruction V5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n39\nInstruction V5 Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n40\nInstruction V6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n41\nInstruction V6 Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n42\nEnergy Costs vs. Correctness Scatterplot . . . . . . . . . . . . . . . . . . . . .\n95\n43\nEnergy Costs vs. Correctness Scatterplot . . . . . . . . . . . . . . . . . . . . .\n96\n44\nDomain Average Energy per Correct Answer\n. . . . . . . . . . . . . . . . . .\n97\n45\nDomain Average Energy per Incorrect Answer . . . . . . . . . . . . . . . . . .\n97\n46\nAverage Energy Consumption per Query by File . . . . . . . . . . . . . . . . .\n99\n47\nTotal Energy Percentage Difference from Baseline . . . . . . . . . . . . . . . . 100\n48\nTotal Energy Consumption by File (CPU vs GPU) . . . . . . . . . . . . . . . . 101\n49\nGeneral Knowledge: Avg. Energy when Straight Model is Incorrect & System\nis Correct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n50\nScience: Avg. Energy when Straight Model is Incorrect & System is Correct . . 103\n51\nScience: Avg. Energy when Straight Model is Incorrect & System is also Incorrect103\n52\nDistribution of Energy Consumption per Query by File . . . . . . . . . . . . . 104\n53\nOverall Performance Overview: Correctness vs. Energy Cost for ARC queries . 106\n54\nAverage GPU Energy Consumption by Domain. . . . . . . . . . . . . . . . . . 111\n55\nAvg. Energy of Analysis Instructions that were also INCORRECT when Base-\nline Failed.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n56\nCorrectness Comparison between system versus a 14B Model. . . . . . . . . . 113\n57\nAverage Energy Consumption by Domain. . . . . . . . . . . . . . . . . . . . . 114\n58\nAverage CPU Energy Consumption by Domain. . . . . . . . . . . . . . . . . . 115\n59\nAvg. Energy of Analysis Models that were CORRECT when Baseline Failed. . 116\n60\nScience: Avg. Energy when Straight Model is Incorrect & System is Correct.\n. 117\n61\nAverage CPU Energy Consumption by Method. . . . . . . . . . . . . . . . . . 118\n8\n62\nEfficiency Score: Energy Cost of a Correct Answer for the system versus the\n14B Model at the science domain. . . . . . . . . . . . . . . . . . . . . . . . . 119\n63\nAverage GPU Energy Consumption by Method. . . . . . . . . . . . . . . . . . 120\n64\nAverage Energy Consumption by Method. . . . . . . . . . . . . . . . . . . . . 121\n65\nScience Domain: Avg. Energy when Baseline is Incorrect & System is also\nIncorrect.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n66\nTotal Energy Consumption by Answer. . . . . . . . . . . . . . . . . . . . . . . 123\n67\nAverage Energy Consumption in Science Domain by Correctness.\n. . . . . . . 124\n68\nAverage Energy for CORRECT Answers in Science Domain (with Counts). . . 125\n69\nAverage Energy for CORRECT Answers in Science Domain. . . . . . . . . . . 126\n70\nAverage Energy for INCORRECT Answers in Science Domain (with Counts). . 127\n9\nList of Tables\n2\nMaximum path lengths, per-layer complexity and minimum number of sequen-\ntial operations for different layer types. [5] . . . . . . . . . . . . . . . . . . . .\n18\n3\nRuntime Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4\nPPL results on Wikitext2 of BLOOM-7B with and without outlier isolation. [22]\n25\n5\nPPL results after pruning 1% weight with different magnitude [22] . . . . . . .\n25\n6\nOutlier fraction distribution in different modules in BLOOM-7B under 3-sigma\nthreshold [22] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n7\nOutlier fraction distribution in different layer index in BLOOM-7B under 3-\nsigma threshold [22]\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n8\nQuantized LLaMA3-8B performance[27]\n. . . . . . . . . . . . . . . . . . . .\n29\n9\nComparison of Decoding Methods . . . . . . . . . . . . . . . . . . . . . . . .\n36\n10\nResults for web search on DL19/20. Best performing w/o relevance and over-\nall system(s) are marked bold. DPR, ANCE and ContrieverFT are in-domain\nsupervised models that are finetuned on MS MARCO training data. [40] . . . .\n38\n11\nAccuracy on each set [57] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n12\nLLMs using CoT+, Humans scores on the multiple domains[58] . . . . . . . .\n57\n10\n3\nAcknowledgments\nI want to express my deepest gratitude to my grandfather, Leonel Pires Ribeiro, for not only\ngiving me a place to stay but also helping with all the expenses. I appreciated every conversation\nthroughout most of college and all the help he provided.\nA great thanks to my advisor, Prof. Cl´audia Sofia Sevivas Ribeiro, for all the great insights\nprovided throughout the thesis and all the constructive discussions held along the way.\nA heartfelt thanks to my beautiful girlfriend, B´arbara H¨oller, for taking care of most of the\nhousehold tasks while I was focused on bringing this thesis to its end.\nNone of this work could have been done without the help of my parents, L´ıdia Ribeiro and\nNuno Ribeiro, for all the resources provided to run the required models and processe including\nthe expensive electricity bill, which was paid without hesitation by my grandmother, Maria\nRibeiro.\nThe biggest thanks go to my other grandmother, Maria Lopes Marc¸al, for all the support\nshe gave me throughout my entire studies, and for all the boxes of vegetables that give me my\nbeautiful eyes according to her.\n11\nList of Abbreviations\nAF\nAdversarial Filtering\nAI\nArtificial Intelligence\nARC\nAI2 Reasoning Challenge\nBART\nBidirectional Auto-Regressive Transformer\nBBH\nBig-Bench-Hard\nBERT\nBidirectional Encoder Representations from Transformers\nBM25\nBest Match 25\nBPTT\nBackpropagation Through Time\nCAG\nCache Augmented Generation\nCCPA\nCalifornia Consumer Privacy Act\nCoT\nChain of Thought\nCoT+\nChain of Thought plus\nDNNs\nDeep Neural Networks\nDPR\nDense Passage Retriever\nEM\nExact Match\nFFN\nFeed-Forward Network\nFLOPs\nFloating Point Operations per Second\nFP16\n16-bit Floating Point\nFP32\n32-bit Floating Point\nGDPR\nGeneral Data Protection Regulation\nGPQA\nGraduate-Level Google-Proof Q&A\nGPT\nGenerative Pre-trained Transformer\nGRU\nGated Recurrent Unit\nGSM8K\nGrade School Math 8K\n12\nHBM\nHigh Bandwidth Memory\nHotPotQA\nA dataset for question answering\nHyDE\nHypothetical Document Embedding\nICT\nInverse Cloze Task\nIFEval\nInstruction-Following Evaluation\nINST\nInstruction\nINT4\n4-bit Integer\nINT8\n8-bit Integer\nIR\nInformation Retrieval\nkNN\nk-Nearest-Neighbor\nKV\nKey-Value\nLC\nLong-Context\nLLM.int8\nA specific quantization method\nLLM-QAT\nLanguage Model - Quantization-Aware Training\nLLMs\nLarge Language Models\nLoRC\nLow-Rank Compensation\nLSTM\nLong Short-Term Memory\nLUT-GEMM\nLook-Up Table based General Matrix-matrix Multiplication\nMIPS\nMaximum Inner Product Search\nMLM\nMasked Language Modeling\nMMLU\nMassive Multitask Language Understanding\nMoCo\nMomentum Contrast\nMUSR\nMulti-Step Reasoning\nNLI\nNatural Language Inference\nNLP\nNatural Language Processing\nNSP\nNext Sentence Prediction\n13\nOBQ\nOptimal Brain Quantization\nOPTQ\nOptimal Quantization\nPCIe\nPeripheral Component Interconnect Express\nPGDSF\nPrefix-aware Greedy-Dual-Size-Frequency\nPIQA\nPhysical Interaction: Question Answering\nPMI\nPointwise Mutual Information\nPPL\nPerplexity\nPTQ\nPost-Training Quantization\nQAT\nQuantization-Aware Training\nQRA\nQuestion-Reference-Answer\nRAG\nRetrieval-Augmented Generation\nRNN\nRecurrent Neural Network\nRPTQ\nReorder-based Post-training Quantization\nRTN\nRounding to Nearest-Number\nRTRL\nReal-Time Recurrent Learning\nSKR\nSelf-Knowledge Guided Retrieval\nSLMs\nSmall Language Models\nSMEs\nSmall to Medium Enterprises\nSMLs\nSmall Language Models\nSpQR\nSparse-Quantized Representation\nT-PTLMs\nTransformer-based Pre-trained Language Models\nThrutfulQA\nTruthfulQA\nvLLM\nA high-throughput LLM serving engine\nWinoGrande\nWinograd Schema Challenge\n14\n4\nIntroduction\nArtificial Intelligence (AI) has revolutionized natural language processing (NLP) through\nthe advent of Large Language Models (LLMs), which demonstrate exceptional capabilities\nin understanding and generating human-like language, with widespread applications across\ndiverse industries. However, deploying these models in real-world, regulated environments\npresents substantial challenges.\nOrganizations like banks, hospitals, and government offices are likely to handle sensitive\ninformation that should not exit their premise under strict data privacy laws like GDPR and\nCCPA. Legal restrictions like these inhibit the use of AI models that are often hosted on external\nservers, where there is limited transparency in data processing operations. Furthermore, the\ncomputational demands of LLMs make them prohibitively expensive for small and medium-\nsized enterprises (SMEs), which often lack access to high-performance hardware infrastructure.\nDespite advancements such as model quantization and the development of lightweight LLMs,\na major gap still remains in effectively adapting these models to specialized, domain-specific\ntasks under limited computational resources. Methods like Retrieval-Augmented Generation\n(RAG) and Hypothetical Document Embedding (HyDE) have emerged as strong contenders\nin the sense that they can integrate external knowledge into the models with ease. However,\nsuccess in such applications largely relies on the quality of query rewriting and retrieval.\nThis thesis focuses on retrieval-based question answering in such contrained domains, lever-\naging query rewriting to improve relevance and reduce computational overhead. This is achieved\nby at first optimizing Small Language Models (SLMs) through advanced quantization tech-\nniques, which significantly reduce their computational and memory footprint. However, this ap-\nproach introduces a critical challenge, that is the degradation in model performance and knowl-\nedge retention. To counter this effect, the system integrates a powerful Retrieval-Augmented-\nGeneration (RAG) framework. This framework is not merely just add-on for external knowl-\nedge but it serves as a targeted mechanism to recover most of the performance lost during the\nquantization stage. By leveraging instructions optimization, and Chain-of-Thought reasoning,\nthe RAG component ensures that the quantized SLM can access and effectively utilize precise,\nrelevant information from external document.\n4.1\nMotivation\nArtificial Intelligence (AI) has made remarkable advancements in natural language process-\ning (NLP) through the development of large language models (LLMs). Despite their capabili-\nties, significant challenges remain in deploying these models in highly regulated and resource-\nconstrained environments. Organizations such as banks and public institutions face significant\nbarriers in adopting AI models due to strict data protection laws (eg., GDPR, CCPA) and high\ncomputational requirements. These constraint limit their ability to utilize externally hosted\nLLMs or fine-tune large models for domain-specific tasks. Additionally, the computational\n15\ndemands of LLMs make them expensive to deploy, requiring powerful hardware infrastruc-\nture that is often unaffordable for small to medium enterprises (SMEs). While advancements\nin quantization techniques and lightweight models have made LLMs more accessible, there is\nstill a gap in optimizing these models for domain-specific tasks without extensive computa-\ntional resources. Retrieval-Augmented Generation (RAG) and HyDE methods have emerged\nas promising solutions, enabling models to integrate external knowledge efficiently. However,\ntheir effectiveness depends on the quality of query rewriting and retrieval mechanisms. This\nthesis seeks to address these challenges by evaluating lightweight, domain-aware strategies for\nquery rewriting within a RAG framework. By leveraging techniques such as query augmen-\ntation, voting system, Retrieval Augmented Generation and Chain-of-Thought reasoning, the\ngoal is to improve retrieval relevance and performance in regulated and resource-constrained\nsettings. This work will also explore the trade-offs between large quantized models and small\nnon-quantized models, providing practical insights for deploying AI systems in real-world ap-\nplications.\n4.2\nAims and Research Questions\nThis thesis aims to optimize small language models (SLMs), explore the trade-offs between\nquantized and non-quantized models, investigate retrieval methods to enhance SLM perfor-\nmance, and outline directions for future research. SLM optimization will be approached by ex-\nploring recent techniques identified throughout the course of this work, with the goal of making\nthese models more practical and resource-efficient. Analyzing the trade-offs between quantized\nand non-quantized models is an important step, as it will help determine whether future research\nshould focus on quantizing larger models or on further refining smaller ones.\nRetrieval methods are a good way to achieve better performance on SMLs on tasks that\nnormally would require training with more specific data-sets. Selecting an effective retrieval\nstrategy is key to developing a lightweight, high-performing system.\nModel optimization may also include simple strategies such as query injection. These ap-\nproaches will be evaluated to determine their usefulness and relevance to the overall objectives\nof the thesis.\nThis research aims to answer the following key questions:\nRQ1: How can a system using smaller models still compete with larger ones in terms of\nperformance and efficiency?\nRQ2:Can HyDE be applied to a system designed for efficiency?\nRQ3: What are the trade-offs among answer quality, inference latency, and energy con-\nsumption for each rewriting strategy?\nRQ4: Can an efficient approach still achieve high accuracy while remaining useful?\n16\n4.3\nDocument Outline\n1. Introduction\n• Sets the stage by explaining the motivation, challenges, and research questions.\n2. Background and State-of-the-Art\n• Reviews existing techniques and it’s limitations.\n3. Methodology\n• Details the approaches used to optimize SLMs and enhance retrieval.\n4. Evaluation Framework\n• Explains how the methods are assessed using different metrics.\n5. Results and Discussion\n• Analyzes the outcomes and trade-offs of the proposed methods.\n6. Conclusion and Future Work\n• Summarizes the contributions and suggests future work.\n5\nState-of-the-Art\n5.1\nTransformers and Language Models\nNatural language processing (NLP) has been improving significantly over the last decades\ndue in part to the resurgence of deep neural networks (DNNs) [1]. The first sequential archi-\ntecture, RNN [2], had limitations regarding it’s ability to capture temporal dependencies. This\nlimitation is related to the vanishing or exploding gradient, which results in the impossibility for\nRNN to retain information over longer sequences. The longer the sequence, the more the gradi-\nents would diminish to near zero or infinity, resulting in less relevant weight updates. LSTM [3]\nand GRU [4] are two sequential models developed to overcome this limitation. LSTM was the\nfirst to use an algorithm to consider gradient-based learning, which could bridge time intervals\nin excess of 1000 steps. This was achieved using memory cells and gating mechanisms (input,\nforget and output). This allowed the networks to retain, update, or forget information in the\nmemory cell, avoiding the vanishing/exploding gradient. On the other hand GRU, doesn’t have\na separate memory cell; instead, it directly updates the hidden state using two gates: an update\ngate that combines the forget and input gates of the LSTM model and a reset gate that gives the\nnetwork the ability to control how much information it forgets.\n17",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_26_image_9.png"
  },
  {
    "pageNumber": 27,
    "imageNumber": 9,
    "type": "figure",
    "folderName": "Figure 9 An illustration of the HyDE model.[40]",
    "precedingText": "The first required model is a retriver DPR based on [36]. This retriver works by indexing all\nthe passages in a low-dimensional and continuous space, so that later the top K passages can\nbe retrived efficiently. At run-time, passages that are relevant to the input question are retrieved\nfor the reader module. The number of passages from which the model can select is extremely\nlarge the paper refers to a corpus of 21 million passages, with the value of K (i.e., the number\nof retrieved passages) ranging between 20 and 100.\nDPR represented by pn(z|x) follows a bi-encoder architecture:\npη(z | x) ∝exp(d(z)⊤q(X)),\nd(z) = BERTd(z),\nq(x) = BERTq(x)\n[35]\nWhere d(z) is a dense representation of a document produced by a BERTBASE document\nencoder [12] and q(x) a query representation produced by a query encoder, in this case also\nusing BERTBASE. The retrieval is done using (MIPS) Maximum Inner Product Search to\ncalculate the top-k(pη(· | x)), which represents the list of k documents z with highest prior\nprobability pn(z|x). MIPS identifies these documents by finding those with the largest inner\nproduct between their dense representations and the query representation. This process can be\napproximately solved in sub-linear time using an efficient approximation to the nearest neighbor\nsearch, like FAISS or hierarchical navigable small-world graphs. This is crucial for enabling\nthe scalability of large document collections such as Wikipedia.\nA pre-trained bi-encoder from DPR is used to initialize the retriever and to build the doc-\n34\nument index. The Generator is used to combine the input x with the retrieved content z using\nBART, these are simply concatenated.\nThe generator component given by p (yi | x, z, y1:i−1) could be modeled using any encoder\ndecoder, however BART-large [37] a pre-trained seq2seq transformer with 400M parameters, is\ncommonly used.\nBART pre trainned using a denoising objective which served as its foundation. For this\nspecific use case, they also added a variety of different noising functions in the training to\nprevent BART from overfitting and also encourage contextual understanding [38].\nThey refer to the BART Generator parameters 0 as the parametric memory and all of the\nretrieved external knowledge as non-parametric knowledge. The marginalization can be done\nvia two methods described in the paper RAG-Sequence model [39] uses the same retrieved\ndocument to generate the entire output sequence. Specifically, it treats the retrieved document\nas a single latent variable, which is marginalized to obtain the sequence-to-sequence probability\np(y—x) using a top-K approximation.\npRAG-Sequence(y|x) ≈\nX\nz∈top-k(p(·|x))\npη(z|x)pθ(y|x, z)\ni=1\npθ(yi|x, z, y1:i−1)\n[35]\n(3)\nN\nY\n=\nX\nz∈top-k(p(·|x))\npη(z|x)\nThis marginalization allows the model to combine information from the topk document,\neffectively converging the information from diverse sources within the same document to gen-\nerate a coherent and contextually accurate output. The final retrieval step ensures that the most\nrelevant documents are selected based on their dense representations,\nThe second method is RAG-Token model which can be used to draw a different latent doc-\nument for each target token and marginalize accordingly. This makes it possible for the gen-\nerator to choose the content from several documents when producing the answer. The top-K\ndocuments are retrieved using a retriever. The generator then produces a probability distribu-\ntion for the next output token for each retrieved document. This process is repeated iteratively,\nmarginalizing over the documents at each step, to generate the subsequent output tokens:\nN\nY\nX\npRAG-Token(y|x) ≈\nz∈top-k(p(·|x))\npη(z|x)pθ(yi|x, zi, y1:i−1)\n[35]\ni=1\nIn the case for sequence classification tasks RAG-Sequence and RAG-Token can be used by\nconsidering the target class as a target sequence of length one.\n35\nAspect\nRAG-Token\nRAG-Sequence\nDocument Usage\nDifferent documents for each to-\nken.\nSame document for the entire se-\nquence.\nMarginalization\nPer-token over top-K documents.\nPer-sequence over top-K docu-\nments.\nBeam Search\nStandard beam search.\nBeam search for each document.\nEfficiency\nComputationally efficient.\nThorough Decoding is expensive\nFlexibility\nCombines information from multi-\nple documents dynamically.\nRelies on a single document for the\nentire sequence.\nTable 9: Comparison of Decoding Methods\n5.3.1\nHypothetical Document Embeddings (HyDE)\nHyDE [40] is another retrieval method that aims to increase the performance of the model\non zero-shot scenarios, meaning that they can retrieve relevant documents without requiring\nspecific training. This model is meant to work with any type of NLP model and is able to\ngeneralize across multiple tasks. This method differs from RAG [35] in that it uses a generator\nto produce a hypothetical document based on the input query. This document does not need\nto be factually correct, as it is only used by the retriever (e.g., Contriever), which transforms\nit into a dense embedding vector. This embedding represents the hypothetical document in a\nhigh-dimensional vector space.\nThe retriever is then used to search the corpus for real documents that are similar in the\nembedding space, this similarity is measured using inner product similarity between the hy-\npothetical document embeddings and the real document embeddings. Then the most similar\ndocument is fed into the model which also receives the input query, it then generates the output\nfor the query based on the retrieved document.",
    "caption": "Figure 9: An illustration of the HyDE model.[40]",
    "imagePath": "/images/figure_9_an_illustration_of_the_hyde_model__40_.png"
  },
  {
    "pageNumber": 30,
    "imageNumber": 10,
    "type": "figure",
    "folderName": "Figure 10 Comparison RAG on the top and CAG on the",
    "precedingText": "Table 10: Results for web search on DL19/20. Best performing w/o relevance and overall\nsystem(s) are marked bold. DPR, ANCE and ContrieverFT are in-domain supervised models\nthat are finetuned on MS MARCO training data. [40]\n38\n5.3.2\nCache Augmented Generation\nRAG is the most used approach for enhancing language models but as discussed previously\nit has two main drawbacks, specifically the retrieval latency and the potential errors in document\nselection [43]. LLMs have been increasing their context size over the years and the CAG [43]\napproach proposes a method that uses this increased context size to reduce the model latency\nand potential errors that could occur on RAG systems.",
    "caption": "Figure 10: Comparison RAG on the top and CAG on the botom[43]",
    "imagePath": "/images/figure_10_comparison_rag_on_the_top_and_cag_on_the.png"
  },
  {
    "pageNumber": 31,
    "imageNumber": 11,
    "type": "figure",
    "folderName": "Figure 11 Comparison between performance and costs",
    "precedingText": "This approach works by enabling retrieval-free knowledge integration. This is done by\npreloading external knowledge sources, such as a collection of documents D = {d1, d2, ..., dn}\nand precomputing these documents key-value (KV) cache CKV , this addresses some of the\ncomputational challenges and inefficiencies inherent to real-time retrieval on RAG systems.\nThis approach is divided into three main steps:\nExternal Knowledge Preloading, represents the adaptation and preprocessing of the collec-\ntion of documents D that are relevant to the target application, this adapts them to fit within the\nmodel’s context window. This is done by the LLM M, with parameters 0, and processed the\ndocuments D, thus transforming it into a precomputed KV cache:\nCKV = KV −ENCODE(D)\n[43]\nThis KV cache which contains the inference state of the LLM, is stored on disk or in memory\nfor future use. This implementation brings some of the computational cost down because the\ncost of processing D is incurred only once even if used in multiple subsequent queries.\nDuring this stage the precomputed KV cache CKV is loaded alongside the user’s query Q,\n39\nthen the LLM uses this cached context to generate the responses:\nR = M(Q|CKV )\n[43]\nBy giving the model the external cached knowledge it eliminates the retrieval latency and re-\nduces the risk of errors or omissions that comes with dynamic retrieval. The prompt P =\nConcat(D, Q) ensures a unified understanding of the user query and the external knowledge.\nCache Reset is the part of the system is responsible for maintaining performance across\nmultiple inference sessions. Since the KV cache grows in an append-only manner sequentially\nstoring new tokens t1, t, 2, ..., tk the context may eventually need to be freed. This is achieved\nby truncating the oldest tokens to prevent the cache from exceeding memory limits.\nCreset\nKV = Truncate(CKV , t1, t2, . . . , tk)\n[43]\nThis ensures that the re-initialization is fast without the need to reload it from disk, ensuring a\nconstant speed and responsiveness.\n5.3.3\nHybrid approaches\nThere are two main Hybrid approaches focusing on different implementations RAGCache\n[44] and Self-Route [45] though these can’t be compared directly in terms of their implemen-\ntation since RAGCache is a system-level optimization for RAG and Self-Route is an approach\nthat selects the optimal way to give context to the model.\nStarting with Self-Route, this method combines the benefits of Retrieval-Augmented Gen-\neration (RAG) notably its proven effectiveness and efficiency in leveraging external knowledge\nwith the capabilities of recent long-context (LC) models, which can directly process and un-\nderstand extended contexts. Models like Gemini 1.5 Pro [46] is able to achieve near-perfect\nrecall of up to 1M tokens and maintains this recall performance of up to 10M tokens. If this\ntrend of bigger and bigger context sizes continues this method could be a big improvement over\ntraditional RAG implementations.\nAlthough one problem with using long-context (LC) models is the increased computational\ncost, their performance can sometimes exceed that of RAG implementations. However, RAG\nis significantly more efficient, with its cost estimated to be around 20% of that required by LC\nmodels.\n40",
    "caption": "Figure 11: Comparison between performance and costs on multiple models using LC, RAG and\nSelf-Route[45]",
    "imagePath": "/images/figure_11_comparison_between_performance_and_costs.png"
  },
  {
    "pageNumber": 33,
    "imageNumber": 12,
    "type": "figure",
    "folderName": "Figure 12 Distribution of the difference of predic",
    "precedingText": "As seen in Figure 11 the performance is maintained if not improved on some models but the\ncost on most cases is less than half. This is due to the nature of the approach, which combines\nthe strengths of both methods. When RAG can be applied, it offers reduced computational costs\nwhile maintaining most of the performance. However, despite the performance gap, there is a\nhigh degree of overlap in the predictions made by both methods.",
    "caption": "Figure 12: Distribution of the difference of prediction scores between RAG and LC[45]",
    "imagePath": "/images/figure_12_distribution_of_the_difference_of_predic.png"
  },
  {
    "pageNumber": 33,
    "imageNumber": 12,
    "type": "page",
    "folderName": "Page_33_Image_12",
    "precedingText": "With that it also achieved efficiency gains compared to OBQ which achieved O(drow · d3\ncol)\ncomparing it to OPTQ which achieves O(max{drow · d2\ncol, d3\ncol}), reducing it by a factor of\n{drow, dcol}. For larger models this can be proven to be more efficient into several orders of\nmagnitude. See Table 3 for details on runtime analysis.\nThe second step involves the use of lazy batch updates, which were introduced to improve\nupon the original Optimal Brain Quantization (OBQ) method. In the original approach, weights\nwere iteratively quantized, requiring updates to all elements of a potentially large matrix while\n22\nParameter\nValue\ndrow\n10000\ndcol\n100\nRuntime Type\nCalculation\nOBQ Runtime\nO(10000 · 1003) = O(10, 000 · 1, 000, 000) = O(1010)\nMax Term: O(max{10000 · 1002, 1003})\nCalculations: 10000 · 1002 = 10, 000 · 10, 000 = 108\nOPTQ Runtime\n1003 = 106\nResult: O(max{108, 106}) = O(108)\nTable 3: Runtime Analysis\nusing only a few FLOPs per entry[21]. This means that the GPU usage was limited by the\nspeed of the memory bandwidth. Lazy Batch-Updates addressed this issue by grouping updates\nacross B × B blocks of the inverse Hessian matrix H−1, with B being typically set to 128\ncolumns at a time. These blocks are processed and after that, they are used to apply updates to\nthe matrix. Thus avoiding the need for frequent recalculations throughout the matrix, cutting\non memory-bound operations.\nThe final modification was the Cholesky Reformulation that was used to improve on two\nkey issues, numerical inaccuracies and error accumulation, the numerical inaccuracies occur as\nmodel sizes increase beyond a few billion parameters, these can lead to instability. This hap-\npens because the matrix H−1\nF\nbecomes indefinite during iterative updates causing erratic weight\nupdates, the error accumulation is caused by the compounding numerical errors of the matrix\ninversion. Though previously they used dampening techniques like adding a small constant λ\n(which was normally always 1% of the average diagonal value) to the diagonal elements of H.\nHowever, this proved to be inefficient on larger models, but by combining Cholesky decom-\nposition, precomputation of necessary rows, and dampening prevents the H−1\nF\nfrom becoming\nindefinite mitigating the accumulation of numerical errors, and makes the algorithm suitable\nfor large models. A deeper dive into some of these methods are described in the subsequent\nsections.\n5.2.2\nWeight-Only Quantization\nThe Weight-Only Quantization focuses on quantizing only the weights of the LLM and not\nthe activations, this reduces the model size and memory transfer time. However, this doesn’t\nbenefit from hardware-accerlerated low-bit operations.[16] The most used quantization method\n23\nis rounding to nearest-nember (RTN).[22], this works by quantizing a tensor x into k-bits.\nQ[x] = s × clamp\n\u0010x\ns, lmin, lmax\n\u0011\n[22]\nHere the s is the quantization scale, lmin and lmax are the low and upper bound clipping, which\nis then rounded to the nearest number ⌊·⌋. Usually setting lmin = −2k−1 + 1 and lmax = 2k −1\nand set s to be the maximum absolute value in x. There are two main ways to find the best\nconfiguration in weight only quantization. The first one is by minimizing the reconstruction\nerror of the weight parameter which is define as\nr(W) := ∥Q[W] −W∥2\n[22]\nOn the function above only the weights are accessed therefore it’s a data-free process. However\nrecent studies ([19], [23]) propose the usage of output error as compensation.\ne(W) =\nX\nX∈D\n∥Q[W]X −WX∥2\n[22]\nD corresponds to the calibration set sampled form of the original training data, for optimization.\nBy regularizing the model with its training data, more promising results are achieved com-\npared to the reconstruction-based method. The data requirements of these 2 methods have a two\nbig draw backs, the first one being that there’s a requirement of having the original training data\nof the model since most quantizations are done by others than the creators off the model makes\nit hard to find exactly which data was used to train the model. The second problem is that using\nthe same data again can jeopardize the ability of generalization of the model due to the model\nover-fitting to the calibration set. For this two reasons it is clearly very important to achieve a\nData-free quantization.\n5.2.3\nNon Uniform Weight Quantization\nMost of the typical quantization methods handle the weights differently from this approach.\nThis method quantizes weights differently depending on their importance to the LLM. Not all\nweights are of equal importance to the performance of a model, and so they should not be\ntreated similarly, which is the case with techniques such as EasyQuant.\nEasyQuant[22] proposes the usage of the reconstruction error as the regulation metric since\nthis can be used to optimize the quantized model indirectly improving the generalization ability\nof the model. According to [22] the performance gap of the quantized model (INT4) and the full\nprecision model is due to two main factors. The first being that normally the quantization range\nis picked as the maximum absolute value of the weight thus inducing a large reconstruction error\nfor low-bits quantization. The latter refers to the fact that the 0.1% of weights corresponds to\noutliers, although representing a small percentage of the total, have a significant impact on the\n24\nmodel’s performance.Keeping this in mind, if we try to define the outliers using the condition:\n|Wi,j −mean(W)| ≥n · var(W)\n[22]\nFor any weight W, where Wij is the (i, j)-th weight and (n) representing the threshold for\nidentifying the outliers, we can classify certain weights as outliers [22]. However, the challenge\nis that simply detecting the outliers and avoiding their quantization is not sufficient to achieve\ngood model performance. Furthermore, if the percentage of outliers becomes too large, the\noverhead introduced by the dequantization kernel increases, which can lead to a reduction in\noverall throughput.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_33_image_12.png"
  },
  {
    "pageNumber": 33,
    "imageNumber": 13,
    "type": "page",
    "folderName": "Page_33_Image_13",
    "precedingText": "Table 4: PPL results on Wikitext2 of BLOOM-7B with and without outlier isolation. [22]\nEasyQuant also experimented with an ablation study focusing on three aspects, the outlier\ninfluence, outlier distribution, and the Quantization Range. The ablation study began by pre-\nserving 10% of the weights in FP16. This resulted in an 8% increase in perplexity, compared to\nonly a 1% increase achieved with EasyQuant. These findings suggest that simply isolating the\noutliers was not sufficient to maintain the expected perplexity levels. To check the outlier influ-\nence on EasyQUant, outlier isolation is key however this can only impose an indirect influence\non the model accuracy. The phenomenon found is the outliers behave like a gating mechanism\nmeaning that without the outlier isolation, the model performance deteriorates significantly with\nsmaller reconstruction error, and with outliers in FP16 the model shows continuous improve-\nment decreasing the perplexity with smaller reconstruction error Table 4. Another study was\ndone to understand how much influence outliers with big weight magnitude and small weight\nmagnitude have on the model performance, this was done by pruning 1% of the values (accord-\ning to their magnitude) in the weights into 0 and see the perplexity results.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_33_image_13.png"
  },
  {
    "pageNumber": 34,
    "imageNumber": 14,
    "type": "page",
    "folderName": "Page_34_Image_14",
    "precedingText": "Table 5: PPL results after pruning 1% weight with different magnitude [22]\nBased on Table 5 [22] shows that the largest magnitude outliers imposed the same influence\non the model performance as the normal values. This suggests that outliers exert a similar direct\n25\ninfluence on model accuracy as regular weights, thereby indicating that isolating outliers has an\nimportant indirect impact on the overall performance of the model.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_34_image_14.png"
  },
  {
    "pageNumber": 34,
    "imageNumber": 15,
    "type": "page",
    "folderName": "Page_34_Image_15",
    "precedingText": "Table 6: Outlier fraction distribution in different modules in BLOOM-7B under 3-sigma thresh-\nold [22]",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_34_image_15.png"
  },
  {
    "pageNumber": 35,
    "imageNumber": 13,
    "type": "figure",
    "folderName": "Figure 13 Trade-off curves between (a) model perfo",
    "precedingText": "Figure 12 shows the differences between RAG prediction scores SRAG and LC prediction\nscores SLC these are not just similar, in 63% of queries the model predictions are exactly identi-\ncal, and for 70% of queries, the score difference is less than 10%. This is also true for incorrect\nanswers when looking at the red color which corresponds to an accuracy of 0 it can be seen that\nRAG and LC make similar errors as well.\nSelf-Route uses the LLM itself to route queries based on self-reflection, under the idea\nthat LLMs are well-calibrated in predicting whether a query is answerable given the provided\ncontext, this is done using a two-step approach RAG-and-Route and long-context prediction.\n41\nRAG-and-Route works by providing the original query along with the retrieved text chunks\nto the LLM, prompting it to assess whether the query is answerable based on the given context.\nIf the model determines that the query can be answered, it proceeds to generate a response.\nHowever, if it deems the query unanswerable, it is instructed to return the phrase ”unanswer-\nable” as per the prompt: ”Write ’unanswerable’ if the query cannot be answered based on the\nprovided text.” In such cases, a fallback approach is then triggered.\nThis approach consists of giving the LLM the full LC which consists of all documents able\nto fit the context , although this is not explained in the paper, it gives some hints that this must be\nthe case. This as seen on Figure11 giving a better result although with a higher cost. This trade-\noff is most cost-efficient when k = 5, meaning the number of retrieved documents is five. This\nis because, as k increases, the cost of RAG also increases, but so does the number of queries\nthat can be successfully routed. Ultimately, the efficiency depends on the specific task being\nevaluated. For instance, in extractive question answering tasks where multi-hop reasoning is\nnot required a lower k (e.g., k = 1) may result in lower computational cost. Conversely, tasks\nthat require deeper reasoning may benefit from a higher k. Therefore, the optimal value of k is\ndependent on both the nature of the task and the level of performance required.",
    "caption": "Figure 13: Trade-off curves between (a) model performance and (b) token percentage as a\nfunction of k.[45]",
    "imagePath": "/images/figure_13_trade_off_curves_between__a__model_perfo.png"
  },
  {
    "pageNumber": 37,
    "imageNumber": 14,
    "type": "figure",
    "folderName": "Figure 14 RAGCache Overview[44]",
    "precedingText": "The other hybrid approach RAGCache works by caching the key-value tensors of retrieved\ndocuments across multiple requests, this is done to try and minimize redundant computation for\nefficiency gains.\n42",
    "caption": "Figure 14: RAGCache Overview[44]",
    "imagePath": "/images/figure_14_ragcache_overview_44_.png"
  },
  {
    "pageNumber": 37,
    "imageNumber": 18,
    "type": "page",
    "folderName": "Page_37_Image_18",
    "precedingText": "The main idea is to rank the columns by their salience in descending order and use an\noptimized search algorithm to minimize quantization error. This process determines the optimal\nnumber of columns to include in the salient group. Based on this formula:\nWb = α · sign(Wf),\n[24]\n27\nwhere Wb corresponds to the binarized output, α denotes the scaling factor and Wf denotes the\nweights at full precision (FP16). This was then used to define the objective of the binarization\nquantization, used in this equation:\narg min\nα,B ∥W −αB∥2,\n[24]\n(1)\nwhere the B is the number of selected columns, α and B can simply be solved as α =\n∥W∥ℓ1\nn×k\nand B = sign(W). Then the optimization function to select salient columns is defined as:\narg min\nWuns ∥W −(αsal sign(Wsal) ∪αuns sign(Wuns))∥2 ,\n[24]\nwhere Wsal denotes the column-wise combination of the original weight and Wuns is the\nleft non-salient part. W can be determined by Wsal ∪Wuns so the only variable parameter is the\nnumber of rows in Wsal.\nBinary Residual approximation is a technique use to address the challenge of preserving\nsalient weights which are limited in quantity, but exhibit significant variance when aggregated.\nIf these weights are preserved at their original formats FP16 or INT8 it increased the aver-\nage weight bit-width, reducing the compression beneficts of binarization. However traditional\nmethods of binarization result in a substantial quantization errors. Contrary to the comprehen-\nsive high-order quantization [26] which also applies quantization to the entire weight matrix,\nthe approach described in [24] uses a residual approximation method. This approach focuses on\nbinarizing only a subset of salient weights minimizing the error through a second-order aprox-\nimation. This method grants the precision of salient weights while simultaneously decreasing\nbit-width overhead. As shown in Figure 3 this approach incorporates a recursive computation\nstrategy for weight binarization compensation, applying a subsequent binarization process to\nthe residuals remaining after the initial binary process. Based on the equation 1 they redesigned\nthe residual approximation optimization specifically for salient weights by implementing:\n\n\n\nα∗\no, B∗\no = arg minαo,Bo ∥W −αoBo∥2,\nα∗\nr, B∗\nr = arg minαr,Br ∥(W −α∗\noB∗\no) −αrBr∥2\n[24]\n\n\nthe Bo represents the original binary tensor, while Br denotes the residual binarized matrix\nwith the same size as Bo. Efficiently solving it for the two binarized optimization objectives\nresults on this approximation:\nW ≈α∗\noB∗\no + α∗\nr B∗\nr\n[24]\nTo prove that the residual approach of the equation above has a lower quantization error than\n28\nthe direct one of 1. The residual binarization error was defined by E.\nErb = ∥W −α∗\noB∗\no −α∗\nr B∗\nr ∥2\n2\n[24]\nThe original binarized quantization error is calculated as ∥W −α∗\noB∗\no∥2\n2, and from the second\nsub-equation of equation 5.2.3 it’s determined that loss Erb ≤∥W −α∗\noB∗\no∥2. Thus showing\nthat the method of residual approximation proved to be able to further reduce the binary quanti-\nzation error of salient weights with ultra-low bit-width storage compared to retaining the salient\nweights at their full precision or even INT8.\nThe performance of a LLaMA model with this super low quantization method is still impres-\nsive, only losing about 45% of the performance of the original model on a suit of benchmarks\nconsisting of PIQA, ARC-e, ARC-c, HellaSwag and WinoGrand, as depicted on Figure 8.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_37_image_18.png"
  },
  {
    "pageNumber": 39,
    "imageNumber": 15,
    "type": "figure",
    "folderName": "Figure 15 Knowledge Tree[44]",
    "precedingText": "Cache Structure and Replacement Policy operate differently from traditional cache systems\nthat cache individual objects. Instead, this method caches the key-value tensor of the retrieved\ndocuments, which are sensitive to the order in which they are referenced. For instance consider\ntwo document sequences [D1, D3] with key-value tensors KV and [D2, D3] with KV ′ respec-\ntively. Although KV [1] and KV ′[1] both contain D3, their value differs. This occurs because\nthe key-value tensor of a given token being generated based on the preceding tokens, thus un-\nderscoring the order-dependence of key-value tensors. To aid retrieval speed while maintaining\ndocument order, RAGCache structures the document’s key-value tensors with a knowledge tree\n15. The knowledge tree assigns each document to a node that refers to the memory addresses\nof the document’s key-value tensors. Similarly to vLLM [47], RAGCache also stores key-value\ntensors in non-contiguous memory blocks, allowing the KV cache to be reused. The root of\nthe tree, S, corresponds to the shared system prompt. A path from the root to any node rep-\nresents a unique sequence of documents, thus allowing RAGCache to handle multiple requests\nsimultaneously by leveraging overlapping paths.\nRAGCache retrieves tensors by performing prefix matching along these paths. If a subse-\nquent document in a sequence cannot be found among the child nodes, the traversal is termi-\nnated, and the longest identified document sequence is returned. This method ensures efficiency\nwith a time complexity of O(h), where h is the height of the tree.",
    "caption": "Figure 15: Knowledge Tree[44]",
    "imagePath": "/images/figure_15_knowledge_tree_44_.png"
  },
  {
    "pageNumber": 41,
    "imageNumber": 16,
    "type": "figure",
    "folderName": "Figure 16 Cost estimation PGDSF[44]",
    "precedingText": "Prefix-aware Greedy-Dual-Size-Frequency (PGDSF) replacement policy is the name for the\n43\nnode placement optimizer. The knowledge tree decides each node’s placement within a hier-\narchical cache. For example, nodes that are constantly accessed are ideally stored in GPU\nmemory, while those that are accessed less often are stored in slower host memory or are freed\ncompletely. The node placement optimization occurs when RAGCache uses a PGDSF replace-\nment policy that evaluates each node based on the access frequency, size, and access cost, due\nto its limited storing capacity the priority is defined by:\nPriority = Clock + Frequency × Cost\nSize\n[44]\n(6)\nNodes that have a lower priority get freed first while the opposite is also true. The Clock\ntracks node access frequency, but to adapt to the cache hierarchy, there are two separate logical\nclocks: one for the GPU and another for the host memory. The Clock starts at zero and updates\nevery eviction, when a document is retrieved its clock is set and its priority gets adjusted this\nimposes that nodes with older clocks meaning less recent use, receive lower priorities.\nClock = max\nn∈E Priority(n)\n[44]\nFrequency in Equation 6 represents the total retrieval count for a document within a time\nframe, this count is reset upon system start or cache clearance. Priority is directly linked to\nthe frequency so the more frequently a document is accessed the higher the priority. Size is the\nnumber of tokens in the given document post-tokenization, thus directly linked to the memory\nrequired for its key-value tensors. The Cost is defined as the time taken to compute a docu-\nment’s key-value tensors, this can vary depending on GPU performance as well as document\nsize and the sequence of preceding documents.",
    "caption": "Figure 16: Cost estimation PGDSF[44]",
    "imagePath": "/images/figure_16_cost_estimation_pgdsf_44_.png"
  },
  {
    "pageNumber": 43,
    "imageNumber": 17,
    "type": "figure",
    "folderName": "Figure 17 Cache aware Reordering[44]",
    "precedingText": "Prefix awareness for RAG is achieved by the PGDSF method through two primary com-\nponents, cost estimation and node placement. Accurately determining the computational cost\nfor RAG operations is challenging due to the complex dynamics of LLM generation. Figure\n16 illustrates this challenge by showing the cost differences for an identical request, denoted as\n[S, D1, D2, Q], under different caching conditions.\nEstimating the cost contribution of D2 is imprecise because the marginal cost depends heav-\n44\nily on the cache prefix. For instance, if the prefix [S, D1] is already cached, the subsequent\ncomputational cost includes the generation of key-value tensors for both D1 and Q. Making\nit extremely difficult to isolate the cost attributed solely to D2. To address this issue, PGDSF\nreplaces the Cost/Size term in Formula 6 with the following Equation:\nm\nX\nCost Size = 1\nCosti\nNewSizei\n[44]\n(7)\nm\ni=1\nIn Equation 7, m represents the number of requests that access a document not currently\nin cache. The term Costi/NewSizei denotes the compute time per non-cached token for the\ni-th request. This approach effectively amortizes the computational cost across all non-cached\ntokens, thereby incorporating the document’s size into the priority calculation. The cost, Costi\n, is determined through an offline profiling process where RAGCache measures the LLM prefill\ntime for multiple combinations of cached and non cached token lengths. Subsequently, it em-\nploys bilinear interpolation to estimate the cost for any given request at runtime. Each document\nretrieval event triggers an update to the corresponding node’s frequency, its cost estimation, and\nthe clock mechanism within the knowledge tree. Furthermore, if a retrieved document is not\nalready in the cache, a new node is created for it in the tree.\nThe management of the node placement on the GPU, host, or free is performed by the\nPGDSF as seen on Figure 15. The nodes in GPU serve as parent nodes to those in host memory,\nestablishing a hierarchical structure. RAGCache also manages node eviction across these two\nsegments for efficiency, which is especially true when GPU memory is full. When this occurs,\nRAGCache swaps the lowest-priority node in the leaf nodes to the host memory; this process\nalso occurs on the host memory when it is full, though in that case, it is an eviction. This\nstrategy takes into account the Knowledge tree hierarchical partitioning, which is one key point\nto align with memory sensitivity and prefix sensitivity in LLM generation. Due to the node\nneeding its parent node for key-value tensor calculation, the required placement of the parent\nnode is prioritized this is so rapid retrieval can be achieved.\nBecause of PCIe limitations when connecting the GPU with the host memory in comparison\nwith GPU HBM, RAGCache adopts a swap-only-once strategy depicted in Figure 15, where\nyou can see that the key-value tensors of a node are swapped out to the host memory only for\nthe first eviction. The host memory is responsible for keeping the key-value tensors until the\nnode is fully evicted from the entire cache. For any subsequent evictions in GPU memory,\nRAGCache directly frees the node node without copying any data Due to the size of the host\nmemory being two orders of magnitude larger than the GPU memory, keeping one copy of the\nkey-value tensors in the host memory is acceptable.\n45",
    "caption": "Figure 17: Cache aware Reordering[44]",
    "imagePath": "/images/figure_17_cache_aware_reordering_44_.png"
  },
  {
    "pageNumber": 45,
    "imageNumber": 18,
    "type": "figure",
    "folderName": "Figure 18 Speculative Pipelining[44]",
    "precedingText": "Cache hit rate is vital for RAG Cache’s cache efficiency, but when paired with the unpre-\ndictability of the arrival pattern in user requests, this results in substantial cache trashing.\nRequests that refer to the same document may not be issued on the same time frame thus af-\nfecting the cache efficiency. For example given the requests {Qi, i%2 == 0} and {Qi, i%2 ==\n1} that target the documents D1 and D2 respectively. When the cache capacity is only one\ndocument, the sequence {Q1, Q2, Q3} causes frequent swapping of the key-value cache of D1\nand D2, making it a zero cache hit rate. But if a bit of attention is paid to rearrange requests\nto {Q1, Q3, Q5, Q2, Q4, Q6, Q6, ...} this achieves a cache hit rate of 66% thus optimizing cache\nutilization. This shows how strategic request ordering can mitigate cache volatility and improve\ncache efficiency. To introduce the cache-aware reordering algorithm, two scenarios were con-\nsidered to show the key insights, the recomputation cost was assumed to be proportional to the\nrecomputation length. The first scenario is shown on Figure 17, (a) where it considers requests\nwith identical recomputation demands but varying cached context lengths with a limit of four\ncached documents. With the initial order of Q1, Q2 the system must clear Q2’s cache space to\nfit Q1’s computation, then reallocate memory for Q1’s processing effectively uses Q1’s cache\nwhile discarding Q2’s, resulting in a computational cost of 2 + 1 + 2 = 5. On the other hand,\nif the order was given as Q2, Q1 this would result in a usage of Q2’s cache but discarding Q1’s,\nwhich would increase computation to six due to 2+2+2 = 6. This is why cache-aware reorder-\ning advocates to prioritize requests with larger cached context thus improving cache efficiency\nas this brings larger benefits. In the second scenario (b), the aim was to examine requests with\nsimilar cached context lengths but varying recomputation demands, with a cache capacity of\nfive documents. On a sequence {Q1, Q2}, the system must clear Q′\n2s cache to allocate space for\nQ′\n1s computation, given only one available memory slot. This makes it necessary to recompute\nQ2 entirely, which results in a cost of 2 + 2 + 1 = 5. On the other hand the sequence {Q2, Q1}\nallows for direct computation of Q2, due to adequate cache availability. It also reduces the total\ncomputation cost to 2 + 1 = 3, thus the reason why cache-aware reordering is beneficial when\nit prioritizes requests with shorter recomputation segments, this way results in a minimization\nof the adverse side effects on cache efficiency. RAGCache uses a priority queue for manag-\ning incoming requests, this prioritizes the incoming requests based on their impact on cache\nperformance, the priority metric is defined by:\nOrderPriority =\nCached Length\nComputation Length\n[44]\n(8)\n46\nEquation 8, directly prioritizes the requests that will probably lead to enhanced cache ef-\nficiency. This is directly linked to the increase in the cache hit rate and the decreased total\ncomputation time of RAGCache. Model performance and resource usage are also improved\nthanks to this implementation. To avoid possible starvation when requests don’t align with the\ncached documents RAGCache sets a window for each request to ensure that all requests are\nprocessed in a timely manner.\nOn an LLM enhanced with RAG, the key performance bottleneck is usually the LLM gen-\neration, however, if the vector database grows to a larger scale or a higher accuracy is needed in\nthe retrieval, this may cause the retrieval step to incur a substantial latency.\nTo control the impact of retrieval latency, RAGCache employs dynamic speculative pipelin-\ning to overlap knowledge retrieval and LLM inference, and one thing that can occur is that the\nvector search may produce results earlier in the retrieval step, which can be used by the LLM\nfor speculative generation ahead of time. This works by a vector search maintaining a queue\nof top-k candidate documents, which are ranked based on their similarity to the request. Dur-\ning the retrieval process the top-k documents are being constantly updated this is done so that\ndocuments that still being discovered, might come with greater similarity and so they get in-\nserted into the top-k. What could also occur is that the final documents may emerge early in the\nretrieval step [48]. Based on that RAGCache introduced a speculative pipelining strategy that\nsplits a request’s retrieval process into several stages. In each stage RAGCache ticks the vector\ndatabase to send the possible document to the LLM for a speculative generation, if the received\ndocument is changed then the LLM will start a new speculative generation and terminate the\nprevious one, if that doesn’t happen then the LLM engine just continues with the generation.\nWhen the top-k documents are finalized and there are no more changes to the top-k these are\nsent by RAGCache to the LLM engine and if they match with the ones previously received the\nengine simply returns the latest speculative generation. Otherwise, the LLM performs a new\ngeneration with the new top-k documents.",
    "caption": "Figure 18: Speculative Pipelining[44]",
    "imagePath": "/images/figure_18_speculative_pipelining_44_.png"
  },
  {
    "pageNumber": 46,
    "imageNumber": 25,
    "type": "page",
    "folderName": "Page_46_Image_25",
    "precedingText": "The main issue addressed by [40] is the dependence on a separate query encoder required\n36\nby RAG [35] systems. This is because dense retrievers compute similarity between the query\nand documents using inner product similarity, which necessitates a dedicated query encoder.\nFirstly it uses two encoder encqencd that maps the query q and the document d into d di-\nmension vectors vq, vd, whose inner product is used as the similarity measurement.\nsim(q, d) = ⟨encq(q), encd(d)⟩= ⟨vq, vd⟩\n[40]\nThis is where zero-shot dense retrieval problems lie, it requires learning two embedding func-\ntions one for the query and the other for the document, these need to align into the same em-\nbedding space where the inner product can capture the document’s relevance. HyDE solves this\nproblem by performing a search in the document-only embedding space that captures the doc-\nument’s similarity. This method can be easily learned using unsupervised contrastive learning\n[41]. They set the encd directly as the contrastive encoder enccon as follows:\nf = encd = enccon\n[40]\n(4)\nThis unsupervised contrastive encoder is be shared by all incoming document corpus. The\nfunction 4 is also denoted as f.\nvd = f(d)\n∀d ∈D1 ∪D2 ∪· · · ∪DL\n[40]\nTo build the query vector they use an instruction following LLM, in this case, text-davinci-\n003 from OpenAi’s GPT-3 series [42], this was specifically picked due to its generalization\nability, they call it InstructLM so it’s easy to represent. It then takes the query q and a textual\ninstruction INST and follows them to perform the task specified in INST, like so:\nTo build the query vector, they use an instruction-following LLM in this case, text-davinci-\n003 from OpenAI’s GPT-3 series[42]. This model was specifically chosen for its strong gener-\nalization capabilities and is referred to as InstructLM for ease of representation. It takes the\nquery q and a textual instruction INST, and follows the instruction to perform the specified\ntask, as shown below:\ng(q, INST) = InstructLM(q, INST)\n[40]\nThe g can be used to map queries to the hypothetical documents by sampling from g, the INST\nis set to be ”write a paragraph that answers the question”, the generated document isn’t real and\nmay be factually incorrect due to models hallucinations [42]. However, this is not important\nbecause the hypothetical document is used only to capture the relevance pattern. Then the\nrelevance modeling is offloaded to an NLG that has the ability to generalize more easily, nat-\nurally, and more effectively. Another great thing about generating the examples is that this\nalso replaces explicit modeling of relevance scores making it so there’s no need to compute the\n37\nquery-document relevance.\nE[vqij] = E[f(g(qij, INSTi))]\n[40]\n(5)\nThe f corresponds to the document encoder, g defines a probability distribution based on the\nchain rule. In their implementation, they assume that the distribution of vqij is uni-modal,\nimplying that the query is not ambiguous. To estimate Equation 5, they sample N documents\nfrom g,\nh\nˆd1, ˆd2, . . . , ˆdN\ni\n.\nˆvqij = 1\nX\nˆdk∼g(qij,INSTi)\nf(dk)\nN\nN\nX\n= 1\nk=1\nf( ˆdk)\n[40]\nN\nThey also consider the query as a possible hypothesis,\n\" N\nX\n#\nˆvqij =\n1\nN + 1\nk=1\nf( ˆdk) + f(qij)\n[40]\nThe inner product is computed between ˆvqij and the set of all document vectors {f(d)|d ∈Di},\nthen the most similar documents are retrieved. In their implementation, the encoder function\nf acts as a lossy compressor, producing dense vectors in which unnecessary details are filtered\nout. This enables the system to use a generated document even if it lacks factual accuracy to\neffectively search for the correct one.\nAccording to their study, HyDE remains competitive even when compared to fine-tuned\nmodels. Another strong result comes from the web research setting, where the performance\nof HyDE is particularly impressive even when compared to methods that rely on relevance\njudgments. Notably, HyDE achieves these results without requiring such judgments.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_46_image_25.png"
  },
  {
    "pageNumber": 47,
    "imageNumber": 19,
    "type": "figure",
    "folderName": "Figure 19 Optimal speculative pipelining strategy",
    "precedingText": "Figure 18 shows how RAGCache splits the retrieval process into four stages. The top-2\ndocuments in candidate queue are [D1, D3], [D1, D2], [D1, D2] and [D1, D2] in the four stages.\nAfter the first stage is concluded, RAGCache sends [D1, D3] to the the LLM engine for specu-\nlative generation. When stage two is concluded, RAGCache sends [D1, D2] to the LLM engine,\nthe LLM engine is responsible for checking if the [D1, D3] and [D1, D2] are different if that’s\n47\nthe case it terminates the previous speculative generation and starts a new one with the correct\ndocuments. In stage three, the LLM engine receives the exact same documents as for stage two,\nso it continues with the previously started speculative generation. In the last stage, RAGCache\nsends the final top-2 documents to the engine which are still the exact same as the ones in stage\ntwo so there is no change to the speculative generation which is directly returned by the LLM\nengine as the result.\nThe speculative pipelining allows RAGCache to overlap the retrieval and generation steps,\nand this greatly improves the end-to-end latency of RAG systems. But this can introduce a\nlot of extra steps in the computation of the engine response. This can be seen above Figure\n18, some speculative generations are incorrect and need to be recalculated. This can lead to\nperformance degradation under high system loads, but to solve this RAGCache dynamically\nenables speculative pipelining based on the system load. As an example, they assumed that the\nvector search and the LLM both serve only one request at a time. This vector search produces\ncandidate retrieval results at the end of each stage with a fixed time interval d. Since the batch\nsize was set to only one, they could terminate any incorrect speculative generation requests.",
    "caption": "Figure 19: Optimal speculative pipelining strategy [44]",
    "imagePath": "/images/figure_19_optimal_speculative_pipelining_strategy.png"
  },
  {
    "pageNumber": 49,
    "imageNumber": 20,
    "type": "figure",
    "folderName": "Figure 20 The SKR Pipeline and its component inter",
    "precedingText": "RAGCache assumes that the LLM engine can schedule requests in the queue in any order,\nbut it processes speculative generation requests for a single request sequentially. Figure 19\nillustrates the optimal speculative pipelining strategy under this setting.\n5.4\nChallenges and Applications of Quantization in RAG\n5.5\nRetrieval Methods to enhance HyDE\nDue to the approach taken by RAG and HyDE on how they retrieve documents, these meth-\nods could be adapted or specifically chosen based on their ability in certain tasks. Retrievers\nare the basis for content that was retrieved from an external document corpus to enhance the\nLLM output, as well as provide grounds for the generated information on accurate documents.\nA more in-depth research will be done about some of these retrievers.\n48\n5.5.1\nContriever\nThe original implementation of HyDE uses the Contriever model as its retriever. This ap-\nproach works on the basis of contrastive learning, which is based on the fact that every document\nis, in some way, unique. According to [49], this is the only available information in the absence\nof manual supervision. A contrastive loss is used to learn by discriminating between documents.\nThis loss compares either a positive loss when they are the same document or negative when\nit’s from different documents. The formula responsible for this is:\nL(q, k+) = −\nexp (s(q, k+)/τ)\nτ\n\u0011′\n[49]\n(9)\nexp\n\u0010\ns(q,k+)\nτ\n\u0011\n+ PK\ni=1 exp\n\u0010\ns(q,ki)\nIn Equation 9, q corresponds to the given query, which has an associated positive document\nk+, and a pool of negative documents(ki)i=1..k, τ is the temperature parameter used to adjust\nthe sensitivity of the Contriver. This function’s construction encourages positive pairs to have\nhigh scores and negative pairs to have low scores. One crucial piece of this method is how to\nbuild positive pairs from a single input. This could be done in two main ways: the Inverse Cloze\nTask or Independent cropping.\nThe usage of the Inverse Cloze Task is a data augmentation strategy that generates two\nmutually exclusive views of a document. This approach was first described in [50]. The first\nview is obtained by randomly sampling a span of tokens from a segment of text, and the second\nview is obtained by using the complement of the span. This is done by in a given sequence of\ntext (w1, ..., wn), ICT samples a span (wa, ..., wb), where 1 ≤a ≤b ≤n, and then uses the\ntokens of the span as the query and the complement (w1, ..., wa−1, wb+1, ..., wn) as the key.\nThe usage of the Independent is critical for matching the query with the document directly.\nThis method is commonly used on images where multiple views are generated independently\nby cropping the input. Since this implementation is used for text it is done by sampling a span\nof tokens. Because of the importance of the positive pairs this strategy samples independently\ntwo spans from a document to form the needed pair. Contrary to the inverse Cloze task in the\ncropping stage both views of the example correspond to the same contiguous subsequence of the\noriginal data. Another difference is that between cropping and ICT is that independent random\ncropping is symmetric meaning both of the queries and documents follow the same distribution.\nThis also causes overlap between the two views of the data, this being one of the reasons that\nencourages the network to learn exact matches between query and document. This works very\nsimilar to how lexical matching methods function, BM25 being a great example of this. So you\ncould either fix the length of the span for the query and key or sample them both.\nA big part of contrastive learning is how the system handles negative pairs this includes\nsampling a large set of negatives. This was tested by [49] using two methods, in-batch negative\nsampling and MoCo.\nThe first approach is to generate the negatives by using the other samples from the same\n49\nbatch. For example, each item in the batch is transformed twice to generate the positive pairs,\nand the negatives are generated by using the other examples views from the batch, they called\nthese ”in-batch negatives”. In this specific case, the gradient is back-propagated through the\nrepresentations of both the queries and the keys. The main downside to this method is the\nrequirement for extremely large batch sizes to work well.\nThe other approach Negative pairs across batches tries to solve the problem by storing the\nrepresentations from previous batches in a queue and using these as negative examples in the\nloss calculation. This makes it possible to have a smaller batch size but may slightly change\nthe loss by making it asymmetric between the queries. This being the view generated from\nthe elements of the current batch, and the keys, which are the elements already stored in the\nqueue. This occurs as the gradient is only back-propagated through the queries, leaving the\nrepresentation of the keys fixed. This is caused by the features being already stored in the queue\nfrom prior batches coming from past interactions with the network. A problem occurs when the\nnetwork is rapidly evolving during training can cause the performance to drop.\nInstead, they used the approach called MoCo [51] which generates representation keys from\na second network that is updated more slowly, the two networks are as follows one is responsible\nfor the keys, parametrized by 0k, and another network for the query, parametrized by 0q. The\nparameter for the query network gets updated using back-propagation and stochastic gradient\ndescent. This works similarly to when in-batch negatives are used. On the other hand, the\nkey network also called Momentum encoder is only updated from the parameters of the query\nnetwork using an exponential moving average.\n5.6\nSelf-Knowledge Guided Retrieval Augmentation\nThis approach uses the few-shot prompts to make the LLM judge if it knows the answer or\nnot. In the case that the answer isn’t known, SKR [52] proceeds to the retrieval using RAG to\nimprove on the model response.\nTheir method works by three main ways: Collecting Self-Knowledge, Eliciting Self-knowledge,\nand Using Self-Knowledge. These represent the main pipeline for SKR, as shown in Figure 20.",
    "caption": "Figure 20: The SKR Pipeline and its component interactions.[52]",
    "imagePath": "/images/figure_20_the_skr_pipeline_and_its_component_inter.png"
  },
  {
    "pageNumber": 51,
    "imageNumber": 21,
    "type": "figure",
    "folderName": "Figure 21 Direct Prompting [52]",
    "precedingText": "50\n5.6.1\nCollecting Self-Knowledge\nGiven a dataset D with question-answer pairs {qj, aj}|D|\nj=1, the model M is used to generate\nthe answers for each entry qi:\nˆa(M, qi) = M(q1 ◦a1, . . . , qd ◦ad, qi)\n[52]\n(10)\nWhere ◦denotes the concatenation and {qj ◦aj}d\nj=1 are d demonstrations. Equation 10\nrepresents the generated answer with ˆa(M, qi), this also represents the internal knowledge to the\nquestion qi in M. The other approach is to possibly find passages from external resources that\nmay be related with said question qi, these passages can then be used as additional information\nprovided on the model input. This is done per query, using a pretrained retriever represented by\nR to find the related information from the corpus C:\npi = {pi1, pi2, . . . , pik} = R(qi, C),\n[52]\n(11)\nAccording to Equation 11 the top-k retrieved passages for the question qi are represented\nby pi = {pi1, pi2, . . . , pik}. A dense passage retriever is used [36] for R, and C consists of\npassage chunks from Wikipedia. Then they use M again to generate the answer with retrieval\naugmentation:\nˆaR(M, qi) = M(q1 ◦p1 ◦a1, . . . , qd ◦pd ◦ad, qi ◦pi).\n[52]\n(12)\nBased on both answers ˆa(M, qi), ˆaR(M, qi) 12, and the ground-truth answer ai, they can cat-\negorize each question into a positive subset D+ and a negative sub-set D−using the differences\nbetween the results:\n\n\n\nD+,\nif E[ˆa(M, qi)] ≥E[ˆaR(M, qi)];\nqi ∈\nD−,\notherwise,\n[52]\nIn Equation 5.6.1 E is an evaluation metric like accuracy and exact match score, but they\nget discarded if the question qi, answer ˆa(M, qi) and ˆaR(M, qi) are incorrect. They then split\nthe training set into a subset D+ = {q+\ni , . . . , q+\nm} which include questions that M can directly\ngive correct answers to without external knowledge R and the other subset D−= {q−\n1 , . . . , q−\nn }\nwhere the R is needed for more accurate results.\n5.6.2\nEliciting Self-Knowledge of LLMs\nThe four different strategies proposed to detect the self knowledge of the target questions\nare direct prompting, in-context learning, training classifier, and nearest neighbor search. These\nwork by on the first two using the LLM itself and the latter two using smaller nodes purposely\nbuilt.\n51\nDirect Prompting given a question qt, a straight-forward approach to detect wether LLMs\nare capable of solving it is to ask them directly:",
    "caption": "Figure 21: Direct Prompting [52]",
    "imagePath": "/images/figure_21_direct_prompting__52_.png"
  },
  {
    "pageNumber": 53,
    "imageNumber": 22,
    "type": "figure",
    "folderName": "Figure 22 In-Context Learning [52]",
    "precedingText": "On this method the prompt is used in conjunction with ’Do you need additional information\nto answer this question?’ to detect self-knowledge based on the response provided by the LLM.\nThis approach results in direct prompting the model and it may work. Although this doesn’t\nuse any of the collected training questions shown previously. To improve on that 3 different\nstrategies where created.\nIn-Context Learning some questions where selected from D+ and D−as demonstrations to\nshow the self-knowledge of the question qt:",
    "caption": "Figure 22: In-Context Learning [52]",
    "imagePath": "/images/figure_22_in_context_learning__52_.png"
  },
  {
    "pageNumber": 55,
    "imageNumber": 23,
    "type": "figure",
    "folderName": "Figure 23 k-nearest-neighbor to understand model k",
    "precedingText": "52\nIn here answer templates where used, ”No, I don’t need...” or ”Yes, I need...” in demonstra-\ntions based on wether the answer comes from the positive set D+ or the negative one D−.\nThis direct prompting and in-context learning methods can induce self-knowledge of LLMs\nto some extent. But they come with three main drawbacks. First one being that both methods\nrequire designing prompts and calling the LLMs for each new question, making it cumbersome.\nSecond, in-context learning could be also unstable due to contextual bias and sensitivity which\nis difficult to address in closed source LLMs. Third, use of all questions cannot be guaranteed,\ndue to the maximum tokens input of the LLMs. To avoid the above issues smaller models were\nused to help elicit self-knowledge.\nA classifier was trained using D+ and D−, as a two-way classification problem using the\nsamples to train a BERTBase classifier [12]:\nˆyi = softmax(Whcls(qi) + b),\n(13)\nWhere qi ∈D+ ∪D−is a training question, hcls(qi) is the sentence-level representation\nfrom BERTBase, W and b are parameters used by the classification head. The parameters can\nbe optimized to improve the cross-entropy loss between the predicted label distribution ˆyi and\nthe ground-truth label of qi. Latter the training model can also be used to infer the label of new\nquestion qt described in equation 13.\nThe other method also tested was Nearest Neighbor Search this method doesn’t require\ntraining since the inference can be directly done based on the label of the questions through\nk-nearest-neighbor (kNN) search using a pre-trained fixed encoder as showed by Figure 23.\nThe kNN[53] is an algorithm widely used for a range of NLP tasks. This idea comes from\nthe similarity between the semantically embedded space of two questions if these are closely\nrelated then the knowledge needed for the model to answer would also be similar.",
    "caption": "Figure 23: k-nearest-neighbor to understand model knowledge [52]",
    "imagePath": "/images/figure_23_k_nearest_neighbor_to_understand_model_k.png"
  },
  {
    "pageNumber": 57,
    "imageNumber": 24,
    "type": "figure",
    "folderName": "Figure 24 RAGEval System 1 summarizing a schema co",
    "precedingText": "Table 12: LLMs using CoT+, Humans scores on the multiple domains[58]\nThis data set was constructed using an LLM that is prompted to generate the gold facts\nrequired to deduce the correct answer. Subsequently, these facts form the basis of a recursive\nquerying process, where the LLM is used to establish the reasoning that connects them, therefor\nconstructing a reasoning tree. This tree components get then used one by one to generate the\nfinal narrative. This method generates a narrative that is a hard for machines yet solvable by\nhumans refer to Table 12. This is true even when using multiple prompting strategies and\nneurosymbolic approaches like chain of thought plus. The limiting factor discovered by the\nMUSR creators is the limitation that LLMs encounter when generating the deep reasoning trees\nthis greatly limits the narrative complexity.\n5.7.5\nBBH\nBBH or Big-Bench-Hard is the improvement of the original Big-Bench which is a bench-\nmark consisting of 204 tasks from 405 authors across 132 institutions. This extensive and\ndiverse authorship is a significant strength of the benchmark, as it serves to lower any potential\nfor institutional or individual biases that might come from more limited set of contributors.\nThe topics covered by the benchmark are exceptionally diverse, spanning a wide range of\ndisciplines including linguistics, childhood development, mathematics, common-sense reason-\ning, biology, physics, social bias, and software development, among others. The tasks selected\nfrom these domains were specifically chosen because they were considered to be beyond the\ncapabilities of state-of-the-art models at the time of its creation.\nBBH was formed, by curating a specific subset of tasks from the original Big Bench collec-\ntion. The selection process was designed to isolate the most dificult challenges for contemporary\nmodels. The resulting benchmark consists of merely 23 tasks, that were chosen exclusively due\nto the performance being lower for State-of-Art models than those achieved by human raters.\nFrom this group of tasks that proved difficult for machines, only the most demanding were\nselected to form the final BBH set.\nThe new benchmark prompting also differed in prompting since the new approach uses\nChain-of-Thought prompting. With this prompting style all models suffered an improvement\n57\nsome as much as 28.5%. The categories of this dataset are also relevant since they don’t just\nfocus on algorithmic tasks, as they also have natural language understanding, world knowledge,\nand multilingual. The last one being great addition since it transforms this dataset into a mul-\ntilingual one. Though the linguistics part doesn’t affect the score by much due to its size in\nrelation with the whole dataset.\n5.7.6\nIFEval\nIFEval [59] or instruction-following evaluation is used to project the ability of LLMs to\nadhere to verifiable instructions.\nThis evaluation task consists of 25 verifiable instructions, these are divided into seven\ngroups, keywords, language, length constraints, detectable content, combination, case change,\nstart with / end with, and punctuation. The instruction also comes with a description to exem-\nplify what the model is required to do.\nThis metric is designed to evaluate the model’s ability to adhere to the instructions provided\nby the proposed system, as illustrated in Figure 27. The method also accounts for errors in the\nmodel’s text formatting relative to the given instruction. This is accomplished using a flexible\naccuracy metric that tolerates superficial differences, such as formatting variations, provided\nthe core intent of the instruction is fulfilled.\nTo provide a comprehensive assessment, instruction-following is evaluated at two granular-\nities. These being the per-prompt basis and the per-instruction basis. This dual-level analysis\nreveals whether the model can maintain adherence to a sequence of instructions or if it only\nfollows the initial ones successfully.\n5.7.7\nARC\nAI2 Reasoning Challenge [60] consists of a dataset and evaluation framework created with\nthe intuition of assessing and advancing the reasoning capabilities of AI systems. This system is\nspecially designed for assessing the models capabilities at answering challenging grade-school-\nlevel science questions.\nARC contains two different sets, the challenge consists of 2590 queries and the easy con-\ntaining 5197. The hard set is composed with queries that both retrieval-based solutions and\nword co-occurrence fail to solve, on the other hand the easy set is composed of questions that\ndon’t require a lot of reasoning to be answered.\nThe key difference on this data-set comes from the fact that to answer the question on the\nchallenge-set the LLM require deeper reasoning, due to the fact that these can’t be answered\nusing surface-level cues or simple retrieval methods.\nThe paper also talks about how even models that used IR or Pointwise Mutual Information\n(PMI) failed to outperform random guessing on the Challenge set.\n58\n5.7.8\nHellaSwag\nHellaSwag [61] is a test set created to evaluate LLMs ability in commonsense natural lan-\nguage inference (NLI). The task is to choose the most reasonable continuation of a context,\nfrom four possibilities.\nThe test set is made to be simple for human participants with an accuracy of 95.6%, yet\nsimultaneously difficult for state-of-the-art models, with accuracy below 50%.\nThe dataset is an extension of the original SWAG dataset but with the inclusion of Adver-\nsarial Filtering (AF). This is done to increase the difficulty of the task. AF works by contin-\nuously selecting wrong answers generated by adversarial machines, thereby keeping a chal-\nlenging dataset even for state-of-the-art models like BERT. The novelty lies in generating a\n”Goldilocks zone” of text difficulty, where the wrong answers are nonsensical to humans but\nare often misclassified by models.\nHellaSwag consists of 70,000 examples that are gathered from ActivityNet video captions\nand WikiHow text, thus contributing to the diversity of contexts in addition to the length and\ndifficulty of the examples. The dataset also includes zero-shot test classes, in which models are\ntested on unseen domains to estimate their ability to generalize.\nThe assessment is centered on whether models can reason on what is likely the next event\nor employ dataset-specific bias. Results indicate that even the top models, i.e., BERTLarge, fail\nto generalize but instead depend on shallow lexical patterns rather than actual commonsense\nreasoning.\nThis benchmark indicates the weaknesses of existing language models in reasoning and\nunderstanding the world, demonstrating that natural language processing progress demands the\ndevelopment of benchmarks that evolve together with progress in model capabilities.\n5.7.9\nThrutfulQA\nThrutfulQA [62] is a benchmark designed to evaluate the truthfulness of LLMs when an-\nswering questions. This benchmark is constructed with 817 questions across 38 different cate-\ngories, such as health, law, finance, and conspiracies. These questions were specially designed\nto test whether models generate imitative falsehoods, these are answers that mimic common\nhuman misconceptions or misinformation found in the training data, including falsehoods. For\nexample, some models might say that cracking knuckles causes arthritis, even though this is\na false statement. This benchmark also shows a big gap in human to LLM performance, with\nhumans achieving 94% compared to just 58% of the UnifiedQA LLM. This method can be used\nto see if techniques like fine-tuning to prioritize truthfulness over imitation are achieving the\nwanted results. ThrutfulQA is a great tool to stop the spread of misinformation and reduce the\ndeception caused by the usage of LLMs by users that think these models are truthful.\n59\n5.7.10\nWinoGrande\nWinoGrande is an expanded version of the Winograd Schema Challenge, which originally\nconsisted of 273 expert-crafted pronoun resolution problems. These problems are trivial for\nhumans but challenging for machine learning algorithms, as these require commonsense rea-\nsoning rather than reliance on statistical patterns or word association. But with the state-of-art\nmodels achieving near-perfect scores there was a need to improve on the original method so\nWinoGrande was created.\nThis method introduces 44000 problems inspired by the previous method but this time de-\nsigned to be more challenging and also scalable. These new problems were created using crowd-\nsourcing and after-validated to ensure their trivialness to humans while still being difficult for\nstate-of-the-art models.\nThis data set shows a big gap in performance from LLMs to SLMs.\n5.7.11\nGSM8K\nGSM8K [63] is a dataset specially crafted to evaluate the LLM’s ability to perform multi-\nstep mathematical reasoning. This data set consists of 8.5K high-quality grade school math\nword problems, this data set is split with 7.5k on the data set and 1k for the testing. The\nproblems are linguistically diverse and need 2 to 8 steps to solve, these are focused on basic\narithmetic operations like addition, subtraction, multiplication, and division. The solutions to\nthese are provided in natural language to encourage the model’s interpretability and reasoning.\nThis benchmark is used to test a model’s performance on informal reasoning and problem-\nsolving capabilities.\n5.7.12\nMath Lvl5\nThis data set MATH [64] consists of various levels of math problems with five being the\nmost challenging tier. This test is designed to test advanced reasoning and heuristic applications.\nThe problems require a deep understanding of mathematical concepts, creative problem-solving\nstrategies, and the ability to aggregate various techniques to find a solution. The performance\nof LLM models like LLaMA 3.1 8B achieves only 5.36% [54], while International Mathemat-\nical Olympiad gold medalists achieve a near-perfect score. This highlights the significant gap\nin performance between current AI models and expert-level human reasoning. The problems\nfound in this data set require logical chaining, abstraction, and error-free computation, areas\nwhere LLM’s performance tends to be lackluster.\n5.7.13\nRAGEval\nRAGEval [65] is more than just a dataset, it is a framework design to assess RAG sys-\ntems across various scenarios by generating high-quality documents, questions, answers, and\n60\nreferences. All of these are generated by a schema-based pipeline to maintain accuracy. This\napproach allows them to implement metrics that differ from the standard and are more aligned\nwith factual accuracy, there are three metrics for this, Completeness, Hallucination, and Irrele-\nvance.\nThe process to generate all the needed files is as follows: S −→C −→D −→(Q, A) −→R −→\nKeypoints\nThis sequence shows all the steps taken by this approach starting with the schema summary\nS that leads to the configuration generation C, followed by the document generation D. With\nthe document formed the question answer pairs are formed (Q, A) and the references need\nto come that answer identified R and then the keypoints are extracted, representing the most\ncritical information in the answers.\nThe schema summary is an abstract representation of the key elements in a scenario-specific\ntext generation, these key elements encapsulate the aspects of essential factual knowledge from\nthe input documents. This schema acts as a backbone to ensure that the content is diverse\nand reliable while maintaining a standard across various scenarios. The schema defines the\nstructural framework of key elements for domain-specific documents without containing actual\ndata. As an example in medicine, it can outline categories for symptoms and treatments, in\nfinance it could establish classifications for metrics, sectors, and organizations. One concrete\nexample of a schema generation starts with the initial generation by the LLM, the model gets\nfeed with carefully chosen seed documents, these are real legal documents that represent the\nkind of knowledge and structure that the schema is to take. After the schema is created a series\nof iterative refinements are taken by a human using it’s intuition and contextual understanding\nto fix any nuances the model had generated. Due to the fact that this process occurs more than\nonce, it ensures that a balance between comprehensiveness, accuracy, and generalization, thus\nsupporting content generation across diverse sub-scenarios.\nGenerating a document that is rich with factual information and isn’t contradictory, is cru-\ncial to creating high quality datasets, ensuring that the generated content can be evaluated ac-\ncurately and used effectively in downstream tasks. To generate documents with that quality,\nfirst the configurations C are generated, these derive from the previously established schema S.\nThese configurations are used as references and constrains for text generation, thus maintaining\nconsistency across the document. To generate these configurations a hybrid approach is taken\nthat combines rule-based methods with LLMs to assign values to the schema elements. These\nrule-based methods like selecting values randomly from predefined scenario-specific options,\nensure that high accuracy and factual consistency is maintained for a more structured data. This\nand the more complex or diverse content, balances consistency and creativity. After the con-\nfiguration is ready a GPT-4o is used to convert the factual information from the C into a more\nstructured narrative format that is more aligned with a specific scenario. For example, in medi-\ncal records the generated document can include categories that add a more complex background\nto the document these can be a patient information, medical history, or a treatment plan. The\n61\nsame is done with other topics but with categories that better align with them.",
    "caption": "Figure 24: RAGEval System: 1 summarizing a schema containing specific knowledge from\nseed documents. 2 filling in factual information based on this schema to generate diverse con-\nfigurations. 3 generating documents according to the configurations. 4 creating evaluation\ndata composed of questions, answers, and references derived from the configurations and doc-\numents.[65]",
    "imagePath": "/images/figure_24_rageval_system_1_summarizing_a_schema_co.png"
  },
  {
    "pageNumber": 59,
    "imageNumber": 25,
    "type": "figure",
    "folderName": "Figure 25 Relationship between model size and mont",
    "precedingText": "Question-Reference-Answer (QRA) to generate these RAGEval uses the documents D and\nconfigurations C these are used to establish a robust evaluation framework ready to be used\non information retrieval and reasoning capable applications. The configurations C are used to\nguide the generation of the questions as well as the initial answers, this forces the generated\ncontent to be aligned with the schema elements. To address different types of questions like,\nmulti-hop reasoning, summarization, and multi-document questions, each one is specifically\ndesigned to evaluate specific facets of language understanding. To ensure diversity and control-\nlability of these questions 7 main question types were designed. The model gets provided with\ndetailed instructions and examples for the question type needed to be generated, the model then\noutputs the question Q as well as the initial answer A. Using the Q and A the relevant informa-\ntion fragments get extracted R from the documents D. This is done using an extraction prompt,\nthus ensuring that the generated answer is grounded in the source material this improves the\nreliability and traceability. To reduce the misalignment between A and R, the answers get it-\neratively refined thus also improving the coherence and accuracy. If references contain content\nmissing from the answers they supplement them accordingly. To reduce the hallucinations a\nlook is taken at the answers to find any unsupported content that either gets corrected with rel-\nevant references or removed. Finally the keypoints get generated from the answers A for each\nquestion Q to highlight the critical information in the responses. Normally each answer A gets\nbroken down into 3-5 keypoints, that encompass all essential factual detail, as well as relevant\ninferences, and conclusions.\nDragonBall dataset which means Diverse RAG Omni-Benchmark for All scenarios. This\ndataset was created using all the methods described above and encompasses a range of texts\n62\nand RAG questions across 3 main domains finance, law, and medical. This dataset consists\nof both Chinese and English texts, that serve as a comprehensive resource for multi-language\nscenario-specific research. Due to its big size of 6.711 questions it can be used on just English\nor Chinese assessment.\n5.7.14\nHotPotQA\nHotPotQA [66] is a dataset with 113k question-answer pairs that contain 4 main features\nthat tell it apart. Number one being the question requires finding and reasoning over multiple\ndocuments to achieve a correct answer. Number two is the variety these questions present, these\nare diverse and aren’t constrained to any pre-existing knowledge bases or schemas. Number\nthree this dataset provides sentence-level supporting facts that are needed for reasoning. Lastly\nnumber four it also introduces factoid comparison questions that test wether QA systems can,\nnot only extract relevant facts, but also compare them.\nThe data used for this dataset creation was gathered by crowd-workers on Amazon Mechan-\nical Turk. The gathered data had it’s origin on English Wikipedia, the process followed five\nmajor steps. Starting with finding a paragraph on a specific Wikipedia page, next is the navi-\ngation to a hyperlynk found in that paragraph, this will later be a related article. The following\ntask is the formation of a question about the two articles, this formed query can’t be answered\nwith just the information of one and needs the two of them to be complete. The following\nstep is the generation of a answer to the previous question this answer gets information from\nboth pages. The final step is to identify the supporting facts, this is a crucial explainability step\nwhere the worker must select the specific sentences from the two articles that are needed to\nreason through and arrive to the final answer. The specific sentences found are the ground-truth\nsupporting facts. These steps form the generation pipeline of this method.\nThis method being specially develop with reasoning in mind, comes with two main types of\nreasoning Bridge-Entity, and Comparison Reasoning.\nBridge-Entity Reasoning is most prevalent question in HotPotQA and involves a bridge\nentity, meaning the two necessary documents are linked by a common person, place, or thing.\nTo answer the question, the model must first use one document to identify this bridge and\nthen use the second document to find the final piece of information. As for the Comparison\nReasoning this requires the model to extract information and form various facts, these facts\nneed then to be compared between themselves to find the final solution. This can be something\nlike ”when was the last public record of Astra systems publicized”. This would require the\nmodel to find all the public records about Astra systems, then it would need to compare dates\nof all the documents, and only then would the final answer be evident.\nThis dataset also comes with a evaluation framework specially designed to work with the\ndifferences found from more common datasets. The evaluation is measured in answer accuracy\nthat is the standard evaluation of wether the final answer is correct, this is done using a Exact\nMatch (EM) and F1 score. To measure the Supporting Facts Accuracy the same EM and F1\n63\nscores are also used, this evaluates wether the model correctly identified what was needed to\nreach the final answer. One important thing to note is that the paper argues that for the model to\nbe accurate it should get both the answer and the reasoning path correct. So a combined metric\nis proposed where a model only gets the full credit when success at both tasks is achieved, thus\na stricter and more meaningful evaluation of a model’s true reasoning capabilities is achieved.\n6\nMethodology\nAs more powerful models hit the market with more expensive requirements, finding a model\nthat suits low-budget companies or entities is becoming harder. The problem occurs due to the\ngap in the performance of LMMs. LMMs with more tokens will always tend to have greater\nperformance [8]. However, this performance could be increased with methods described pre-\nviously like HyDE [40], RAG [35], CoT [67], CAG [43], RAGCache [44], SKR [52], and\nContriever [49]. All of these methods aim to improve model performance, but they do so at the\ncost of increased computational overhead. This overhead is variable, meaning that depending\non the method or combination of methods used, the system’s requirements and consequently its\nenergy consumption can change significantly. Since most of the studies are also on bigger mod-\nels a new approach will be taken to see if the results are similar to the bigger models. The key\npoints of the research are the efficiency and performance using some of the enhancing methods\ndescribed above. These will be paired with a small LLM meaning less than 36B tokens.",
    "caption": "Figure 25: Relationship between model size and monthly downloads[68]",
    "imagePath": "/images/figure_25_relationship_between_model_size_and_mont.png"
  },
  {
    "pageNumber": 61,
    "imageNumber": 26,
    "type": "figure",
    "folderName": "Figure 26 Traditional information retrieval archit",
    "precedingText": "6.1\nOverview of the Query Rewriting Flow\nThe use of Large Language Models (LLMs) in Information Retrieval (IR) systems has rev-\nolutionized the way people interact with and retrieve information. Traditionally IR systems,\nsuch as search engines, have evolved from term-based to neural network models, which are\nuniquely suited to detect semantic nuance and contextual hints. Nevertheless, such systems still\npose challenges like query vagueness, data sparsity, and generation of responses that, although\n64\nplausible, are actually incorrect. LLMs, with their remarkable ability in language generation,\ncomprehension, and reasoning, offer a unmatched solution for the aforementioned challenges.\nLeveraging their huge pre-training on diverse text-based data, LLMs enhance various IR com-\nponents like query rewriting, retrieval, reranking, and response generation and enable more\nprecise, contextual, and user-centric search experience as shown in Figure 26.\nBreakthroughs in cutting-edge large language models (LLMs) LLaMA have exhibited the\ncapability to accomplish demanding tasks and optimize information retrieval (IR) performance.\nThese models have not just the capacity to de-modularize user queries and retrieve relevant\ndocuments but also render lengthy and human-sounding responses, thereby overcoming the\nlimitations between traditional IR systems and the present user expectations.",
    "caption": "Figure 26: Traditional information retrieval architecture[68]",
    "imagePath": "/images/figure_26_traditional_information_retrieval_archit.png"
  },
  {
    "pageNumber": 63,
    "imageNumber": 27,
    "type": "figure",
    "folderName": "Figure 27 Graphical representation of the system d",
    "precedingText": "The proposed system will integrate a rewriter module that will be placed between the user\nquery q and the retriever similar to [45], this will enable the injection of the retrieved documents\nas-well as the injection of instruction like CoT. This system works by first receiving the user\nquery, which will be processed by the rewriter module. This module inserts an instruction\ninst that is used to determine the most suitable rewriting strategy to optimize the query. The\ninstruction will prompt the LLM to evaluate which of the three approaches is the more suitable\nnormal response, RAG, or Chain-of-Thought.",
    "caption": "Figure 27: Graphical representation of the system diagram[68]",
    "imagePath": "/images/figure_27_graphical_representation_of_the_system_d.png"
  },
  {
    "pageNumber": 64,
    "imageNumber": 40,
    "type": "page",
    "folderName": "Page_64_Image_40",
    "precedingText": "Each question was encoded into embeddings, and computed the semantic similarity through\ncosine distance sim(qt, qi) = (\ne(qt)·e(qi)\n||e(qt)||·||e(qi)||), where qi ∈{q+\n1 , . . . , q+\nm, q−\n1 , . . . , q−\nn }, e(·) being\nthe representation of a sentence encoder. Then the search for the top-k nearest neighbors can\nbe done based on the results that include the l positive ones and k −l negative ones. This can\n53\nbe used to label the question qt as positive if\nl\nk−l ≥m\nn or negative if\nl\nk−l < m\nn m and n are the\nnumber of questions from D+ and D−respectively.\n5.6.3\nUsing Self-Knowledge for Adaptive Retrieval Augmentation\nThe self knowledge acquired previously from the LLMs responses or the predicted labels\nreflect the necessity or not of retrieving external knowledge, for each question qt accordingly.\nSo that retrieval can be done or not depending on these results.\n5.7\nModel Comparison\nThe model comparison will be used to help pick the correct model based on the require-\nments, these could be based on VRAM, performance or open-sourceness. A well-known leader-\nboard for LLM performance is llm.extratum.io [54]. This leader-board can be sorted based\non many characteristics like VRAM usage, quantization level, model size, and many more.\nLlm.extratum.io also sorts based on performance that is measured on key evaluation data sets\nand metrics like GPQA, MUSR, BBH, IFEval, ARC, HellaSwag, MMLU, ThrutfulQA, Wino-\nGrande, GSM8K, MATH Lvl5 and MMLU Pro. These are some of the best methods to quantize\na model’s performance on multiple aspects and will be discussed further in the next sections.\n5.7.1\nMMLU\nThe MMLU [55] or Massive Multitask Language Understanding is an LLM benchmark\nthat consists of a dataset designed to be a comprehensive test of the model’s ability to respond\ncorrectly on a diverse range of tasks and topics. It includes 57 different subjects that include\nknowledge across many disciplines like humanities, STEM, social sciences, and professional\nfields.\nThe questions in the benchmark are multiple-choice of four options there are over\n15.000 questions ranging from simple elementary math questions all the way to professional\nmedicine making it a very diverse benchmark. Some of the more important features include the\nstandardized evaluation metrics, calibrated difficulty levels, comprehensive coverage of human\nknowledge, and professional-level expertise requirements. All of these points transform this\nbenchmark into a very important benchmark in the field due to its variety and complexity. This\ntest has however become easy for the most recent models like LLaMA 3 70B which achieved\n80.06 out of 100 [54]. To counteract the new advancements an improved version of the MMLU\nwas developed, MMLU-Pro[56].\n5.7.2\nMMLU-Pro\nMMLU-Pro[56] is a more robust and challenging multi-task language understanding bench-\nmark. This was achieved by increasing the complexity of the options expanding from 4 options\nto 10 thus reducing the probability of guessing the correct answer by chance this also made the\n54\nbenchmark more challenging and more discriminative. This was done using GPT4-Turbo to in-\ntroduce six additional choices. These are created with the intuition of being plausible distractors\nthat need discerning reasoning to pick the correct answer. The questions were also improved\nadding to the quality of the benchmark by eliminating trivial and noisy questions from the orig-\ninal MMLU. Which contained some that were found to be too easy by using a list of small\nLLMs when more than four were able to answer the question this question was then removed.\nThe benchmark was also improved by increasing the portion of more challenging questions by\nadding a bigger share of college-level exam problems. All of these changes were then verified\nby two rounds of expert reviews to reduce the dataset noise. This proved to make the benchmark\nmore robust making it less sensitive to prompt variation changing from 4% −5% to just 2%.\nThis came with the benefit of generating a more stable performance across different prompt\nstyles showing a greater consistency in model evaluation. These improvements achieved a big\nimprovement in the discrimination between results of different models that previously scored\nsimilarly, the prior gap between GPT-4o and GPT-4-Turbo was 1% with MMLU-Pro it’s 9%.\n5.7.3\nGPQA\nGPQA [57] this benchmark differs a lot from others since this benchmark was made to\nevaluate an LLM’s ability to respond to 448 multiple-choice questions. Which were created\nby domain experts in biology, physics, and chemistry. These questions were tested on experts\npursuing PhDs in the corresponding domain and still only reached at best 74% when discounting\nthe clear mistakes the experts had identified in retrospect. One good point to mention is that\nthis was also tested on what the paper describes as highly skilled non-expert validators where\nthe accuracy was only 34%. This was done with an average of 30 minutes per question and\naccess to the web to research the topic. This dataset questions were first written by a domain\nexpert, this was then answered by another domain expert who would give constructive feedback\nto improve the clarity of the question and would also suggest revisions if needed. After said\nrevision by the writer of the question, it is sent to another different domain expert and three\nnon-expert validators who are experts in other domains to access the quality of the query and\nvarious options of answer.\nThis method made sure that the questions are proven and tested by at least two different\ndomain experts and three other area experts. GPQA is divided into 3 subsets Extended, Main\nset, and Diamond. The extended subset contains all of the validated questions with 546 different\nquestions.\nGPQA is the name of the main not containing non-objective questions, these being those that\nboth domain expert validators got wrong yet the three non-expert validators got right. This set is\ncomposed of 448 questions, these are also composed of questions that where one of the domain\nexpert validators answered incorrectly but agreed that they made a clear mistake after being\nshown the solution. The strictest set is the Diamond where only 198 questions are present, these\nare the highest quality and thus only include questions where both experts answered correctly\n55\nand the majority of non-experts answered incorrectly. Though this also includes the questions\nwhere the second domain expert validator got the answer wrong yet explains his mistake once\nshown the correct one similarly to the previous set, but in this case, the first domain expert must\nanswer correctly.\nThis benchmark proved very efficient at achieving the proposed results since even with\ninternet access GPT-4 only achieved 39.4% which was an increase of just 0.4% from the original\nscore without internet access. At that point in time when this benchmark was launched GPT-4\nwas the state of the art with older models like GPT-3.5 only achieving 28% check Table 11 for\nmore tests.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_64_image_40.png"
  },
  {
    "pageNumber": 65,
    "imageNumber": 28,
    "type": "figure",
    "folderName": "Figure 28 Analysis Prompt",
    "precedingText": "If the models concludes that it can answer directly then the model proceeds to generate the\nanswer without the need of a new generation, meaning only one hop. As for Chain-of-Thought,\n65\nthe approach works by injecting a command to try and force the model into thinking step by\nstep to obtain the answer. The final approach is Retrieval-Augmented Generation (RAG), which\nis also the most complex. It begins by passing the query to a smaller model that generates an\nembedding representation of the query, aligned with a pre-embedded document collection. A\nsimilarity comparison is then performed to retrieve the most relevant documents based on this\nembedding space. Next, a rewriter is used to extract and pass only the most important parts\nof the retrieved documents, reducing the amount of irrelevant information passed to the model.\nFinally, a reranker prioritizes the most relevant documents so that they are presented first during\nthe generation process. The last two aren’t necessarily needed so they can be turned off as\nneeded for more .\n6.2\nHardware and Software Environment\nThe experiments were conducted on a high-performance workstation running windows 10,\nequipped with an Intel Core i7-13700KF processor, an NVIDIA RTX 4090 GPU, and 32 GB\nof DDR5 RAM operating at 6000 MHz. The software environment was managed using Python\n3.12 within a virtual environment (.venv), ensuring isolation and reproducibility of dependen-\ncies. The main libraries required are PyTorch (with CUDA 12.6 support) and the Hugging\nFace Transformers library, which was used to download and run the deepseek-ai/DeepSeek-\nR1-Distill-Llama-8B LLM. To monitor the system performance and power consumption during\nmodel inference and other system requirements, HWInfo was employed. Additional Python\nlibraries were installed as required to support the various aspects needed for the workflow. This\nsetup provides a robust and efficient platform for running and evaluating the proposed system.\n6.3\nRewriting Approaches\n6.3.1\nStraight LLM\nThis approach uses the initial model response as the final answer. It is designed for ques-\ntions that are too simple to require more complex methods. From an efficiency standpoint,\nminimizing processing steps is desirable, and using the first response avoids redundant genera-\ntion. As the simplest approach, it does not enhance the model’s answer but provides a baseline\nfor comparison.\n6.3.2\nChain-of-Thought\nThe Chain-of-Thought (CoT) approach enhances model performance by encouraging it to\nreason through the logical steps of a query. In some models, this can be achieved with simple\nprompts such as ”Please reason step by step, and put your final answer within a box.”, as seen\nwith DeepSeek models [69]. Other models may require more elaborate and tailored\n66\nThis approach has been widely adopted to improve and enhance the reasoning capabilities of\nLLMs. One of the reasons for this adoption comes from the simple requirements since it doesn’t\nneed much change on already-built systems. This method has shown significant improvements\nin certain tasks that require logical thinking and contextual understanding. For instance, some\nbenchmarks often use both with and without Chain-of-Thought prompting to show the direct\nimpact of the on reasoning performance [70], [58]. The proposed system selects this option\nwhen the initial model response indicates that Chain-of-Thought reasoning is required. This\nmethod involves two inference steps: the first allows the LLM to assess whether CoT is neces-\nsary, and the second passes a modified query containing the appropriate instruction to prompt\nthe model to reason through the problem. Finally the model answer the instruction that now\ncontains a CoT instruction.\n6.3.3\nRAG\nThis approach is used to retrieve documents or passages that are relevant to the user’s query.\nThis is done by embedding the user query into a vector , which is then compared to the vectors\nof stored documents. Based on a top-K similarity ranking, the most relevant documents are\nretrieved. These documents can then be refined by removing non-essential parts, aiming to make\nthe resulting content as concise and relevant as possible. Following refinement, the documents\nmay be reranked, as language models tend to focus more on the initial tokens in the input [71].\nSimilar to the Chain-of-Thought method, this approach also requires two hops: The first hop\nacts as a reflection step to determine whether the LLM deems document retrieval necessary; The\nsecond hops consists on passing the query as well as the retrieved documents to the LL;\n6.3.4\nSelecting the Approach\nThe system has three possible approaches to choose from: Retrieval R, CoT C, and Straight\nanswer S. Since only one of these approaches can be selected for a given query, a selection\nmechanism is required. This selection is performed using the model (M) and the content of the\nquery (q), this is shown in Figure 29",
    "caption": "Figure 28: Analysis Prompt",
    "imagePath": "/images/figure_28_analysis_prompt.png"
  },
  {
    "pageNumber": 65,
    "imageNumber": 41,
    "type": "page",
    "folderName": "Page_65_Image_41",
    "precedingText": "Table 11: Accuracy on each set [57]\n5.7.4\nMUSR\nThis benchmark was developed to evaluate LLMs on multistep soft reasoning tasks specific\nto the natural language narrative [58]. This is done by three main domains murder mysteries,\nobject placements, and team allocation. All of these are synthetic meaning, these were produced\nby an LLM. The creators choose to use synthetic stories due to two main reasons scalability and\nthe ability to regenerate the story due to possible data leaks. The scalability is important since\nmore capable LLMs are being created every year,the dataset could be adapted as needed to\nbecome more complex and add longer narratives thus introducing more difficult access for the\nLLMs. The ability to regenerate the story is also very important since a data leak could mean\nthat the LLMs could be trained with the data of the benchmark which would take away the\nzero-shoot aspect of this benchmark.\n56",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_65_image_41.png"
  },
  {
    "pageNumber": 67,
    "imageNumber": 29,
    "type": "figure",
    "folderName": "Figure 29 Jsonl Data Structure",
    "precedingText": "Figure 28 presents the analysis prompt responsible for this selection. This instruction can\nbe divided into two parts. The first part is responsible for the Straight Answer (S), it asks the\n67\nmodel to respond directly to the query if it is confident it can do so correctly. This approach\nis not only the fastest as well as the simplest, as the system does not need to perform any\nadditional steps to achieve the response. The second part of the prompt is responsible to induce\nthe model into deciding between Retrieval (R) or CoT (C). This is done by asking the model if\nit would benefit from retrieval of documents. If the response is ”Yes” then Retrieval process is\ntriggered, and all subsequent steps such as reranking and refining are also executed. However,\nif the model’s answer is ’No’, the system interprets this as a lack of confidence in the initial\nresponse. To improve the quality of the final output, the system then forces the model to use\nChain-of-Thought (CoT) reasoning.\nTo translate the model’s textual output into a definitive choice, the system employs a so-\nphisticated post-processing and also voting mechanism rather than simply looking for a single\nkeyword. This implementation is a critical part that enables the system to use a decision-making\nprocess robust and resilient to multiple formatting variations. The process unfolds as follows:\n1. Initial Cleaning: The raw output from the model is first decoded and also normalized\nto standardize characters and spacings. Any preliminary ”Chain-of-Thought” reasoning,\nwhich is normally enclosed in special tags like ” < /think > ”, is stripped away to\nisolate the final answer.\n2. Check for a Direct Answer(early exit): Before classifying the need for retrieval, the sys-\ntem first checks if the model already provided the final answer on it’s first analysis. It\nspecifically looks for a ”\\\\boxed{...}” pattern containing a single alphabetic character\n(e.g., ”\\\\boxed{A}”). If this pattern is found, the system assumes it corresponds to strat-\negy S. The process then halts immediately, returning the response as the final output. The\n’method used’ is set to ’none’, indicating that no additional processing was necessary.\n3. A Voting System For Decision Making: Instead of a simple parse, the system incurs a\nscoring mechanism to ”vote” on the best possible approach. It initializes counters for\nboth R and C.\n4. Identifying Strong Signals: The code meticulously scans the output for high-confidence\nindicators. A ”\\\\boxed{1}” pattern is considered a strong, explicit vote for R, adding a\nsignificant score of 100 points. Similarly, a ”\\\\boxed{2}” is a strong vote for C.\n5. Identifying Secondary Signals: To enhance robustness and accuracy, the system also looks\nfor secondary indicators. The presence of the word ”yes”(case-insensitive) in the analysis\nalso adds 100 points to the R counter. This ensures the model’s intent is captured even\nwhen the model fails to use the precise boxed format.\n6. Final Decision: After analyzing the entire output, the approach with the highest accumu-\nlated score is chosen as the ”suggested method”. This multi-signal, voting-based mech-\n68\nanism makes the classification resilient to minor formatting errors from the model, thus\nprioritizing a correct interpretation over strict adherence to a single output format.\n6.4\nDataset Augmentation\n6.5\nQuery-Answer Validation\nThe proposed validation framework is comprised of a three-stage process that is used to sys-\ntematically differentiate between high-quality and problematic queries. This hybrid approach\nintegrates automated analysis with a human-in-the-loop component.\n6.5.1\nAutomated Preparation\nThe data is ingested and categorized using a python script. The script then splits the queries\ninto those that require retrieval and those that do not. Depending on the configuration, one\nof these two sets is selected for further processing. This set is then divided into batches of\nn queries, a value that can be configured in the settings. Each time the Alt key is pressed, a\nnew batch is compiled and copied to the clipboard, ready to be pasted into the LLM chat. The\ngenerated batch already includes the appropriate instruction, tailored to the selected query type.\n6.5.2\nAI-Powered Triage\nThe LLM analyzes the provided query and determines if the options are clear and the ground\ntruth is correct, paying special attention to the number of correct options. If these prove to be\ncorrect then a Green Flag is assigned. This flag contains an emoji so the human supervisor\ncan identified the queries easily. In this workflow, a Green Flag is treated as a definitive pass,\nmeaning these queries are not further reviewed by the supervisor.\nA Red Flag, on the other hand, is assigned to any query that fails to meet the criteria for\nexample, due to ambiguous wording or other inconsistencies. Unlike the Green Flag, a Red\nFlag does not automatically discard the query. Instead, it signals that the query requires human\nverification. Although the model provides an explanation for why the Red Flag was assigned,\nthe final decision rests with the human supervisor.\n6.5.3\nHuman Verification\nThe human validator is instructed to only look at the Red Flags queries to maintain attention\nand reduce the number of queries a human needs to verify. The job of the validator falls into the\nverification of the LLM’s justification for the Red Flag, this verification can lead to the validator\ndoing some research on the query and the correctness of the expected output. In case a query\nfails this verification it is deemed unusable and gets removed from the dataset, a new one is\nadded to it’s place that passes this validation.\n69\n6.5.4\nDataset formation\nThe output of all the previous points are fully answerable queries that are free of ambiguity.\nThis is used for two distinct datasets, the ARC-easy and the HotPotQA, the results are then\njoined to combine queries that require retrieval as well as those needing step-by-step reasoning.\nThis new dataset is specially designed for systems that can decide what approach is required for\nthe best possible outcome. However due to the lack of complexity of the ARC-easy queries this\nis more suited towards weaker than state-of-the-art, more around 32B tokens and lower.\n6.6\nPower Data Collection\n6.6.1\nGPU\nTo compare the different components of the system, one important aspect is energy con-\nsumption. However, collecting this data on Windows is not straightforward. Due to hardware\nlimitations, all measurements will be performed using software tools that query system com-\nponents to report energy usage at a given moment. Starting with GPU the power consumption\nis collected using nvidia-smi [72], this tool acts as bridge that queries the GPU driver directly\nand converts the retrieved data into a useful unit, these data points are collected every 15ms,\nhowever due to driver overhead the real gap is around 50ms. The data collected according to\nNvidia NVML [73] is related to TBP or total board power, meaning that the consumption of\nVRAM and all the necessary components to support the GPU die itself are included in that\npower measurement. This is important as the power consumption of VRAM is highly used by\nLLM’s during inference. This data is collected directly by a purpose-built library that monitors\nGPU power draw. The power measurements are added directly into the JSONL file, which also\ncontains the model’s response and all relevant metadata for later analysis.\n6.6.2\nCPU\nThe other power consumption metric is the CPU package. This measures the power used by\nthe CPU chip itself, excluding any power consumed by supporting components such as voltage\nregulator modules (VRMs), the chipset, and other peripherals. Although it would be interesting\nto measure the full system power draw, this requires specific hardware that was not available for\nthis project. This CPU package draw isn’t super easy to obtain in a Windows system so the use\nof a proprietary tool called HWiNFO this tool offers a logging feature that creates a CSV with\nall the collected data, this collection is done every 20ms theoretically however in reality there\nare a lot of times where it takes more than that, but on average it takes 31ms.\nBecause the CSV is generated by a third-party tool, it is only available at the end of the\nrun when logging is manually stopped. To align energy data with query execution, a script is\nused to match each query’s start and end times (from the previously mentioned JSONL file)\nwith the corresponding timestamps in the CSV. All data points that fall within a query’s time\n70\nwindow are extracted. Using these points and their timestamps, the trapezoidal rule is applied to\napproximate the energy consumption. This method works by summing the areas of trapezoids\nunder the power curve, providing an estimate of the integral of power over time (watts × time).\nThis approach compensates for the irregular intervals in data retrieval by HWiNFO. The result\nis an estimate of energy consumption by the CPU package, expressed in watt-hours. This value\nis then added to the JSONL file, along with the total energy consumption calculated as the sum\nof the GPU’s Total Board Power (TBP) and the CPU package power. GPU power usage is\nalready recorded in the JSONL, and the trapezoidal rule is applied in real time during inference\nto account for variations in the intervals between data points.",
    "caption": "Figure 29: Jsonl Data Structure",
    "imagePath": "/images/figure_29_jsonl_data_structure.png"
  },
  {
    "pageNumber": 69,
    "imageNumber": 30,
    "type": "figure",
    "folderName": "Figure 30 Instruction V1",
    "precedingText": "In Figure 29 is depicted the final JSONL structure. The structure is devised into four main\nparts Query Information, Ground Truth, AI Prediction, and Performance & Metrics each being\nfinalized at different stages. The system initially gets a JSONL with just the Query Information\nand the Ground Truth. Part 2 is formed by the responses and metrics from the system solution.\n6.7\nEvaluation Framework\nThe evaluation framework may vary depending on the specific domain being tested. This\nis because some domains might require different metrics to understand the real capabilities of\n71\nthe model in a given task [74]. Another key point is the need to access each part individually\nas well as combined. This is key in accessing the quality of the system and understand which\npoints could be improved for a better combined performance.One of the most important metrics\nacross all components of the system is efficiency, as it helps to assess how each part contributes\nto the overall energy consumption. What will be compared and obtained is the following:\n• System answers to full dataset.\n• Straight model answers to full dataset.\n• Forced CoT answers to full dataset.\n• Forced Retrieval answers to just full dataset.\nThe full dataset consists of 3000 queries, with 1312 originating from the ARC-Easy dataset\nand the remainder from the DragonBall dataset. The ARC-Easy dataset was selected because\nit primarily contains simple reasoning queries that the model can answer without relying on\nexternal knowledge, although a few questions do require more complex reasoning. The Drag-\nonBall dataset, on the other hand, was chosen because it mostly includes queries that necessitate\nretrieval, with some also requiring advanced reasoning to be answered correctly.\nTogether, these datasets offer a comprehensive evaluation of the system’s capabilities. Ad-\nditionally, if the model under study is a larger one, the ARC-Easy portion can be replaced by\nARC-Challenge, which features more complex queries that demand deeper reasoning than its\nsimpler counterpart.\n6.7.1\nRetrieval Performance Metrics\nDue to the nature of this work, the quality of the retrieval itself will not be evaluated, as it\ndepends on the specific RAG method employed. Instead, the evaluation will focus on whether\nretrieval occurred when it was necessary. This will be represented as a binary outcome: 1 if\nretrieval was triggered, and 0 if not. However, what needs to be evaluated is a direct compari-\nson between the energy consumption of the proposed system and that of a baseline that always\nperforms retrieval. This comparison is important because the system requires two hops to de-\ncide whether to retrieve, whereas always retrieving eliminates the need for this decision-making\nstep. However, since the dataset includes questions both with and without the need for retrieval,\nan evaluation will be conducted to determine whether the system results in lower energy con-\nsumption. This is based on the premise that retrieval is not necessary for every query, and the\nsystem may avoid unnecessary retrieval steps. The quality and correctness of the answer will\nalso be evaluated. This is important because, in cases where retrieval is not necessary, the sys-\ntem may still retrieve documents from the database that are not directly relevant to the query.\nAs a result, these retrieved passages may not contribute meaningfully to the answer.\n72\n6.7.2\nStraight Model\nThis approach will be evaluated in multiple parts. The first aspect is whether the answer is\ncorrect specifically, whether the model’s response matches the ground truth option. Next, the\nevaluation will check if the model correctly identified queries that should be answered without\nretrieval. This part is linked to the previous one: if the answer is correct without retrieval,\nthe classification is also considered correct. Additionally, a comparison will be made between\nanswers generated with and without forced Chain-of-Thought (CoT) prompting to determine\nwhether the increased energy consumption associated with CoT leads to improved answers or\nif the same responses would have been provided without it. This will be done to understand\nif the model is guessing correctly wether it can answer the question directly or not. And will\nalso provide consumption metrics that will be compared in order to understand its efficiency.\nCoT will tend to be more accurate but it also requires more energy due to the thinking phase of\ngeneration.\n6.7.3\nChain-of-Thought Reasoning Performance Metrics\nThe metrics for CoT are the same as those used for the straight model, as both will be\ndirectly compared.\n6.7.4\nAutomated Evaluation Script\nTo implement the evaluation metrics described, particularly for understanding the correct-\nness of the model’s answers, an automated script was employed. This script is responsible for\nprocessing the model’s output for each query, which is stored in a JSONL file format. The pri-\nmary goal is to systematically determine if the model’s final answer matches the ground truth,\nespecially for ARC queries which are multiple-choice questions.\nThe core to this evaluation lies in a multi-step parsing strategy designed to intelligently\nextract the final answer from the model’s potentially complex and verbose output, similarly to\nhow a human user would read the output and understand which character is the one that the\nmodel chose. The process is as follow:\n1. Definitive Answer Extraction:\nThe script first searches the model’s prediction for the\nmost explicit answer format, such as ”\\\\boxed{B}”. This format is often used by models\nto clearly delineate the final answer, so finding it is the most reliable sign. However, since\nthis work is conducted using smaller models, they often do not follow patterns very well\nand may provide the answer surrounded by verbose context reflecting their reasoning.\n2. Pattern-Based Fallback: If the first pattern is not found, the script then looks for common\nnatural language phrases that indicate a final answer, like ”The answer is B” or ”Answer:\nB”. It is designed to take the last match it finds, operating on the assumption that any\nreasoning or changes of mind would occur before the final declared answer.\n73\n3. Positional Fallback: As a final strategy, if neither of the above patterns yields a result,\nthe script searches for all standalone capital letters (A, B, C, or D) within the response.\nSubsequently, the system selects the final occurrence as the intended answer, assuming it\nreflects the model’s ultimate decision. This servers as a robust fallback for cases where\nthe model might provide the final answer without any formatting.\nOnce one answer is extracted through this hierarchy of methods, it is compared directly with\nthe ”ground truth” value from the JSONL. A new metric, ”correct answer arc”, is added to the\ndata inside the metrics section, this then gets marked as ”true” if they match and ”false” if not.\nFurthermore, this script is also responsible for the automation of the retrieval performance\nmetric. It checks if the ”references” inside the ”ground truth” section contains any references\nthat indicate that retrieval was needed. It then cross-references this with the ”method used”\nvalue that represents the path the system chose. If the model used ”retrieval” for a question\nthat required it, a ”retrieved correctly” metric is marked as ”true” on the same metrics section,\naligning with the binary evaluation approach mentioned previously. This automated process\nensures a consistent and scalable way to apply the defined evaluation criteria across the entire\ndataset.\n6.7.5\nEfficiency Metrics\nEfficiency is a metric that can be measure in various ways depending on the focus of the\nevaluation. This could be power consumption, cost-effectiveness, or scalability, each of which\nplays a central role in the direction of this thesis. Power consumption will be measured as watts\nper query . This metric is important due to the multiple processing steps involved in generating\neach output. However, it will not reflect the total system power consumption, as only CPU and\nGPU usage will be measured due to hardware limitations.\nThe cost-effectiveness will be calculated by dividing the cost of the required hardware com-\nponents by the system’s performance. This allows for a direct comparison between this ap-\nproach and more powerful alternatives. Such comparisons can be conducted through a series\nof tests, similar to the benchmarks referenced in model comparison section. Scalability will\nbe assessed by analyzing the requirements needed when using a larger model or when more\ndocuments and keywords are added to the system. This will likely be the most difficult metric\nto evaluate directly, as I do not have access to more powerful hardware. However, I will attempt\nto estimate scalability based on data and findings from other researchers.\n6.8\nOptimizing Query Classification through Iterative Prompt Refinement\nThe performance of a LLM is fundamentally linked to the clarity and quality of the instruc-\ntion provided. In this system, where the initial goal is to classify a user’s query into one of three\npaths retrieving external documents, reasoning step by step, or simply using the model’s first\n74\nresponse as the answer (both relying on the model’s internal knowledge) the construction of the\ninstruction plays a crucial role. By carefully designing this part of the process, we can guide\nand control the model’s decision-making behavior. To determine the most effective approach\nfor this classification task, a series of instructions were developed and tested, evolving from a\nsimple open-ended prompt to a highly structured one that involves a fully rule based framework.\nThis section will analyze the iterative refinement process of the instructions in detail, eval-\nuating the performance of each version. The evaluation on this section focuses on key metrics,\nsuch as: Routing Accuracy (the model’s ability to correctly chose ’retrieval’ or ’no-retrieval’),\nAnswer Accuracy (the correctness of the final response), and Efficiency (measured in energy\nconsumption) though this metric will be more thoroughly looked at at a later stage. By exam-\nining the trade-offs between these factors at each stage, we can identify the best practices for\nguiding an LLM in a complex classification task.\n6.8.1\nInstruction V1: A Simple Baseline",
    "caption": "Figure 30: Instruction V1",
    "imagePath": "/images/figure_30_instruction_v1.png"
  },
  {
    "pageNumber": 71,
    "imageNumber": 31,
    "type": "figure",
    "folderName": "Figure 31 Instruction V1 Results",
    "precedingText": "The initial instruction, V1, was designed as a minimal baseline to assess the feasibility of the\nproposed approach. This instruction directly asks the model for a binary classification (”Would\nthis Query benefit from the retrieval of documents?”) Figure 30, with a heavy emphasis on the\noutput format rather than the decision-making logic. This lack of explicit criteria forced the\nmodel to rely almost totally on its internal, pre-existing biases to interpret the query’s needs.\n75",
    "caption": "Figure 31: Instruction V1 Results",
    "imagePath": "/images/figure_31_instruction_v1_results.png"
  },
  {
    "pageNumber": 73,
    "imageNumber": 32,
    "type": "figure",
    "folderName": "Figure 32 Instruction V2",
    "precedingText": "76\nAs the performance data from the evaluation reveals, this approach was highly inconsistent\nand ultimately unreliable. The model developed a strong bias against retrieving documents,\nleading to a significant imbalance in the classification. This likely occurred due to the smaller\nmodel lacking sufficient internal knowledge, as it is less capable than state-of-the-art models.\nWhile the system was adept at identifying general knowledge questions (those not requiring\nretrieval), it correctly avoided retrieval 98.17% of the time, refer to Figure 31. However, it\nalmost completely failed at the inverse task, only correctly choosing retrieval 16.59% for the\nqueries that required it. As a result, the overall routing accuracy was limited to just 52.3%.\nThis bias is further evident in the routing decision matrix, which shows that out of 1688\nquestions that ideally required retrieval to answer correctly, the model incorrectly routed 1408\nof them to be answer without retrieving. This fundamental failure to identify questions need-\ning external knowledge confirmed that a simple, unguided prompt is insufficient for creating\na reliable query-routing system. While this appears to be true for the chosen model size, fur-\nther testing is necessary to determine whether this limitation persists in larger models or if they\nperform better on this task.\n6.8.2\nInstruction V2: An Aggressive, Safety-First Heuristic",
    "caption": "Figure 32: Instruction V2",
    "imagePath": "/images/figure_32_instruction_v2.png"
  },
  {
    "pageNumber": 75,
    "imageNumber": 33,
    "type": "figure",
    "folderName": "Figure 33 Instruction V2 Results",
    "precedingText": "77\nTo counteract the significant bias against retrieval observed on the first approach Figure\n30, the second iteration introduced a strong, explicit bias towards retrieval. Instruction V2\nFigure 32, framed the model as an ”expert query analyzer” with the primary goal of eliminating\nincorrect answer by trying to force document retrieval for any non-trivial query. It established\nretrieval (’1-Yes’) as the default assumption, permitting a direct answer (’2-No’) only if the\nquery met a very strict and narrow set of criteria: it had to involve exclusively ’globally famous\nentities’ and ask for a single, static, and universally known fact. This ”safety-first” heuristic\nwas designed to try and minimize the risk of factual errors originating from the model’s internal\nknowledge.\nThis agressive change dramatically inverted the model’s behavior. The Retrieval Task Rout-\ning Accuracy skyrocketed from 16.59% to 98.22%, demonstrating that the model could now\nreliably identify questions that required external documents. Out of 1688 such questions, it\ncorrectly chose ”retrieval” for 1659 of them.\nHowever, this success came at significant cost to efficiency and accuracy on the opposite\ntask, with results that were almost predictably inverse to those of the first instruction. The\nmodel’s ability to recognize simple, general knowledge questions plummeted. This can be seen\non the General Knowledge Routing Accuracy that fell from 98.17% to a mere 26.07%. The\nsystem was now incorrectly choosing to retrieve documents for the vast majority of the queries\nthat did not need it. This over-correction is properly showed on the decision matrix, where 970\nout of the 1312 No Retrieval queries, were wrongly sent down the retrieval path.\nAlthough the Overall Routing Accuracy improved to 66.7%, this aggressive heuristic proved\nto be an over-correction. While it successfully enforced the retrieval of necessary information,\nit failed to account for cases where retrieval was unnecessary. This led to inefficient and often\nredundant processing for a large number of relatively simple queries that the base model could\nhave answered directly. This showed perfectly that while a strong default can steer the model’s\nbehavior, a purely aggressive approach is too rigid and fails to balance accuracy with efficiency.\nAll of this is evidenced by the results shown in Figure 33.\n78",
    "caption": "Figure 33: Instruction V2 Results",
    "imagePath": "/images/figure_33_instruction_v2_results.png"
  },
  {
    "pageNumber": 77,
    "imageNumber": 34,
    "type": "figure",
    "folderName": "Figure 34 Instruction V3",
    "precedingText": "79\n6.8.3\nInstruction V3: Introducing Balanced Criteria",
    "caption": "Figure 34: Instruction V3",
    "imagePath": "/images/figure_34_instruction_v3.png"
  },
  {
    "pageNumber": 79,
    "imageNumber": 35,
    "type": "figure",
    "folderName": "Figure 35 Instruction V3 Results",
    "precedingText": "The third iteration, Instruction V3, tried to strike a balance between the aggressive retrieval\nstrategy of V2 and the passive approach of V1. The goal was to try and improve efficiency by re-\nducing unnecessary retrievals without sacrificing the accuracy gains made on complex queries.\nThe key refinement was the introduction of explicit, positive categories for non-retrieval (”2-\nNo”). For the first time , the model was given clear examples of queries that were meant to be\nanswer directly, such as ”Universally Known Facts”, ”Creative Tasks”, and ”General Explana-\ntions”.\nThis structured, two-step process first checks for a simple case, and only then defaulting to\nretrieval on more complex queries. This proved to be a significant step in the right direction.\nThe model was no longer forced into an overly aggressive default and was instead required to\n80\nreason through its decision-making process to select the appropriate path.\n81",
    "caption": "Figure 35: Instruction V3 Results",
    "imagePath": "/images/figure_35_instruction_v3_results.png"
  },
  {
    "pageNumber": 81,
    "imageNumber": 36,
    "type": "figure",
    "folderName": "Figure 36 Instruction V4",
    "precedingText": "82\nThe results depicted in Figure 35 reflect this new found balance. The Retrieval Task Routing\nAccuracy remained exceptionally high at around 98%, indicating that the safety-first principle\nfor complex questions was successfully maintained. The model correctly identified 1655 out of\n1688 queries that required retrieval.\nCrucially, the opposite task had been the main weakness of the previous iteration, and this\nremained true in the current version, with results deteriorating further the General Knowledge\nRouting Accuracy dropped from 26.07% to 21.27% on the General Knowledge Routing Accu-\nracy. This can also be seen on the number of times that the model picked ”retrieval” 1033 of the\n1312 general knowledge questions that don’t require it.\n83\n6.8.4\nInstruction V4: A Shift to Profile-Based Classification",
    "caption": "Figure 36: Instruction V4",
    "imagePath": "/images/figure_36_instruction_v4.png"
  },
  {
    "pageNumber": 82,
    "imageNumber": 37,
    "type": "figure",
    "folderName": "Figure 37 Instruction V4 Results",
    "precedingText": "The mixed results presented on the previous iteration highlighted a potential weakness in\nthe sequential, rules-based checklist approach. This new instruction V4 represented a major\nconceptual shift, this time re-framing the task following a procedure to a more holistic classi-\nfication exercise. Instead of the previous step-by-step process, the model was now tasked with\nmatching the user’s query to one of two detailed profiles: ”Profile 1: Retrieval Required” or\n”Profile 2: Direct Answer Sufficient”.\nThis new profile-based structure is more intuitive for the LLM, as it leverages a core strength\nof these types of models pattern-matching. The profiles provided a clearer, more organized\n84\nframework, and critically introduced the ”Recent Events” as a trigger for retrieval, this was the\napproach chosen to try and remedy the LLM knowledge cut-off from more recent knowledge.\nThe decision rule, however, still maintained a cautious stance, instructing the model to default\nto the safety of ”1-Yes” in cases of doubt or ambiguity.\n85",
    "caption": "Figure 37: Instruction V4 Results",
    "imagePath": "/images/figure_37_instruction_v4_results.png"
  },
  {
    "pageNumber": 83,
    "imageNumber": 38,
    "type": "figure",
    "folderName": "Figure 38 Instruction V5",
    "precedingText": "86\nThis new approach proved to be a big breakthrough. The results show a dramatically more\nbalanced and effective system as showed in Figure 37.\nThe General Knowledge Routing Accuracy saw a massive crucial improvement jumping\nfrom the previous 21.27% to 69.36%. For the first time, the model could correctly identify the\nmajority of the questions that did not require retrieval.\nThis improvement came with a small trade-off. The Retrieval Task Routing Accuracy saw\na slight dip from the near perfect levels of V2/V3, but still remaining very good at 91.88%.\nAs a result of this new balance, the Overall Routing Accuracy increased to 82.0% the highest\nand most effective level achieved so far indicating that the instruction was finally moving in the\nright direction.\nThe success of this version is best captured in the ”Routing System Vs. Baseline Model”\nanalysis for general knowledge questions. This version of the routing system achieved a 95.27%\nanswer accuracy, which was 20.35 percentage points better than the baseline for just model\nanswering on its own without any guiding instruction. By successfully re-framing the task to\nalign with the LLM’s natural capabilities, this instruction created a far more reliable and smart\nclassification system.\n87\n6.8.5\nInstruction V5: Final Refinement with a Guiding Principle",
    "caption": "Figure 38: Instruction V5",
    "imagePath": "/images/figure_38_instruction_v5.png"
  },
  {
    "pageNumber": 84,
    "imageNumber": 39,
    "type": "figure",
    "folderName": "Figure 39 Instruction V5 Results",
    "precedingText": "Building on the successful profile-based structure of V4, the fifth version was a final re-\nfinement aimed at maximizing reliability. The core structure of the two profiles remained un-\nchanged, but a critical addition was made to the Decision Rule. This new rule introduced an\nexplicit guiding principle to resolve the possible ambiguity: ”A slow but correct answer is al-\nways better than a fast but wrong one.”.\nThis principle served as a powerful tie-breaker, forcing the idea of accuracy on to the model.\nIt explicitly stated that if there was any ambiguity, or if a query even touched on the character-\n88\nistics from ”Profile 1” (like a specific name or date), it must default to the safety of retrieval.\nThis approach was designed to try and solidify the instruction’s focus on producing the most\ntrustworthy assessment possible, even at the cost of possible decrease in efficiency.\n89",
    "caption": "Figure 39: Instruction V5 Results",
    "imagePath": "/images/figure_39_instruction_v5_results.png"
  },
  {
    "pageNumber": 85,
    "imageNumber": 40,
    "type": "figure",
    "folderName": "Figure 40 Instruction V6",
    "precedingText": "The performance data shows that this refinement had a subtle but measurable impact, tuning\nthe model’s behavior as it was intended.\n90\nThe model became slightly more cautious. The Retrieval Task Routing Accuracy slightly\nincreased from 91.88% to 92.0%, which means the model identified 1553 of the 1688 queries\nthat required retrieval. This increased caution also resulted in a minor decrease in the General\nKnowledge Routing Accuracy, which shifted from 69.36% to 67.07%.\nThe model was now slightly more likely to send a simple query for retrieval if it contained\nany element of ambiguity. This adjustment was made based on the reasoning that the model\nmight still produce a correct answer even when retrieval is not strictly necessary. However, the\ninverse failing to retrieve when it is required almost always results in incorrect answers. As\na consequence of this shift, a slight dip in accuracy was observed, with the Overall Routing\nAccuracy decreasing to 81.1%.\nDespite the minor shifts in routing metrics, the final answer quality for general knowledge\nquestions remained identical to that of V4, with the routing system achieving a 95.27% . Simi-\nlarly to V4, this instruction resulted in a 20.35 percentage point improvement over the baseline.\nThis shows that V5 successfully reinforced the system’s reliability.\n91\n6.8.6\nInstruction V6: A Strategic Pivot to Efficiency",
    "caption": "Figure 40: Instruction V6",
    "imagePath": "/images/figure_40_instruction_v6.png"
  },
  {
    "pageNumber": 86,
    "imageNumber": 41,
    "type": "figure",
    "folderName": "Figure 41 Instruction V6 Results",
    "precedingText": "The final iteration, V6, represented a deliberate reversal from the ”accuracy-first” principle\nthat guided the previous version. The main goal was explicitly re-focused to ”AVOIDING un-\nnecessary document retrieval” and to ”reduce incorrect ’1-Yes’ classifications”. This instruction\nwas designed to test a high-efficiency approach, that prioritizes speed and resource conservation\nfor general knowledge questions.\nTo achieve this, the core logic was inverted. Non-retrieval (”2-No”) was made to be the\ndefault path, and the model was instructed to choose this approach unless a ”clear and definite\n’retrieve trigger’” was present on the query. The burden of proof was shifted: instead of de-\nfaulting to the safer retrieval in cases of ambiguity, the model now required compelling explicit\nreason to engage the retrieval system.\n92",
    "caption": "Figure 41: Instruction V6 Results",
    "imagePath": "/images/figure_41_instruction_v6_results.png"
  },
  {
    "pageNumber": 87,
    "imageNumber": 42,
    "type": "figure",
    "folderName": "Figure 42 Energy Costs vs. Correctness Scatterplot",
    "precedingText": "This strategic change had a profound and predictable impact on the system’s performance\neffectively trading accuracy for efficiency.\n93\nThe instruction’s goal was a success for the General Knowledge Routing Accuracy making\nit surge to its highest point across all the previous versions, reaching and impressive 90.47%\nas depicted in Figure 41. The system was now exceptionally skilled at identifying and directly\nanswering simple queries without using wasteful processing when not required.\nThis efficiency came at a significant and expected cost. The Retrieval Task Routing Accu-\nracy fell sharply from 92.00% to a measly 58.77%. By no longer erring on the side of caution,\nthe system failed to identify a large portion of queries that genuinely required the retrieval of\nexternal information.\nAs a result of this trade-off , the Overall Routing Accuracy dropped to 72.6%.\nInterestingly, despite the lower routing accuracy for complex queries, this approach achieved\nthe highest final answer accuracy on general knowledge questions with 97.18%. This was a\n22.26 percentage points over the baseline answers. This outcome shows that by correctly routing\na very high volume of simple questions to the direct-answer paths, the system maximized the\nLLm’s ability to leverage its own knowledge effectively, this was also helped with the Chain-\nof-Thought instruction that improved the base model answer without requiring any external\ninformation.\nThis last experiment shows the high degree of control that prompt engineering provides on\nsuch a system and even on the LLM’s responses. V6 is not inherently better or worse than V5, it\nsimply optimized and constructed with a different objective in mind. The choice between these\ntwo mature instructions depends entirely on the desired system behavior. Which aligns with the\npurpose of this research prioritizing efficiency whenever the trade-off proves to be worthwhile.\nV5 is the ideal choice for a system where reliability and avoiding factual errors are paramount,\nwhile the V6 version is superior for a system where efficiency and speed in handling common\nqueries are the up most concern.\n6.9\nAnalysis of Energy Consumption and Efficiency\nA holistic view of the system’s performance is best captured by plotting the total energy\nconsumed versus the number of correct answers that the system outputted. The resulting scat-\nter plot reveals a fundamental trade-off inherent in the system’s operation: achieving a higher\nnumber of correct answers of the provided dataset is directly correlated with increased in energy\nconsumption (see Figure42).\n94",
    "caption": "Figure 42: Energy Costs vs. Correctness Scatterplot",
    "imagePath": "/images/figure_42_energy_costs_vs__correctness_scatterplot.png"
  },
  {
    "pageNumber": 88,
    "imageNumber": 43,
    "type": "figure",
    "folderName": "Figure 43 Energy Costs vs. Correctness Scatterplot",
    "precedingText": "This is clearly lustrated in the progression from the baseline models, which occupy the\nlower-left quadrant of the graph which represents both the lowest energy consumption and the\nlowest correctness. Although one part of the baseline achieved high correctness, this may be\npartly due to the way correctness was assigned to queries requiring retrieval. In this evaluation,\nif retrieval was triggered for a query that required it, the system marked the answer as correct\nregardless of whether the retrieved content actually led to a correct response. This introduces\na significant limitation when interpreting the results. If correctness had instead been evaluated\nbased on the accuracy of the retrieved content itself, the score would likely have been much\nlower and more in line with the other two baseline results. Another important factor is that, since\nthe retrievable documents come from Wikipedia, many of them coincidentally align with ARC-\nEasy queries. For example, in Query 3, the retrieved document happens to contain information\nthat, through reasoning, leads to the correct answer. This pattern appears in multiple ARC-style\nqueries and may inflate the apparent effectiveness of the baseline. These two points boost the\nanswer correctness by a lot for this specific file thus should be looked at as a skewed result far\nfrom the truth.\nAt the upper-right , the system demonstrates its ability to improve both the model’s correct-\nness and the overall quality of the answers. However, one outlier stands out: System Analysis\nV1. This version reveals a particularly inefficient instruction, likely due to how open it was\nto interpretation. As a result, the model engaged in excessive reflection while still failing to\nchoose the appropriate approach for each query type. This led to a significantly lower number\nof correct answers compared to later, more refined instruction versions.\n95\nAn important takeaway is that the optimization process was not intended to reduce energy\nconsumption, but rather to maximize the productive use of that energy aiming to yield the most\naccurate results possible.",
    "caption": "Figure 43: Energy Costs vs. Correctness Scatterplot",
    "imagePath": "/images/figure_43_energy_costs_vs__correctness_scatterplot.png"
  },
  {
    "pageNumber": 89,
    "imageNumber": 44,
    "type": "figure",
    "folderName": "Figure 44 Domain Average Energy per Correct Answer",
    "precedingText": "Looking further into the energy dynamics of this project, an analysis of the average energy\nper query reveals a striking and consistent pattern, incorrect answers are consistently more\nenergy-intensive than correct ones. This suggests that incorrect answers are often the result of\ninefficient processing, such as retrieving irrelevant documents, pursuing flawed reasoning paths,\nor struggling to analyze conflicting information. In contrast, correct answers appear to follow\na more direct and energetically efficient path. With one exception, the straight model baseline,\nwhere the correct answers consume slightly more energy. This could be because, when the\nmodel is more confident in its answer, it tends to generate longer, more detailed responses,\nwhich in turn require more energy than the simpler, shorter incorrect ones.\n96",
    "caption": "Figure 44: Domain Average Energy per Correct Answer",
    "imagePath": "/images/figure_44_domain_average_energy_per_correct_answer.png"
  },
  {
    "pageNumber": 90,
    "imageNumber": 45,
    "type": "figure",
    "folderName": "Figure 45 Domain Average Energy per Incorrect Answ",
    "caption": "Figure 45: Domain Average Energy per Incorrect Answer",
    "imagePath": "/images/figure_45_domain_average_energy_per_incorrect_answ.png"
  },
  {
    "pageNumber": 111,
    "imageNumber": 46,
    "type": "figure",
    "folderName": "Figure 46 Average Energy Consumption per Query by",
    "precedingText": "Further analysis of the results, now split by the query’s original dataset (domain), reveals\nanother clear efficiency trend: queries related to ’General Knowledge’ consistently require more\nenergy than those from the ’Science’ domain. This pattern is still present for both correct and\nincorrect answers, as can be seen in both Figure 44, and 45. For the more refined instructions\n(V2 through V6), the energy cost for answering a general knowledge question is noticeably\nhigher than for a science question. This disparity likely stems from the increased complexity\nof the general knowledge queries, which typically require retrieval to be answered. This adds\n97\nfurther computational overhead, as the number of tokens the model must process increases\ndue to the inclusion of retrieved documents alongside the instruction. This occurs even after\norganizing the retrieved documents from most to least relevant, and removing any unnecessary\nexpressions that do not contribute to the quality of the response. On the other hand, in the\nScience domain, the number of input tokens is always lower since no retrieval is performed\nwhen the system functions correctly, thereby reducing the total input tokens.\nIn summary, the energy consumption analysis provides a comprehensive, multi-dimensional\nview of the system’s efficiency. It demonstrates that efficiency is not a single metric but a bal-\nance of multiple factors. The inherent complexity of the query’s domain sets a baseline for en-\nergy consumption, with ”General Knowledge” questions proving to be more resource-intensive\ndue to the required retrieval process needed to answer them correctly. UGiven this baseline, the\neffectiveness of the instructional prompt plays a pivotal role in how efficiently the energy is uti-\nlized. Well-calibrated instructions help guide the model down more efficient pathways, leading\nto correct answers at a lower average energy cost. This demonstrates that query editing could be\na promising area of study for improving model efficiency. While also proving that ambiguity or\nflawed logic results in wasted energy on incorrect outputs. Ultimately, this analyses reinforces\nthat the iterative refinement of prompts is not merely a quest for higher accuracy, but a method\nfor controlling the crucial trade-off between correctness and computational cost, while also al-\nlowing for the strategic selection of a system profile that best aligns with the desired balance of\nperformance and resource conservation.\n6.10\nDetailed Energy Consumption Profiles\n6.10.1\nOverall Energy Trends Across Instruction Versions\nA foundational analyses of the the system’s energy consumption begins with the average\nenergy consumed per query for each experimental version of the instructions, as showed in\nFigure 46. This shows a clear high-level comparison of the computational cost associated with\neach iteration of the instructions against the baseline models.\n98",
    "caption": "Figure 46: Average Energy Consumption per Query by File",
    "imagePath": "/images/figure_46_average_energy_consumption_per_query_by.png"
  },
  {
    "pageNumber": 113,
    "imageNumber": 47,
    "type": "figure",
    "folderName": "Figure 47 Total Energy Percentage Difference from",
    "precedingText": "The baseline models establish a lower bound for energy usage, using around 1.1 and 1.3\nWh per query. The introduction of the first routing instruction, V1, immediately results in a\nsignificant spike in energy consumption to nearly 2.0 Wh. This aligns with its characterization\nas a inefficient, open-ended prompt that likely caused extensive and unguided model processing.\nAs the instructions were refined from V2 to V5, the average energy fluctuated between\n1.45 and 1.9 Wh. This demonstrates that the added complexity of the routing system, even\nwhen optimized for accuracy, carries a consistent energy overhead compared to the simpler\nbaseline approaches as expected since it requires the model to output two responses for just\none query. Interestingly, Instruction V6, which was explicitly designed to enhance efficiency\nby trying to reduce the usage off unnecessary retrievals, results on the highest average energy\nconsumption at around 2.2 Wh. misconception during the creation of the instruction, as it was\ninitially assumed that the retrieval process would be the most energy-intensive. However, the\nresults show that although retrieval does contribute to energy consumption, it represents only a\nsmall portion of the overall cost. Another mistake made during the creation of this instruction\nwas that it ended up heavily relying on an internal chain-of-though process to answer general\nknowledge queries, which resulted in higher energy consumption. However, it also led to faster\nresponse times contradicting some of the assumptions made during the instruction’s design.\n99",
    "caption": "Figure 47: Total Energy Percentage Difference from Baseline",
    "imagePath": "/images/figure_47_total_energy_percentage_difference_from.png"
  },
  {
    "pageNumber": 115,
    "imageNumber": 48,
    "type": "figure",
    "folderName": "Figure 48 Total Energy Consumption by File (CPU vs",
    "precedingText": "This increased energy overhead is clearly illustrated in Figure 47. The graph shows that,\ncompared to the baseline, the more advanced routing systems consistently increase total energy\nconsumption by over 40%. This underscores a critical finding, that this implementation of\nan intelligent routing layer, while substantially improving answer accuracy and reliability, it\nalso presents a significantly and quantifiable trade-off in terms of computational and energy\ncost. While this was already considered at the start of this endeavor, the main idea is to try\nand compete with much larger models that, on average, require significantly more energy and\ncomputational power. A deeper analysis of this will be carried out at a later stage.\n6.10.2\nCPU vs. GPU: Deconstructing the Energy Cost\nTo try and understand the nature of the energy overhead introduced by the routing system,\nit is essential to deconstruct the total energy consumption into its primary hardware compo-\nnents, the Central Processing Unit (CPU) and the Graphics Processing Unit (GPU). The CPU\nnormally handles data processing, I/O operations, and logical orchestration, while the GPU is\nresponsible for parallel computations required by the model inference. The Figure 48 illustrates\nthis breakdown for each system version.\n100",
    "caption": "Figure 48: Total Energy Consumption by File (CPU vs GPU)",
    "imagePath": "/images/figure_48_total_energy_consumption_by_file__cpu_vs.png"
  },
  {
    "pageNumber": 117,
    "imageNumber": 49,
    "type": "figure",
    "folderName": "Figure 49 General Knowledge Avg. Energy when Strai",
    "precedingText": "A clear pattern emerges from the data, the baseline ”straight model”, which relies almost ex-\nclusively on model inference, shows the lowest relative CPU energy consumption. On the other\nhand, all subsequent versions that incorporate either forced retrieval or CoT or the intelligent\nrouting system exhibit an increase in the proportion of energy consumed by the CPU.\nConversely, the GPU energy consumption scales more directly with the complexity and\nlength of the generation required from the LLM. The V6 instruction, which was designed to\nfavor internal Chain-of-Thought reasoning over retrieval, shows the highest energy consump-\ntion driven by a massive increase in GPU usage. This shows that while avoiding CPU-heavy\nretrieval processes, version V6 shifts most of the burden to the GPU, requiring it to perform\nmore extensive and energy-demanding computations to generate the answers. As previously\nobserved, this also resulted in poorer performance compared to earlier iterations.\nThis analysis reveals that the choice of instruction foes not just how much energy is con-\nsumed, but also where it is used. A retrieval heavy strategy taxes the CPU, while a reasoning\nheavy strategy taxes the GPU. This distinction is critical for system optimization, as it shows\nhow different prompts and engineering strategies can create distinct hardware usage profiles.\n6.10.3\nThe Energetic Cost of Correcting Errors\nLooking beyond the general energy profiles, a more targeted analysis reveals the specific\nenergy consumption required for the system to add value that is, to correct an answer that the\nbaseline model would have gotten wrong. This scenario represents the core justification for this\nwhole approach.\n101",
    "caption": "Figure 49: General Knowledge: Avg. Energy when Straight Model is Incorrect & System is\nCorrect",
    "imagePath": "/images/figure_49_general_knowledge_avg__energy_when_strai.png"
  },
  {
    "pageNumber": 119,
    "imageNumber": 50,
    "type": "figure",
    "folderName": "Figure 50 Science Avg. Energy when Straight Model",
    "precedingText": "Figure 49 provides a quantitative analysis of the ’correction cost’ associated with ’General\nKnowledge’ queries. When the baseline fails while consuming around 0.95 Wh, the various\nrouting systems successfully provided a correct answer, although this came with a significantly\nhigher energy cost, ranging from 1.5 Wh (V2) to a peak of over 2.1 Wh (V1 and V6). Showing\nthat overcoming the baseline’s knowledge gaps via retrieval is an intensive operation. Though\nlike explained previously this lacks a better understanding since the evaluation of this domain\nis just that retrieval occurred, so the straight model never got a correct answer due to that fact.\nHowever, we can still compare them, and the V6 instruction stands out once again due to its high\nenergy cost. This is primarily attributed to its reliance on a detailed chain-of-thought process\nwhich, while somewhat effective, is significantly more computationally demanding.\n102",
    "caption": "Figure 50: Science: Avg. Energy when Straight Model is Incorrect & System is Correct",
    "imagePath": "/images/figure_50_science_avg__energy_when_straight_model.png"
  },
  {
    "pageNumber": 119,
    "imageNumber": 72,
    "type": "page",
    "folderName": "Page_119_Image_72",
    "precedingText": "The optimal position on this graph is at the top-left quadrant, which represents the highest\naccuracy with the lowest energy consumption. The bottom-right represents the inverse so the\nworst possible outcome . The baseline models establish two reference points. The first is the\n”straight model” (8B), sits in the lower left quadrant, confirming it is a low accuracy but highly\nefficient option. On the other hand , at the upper-right corner sits the ”straight model 14B\n8-bit” demonstrating the brute force approach as it achieves a very high correctness of nearly\n90%, however this comes with the cost of an enormous amount of energy at around 3.7 Wh per\nquery,making it by far the least efficient model\nThe iterative refinement of the routing instructions charts a clear journey toward the ideal\nquadrant. The initial, poorly tuned instructions (V1,V2,V3) show modest accuracy gains over\nthe 8B baseline but at a notable energy cost, placing them in the lower-middle of the graph.\nThe major breakthrough occurs with instruction V4 and later V5. These versions represent\nan optimal sweet spot, achieving a substantial leap in terms of correctness to over 83% while\nstill maintaining an average cost below 1.8 Wh. Most importantly, they deliver a significant\nportion of the accuracy gains of the 14B model while consuming less than half the energy.\nThe efficiency-focused V6 instruction pushes the accuracy to over 85%, however it also\nincreased the energy cost, moving the results slightly to the right. While it is the most accurate\nof the routing instructions, it begins to approach a point of diminishing returns in the accuracy-\n106\nefficiency trade off.\nTo conclude, this analysis of the quadrant provides the most complete validation of the re-\nsearch. It shows that an intelligent routing layer with a combination of a carefully engineered\nprompts are not just incremental improvements. They are part of a transformative strategy\nof optimization. By using carefully designed instructions to guide a smaller, more efficient\n8B model, the system achieves a performance profile that rivals a modelnearlyt twice its size,\nbut at a fraction of the computational and energy costs. This goes to show that architectural\nand instructional refinement can be a more sustainable and effective approach to high perfor-\nmance,rather than simply scaling up the model size.\n7\nExperimental Consistency and Reproducibility\nTo guarantee that the performance and energy consumption results represented in this thesis\nare both valid and reliable, a strict protocol was established to try to minimize the number\nof variables and create a consistent testing environment for all experiments. The following\nmeasures were systematically implemented to ensure that the results shown can be directly\nattributed to the changes in the system’s instruction design and model performance.\nFirst, the test system was maintained in a controlled and isolated state. To prevent interfer-\nence from many background tasks associated with other programs, and also with the operating\nsystem tasks, such as automatic updates or network-related tasks, the machine’s internet con-\nnection was physically disabled for the duration of all test runs. To further improve repeatability,\nprior to initiating any experiment, all non-essential background applications and services were\nfully terminated. The system was then left without any intervention after the scripts started up\nuntil they were finished and all the required data was collected. This ensured that the system’s\ncomputational resources were devoted exclusively to the experimental workload.\nAnother point taken to maintain the system repeatability was that a static software envi-\nronment was kept during the entire research process. The main components of the system,\nincluding the Python interpreter, the PyCharm IDE, and the HWiNFO monitoring utility, were\nnot updated after the initial setup. This strategy is crucial to eliminate the risk of major software\nupdates introducing performance variations that could skew the results.\nFinally, and most critically, the underlying code base for the query-routing system and\nmodel inference remained identical across all comparative experiments. When testing the ef-\nficacy of different instructions, the sole modification between each run was the content of the\nsystem instruction itself. This strict isolation of the independent variable ensures that all mea-\nsured differences in answer accuracy, latency and energy consumption are direct consequences\nof the prompt engineering strategy, rather than unintended changes in the software that supports\nit.\n107\n8\nChallenges and Abandoned Approaches\nIn the course of this research, some promising strategies were explored but ultimately aban-\ndoned due to practical constraints, resource limitations, or conflicts with the project’s core ob-\njectives. This section represents these methodological dead ends, as understanding what these\nwere and their causes could aid further research.\nOne of the initial strategies considered for enhancing the RAG component was the usage of\na hypothetical document similar to HyDE[40]. The technique, which involves generating a hy-\npothetical answer to a query to improve the semantic search for relevant documents, has shown\npromise in other projects. However, the original HyDE (Answer RQ2) paper recommended the\ngeneration of up to eight hypothetical documents per retrieval to achieve optimal performance.\nIn practice, this approach proved to be prohibitively expensive for the presented framework.\nThe computational overhead of generating eight separate documents before the retrieval was\ndone and the formation of the final answer would have dramatically increased both energy con-\nsumption and latency, with the latter representing a change that would make the framework\nbasically unusable with the current hardware. This would directly undermine the primary goal\nof creating a fast and efficient system for resource-constrained environments.\nAnother considered approach was the use of keyword injection to improve the model’s per-\nformance on specialized, domain-specific tasks. The hypothesis was that by programmatically\ninserting key technical terms (e.g., medical terminology for healthcare implementations) into\nthe prompt, the model could be guided toward a more accurate and contextually aware re-\nsponse. This idea was ultimately abandoned due to the immense data curation effort this would\nneed. To be effective, this strategy would necessitate the creation of a large, validated dataset\nmapping queries to the required keywords for each domain if various were to be involved in\ntesting. Getting hold and verifying the accuracy of this much data would be a substantial re-\nsearch project in itself, and it felt outside of the scope of this project which was focused more\ntowards the general public.\nFinally, the scope of model comparison was limited by the hardware available for this re-\nsearch. While the study successfully demonstrates the capabilities of an 8B parameter model\non a single GPU workstation, a more comprehensive analysis would have included benchmarks\nwith higher models as well, which would normally require enterprise-grade GPUs. Such tests\nwere not conducted due to a lack of access to these powerful computational resources. As a\nresult, the performance comparison remains focused on demonstrating the significant leap from\nsmaller models to the proposed SLM framework, rather than a broader spectrum of available\nopen-source models.\n108\n9\nConclusion\nThis thesis confronted the significant challenge of deploying powerful language models\nwithin environments constrained by limited computational resources, strict data privacy regu-\nlations, and tight budgets. The prohibitive cost and operational complexities of state-of-the-art\nLarge Language Models pose a significant barrier for small to medium enterprises and regulated\ninstitutions. In response, this research proposed and evaluated a novel framework (Answer\nRQ1) centered on a compact, 8 billion parameter Small Language Model. The core innova-\ntion of this work is a dynamic, adaptive query routing system that intelligently triages incoming\nqueries, selecting the most efficient and effective path be it a direct answer, a reasoning-intensive\nchain of thought, or knowledge augmentation using retrieval-augmented generation.\nThe results of this study show that architectural control and sophisticated prompt engineer-\ning can substantially bridge the performance gap between small, quantized models and their\nlarger, more resource-intensive counterparts. Through iterative refinement of the controller’s\ninstruction design, particularly with profile-based prompts, (Answer RQ3) the system achieved\nan accuracy exceeding 85% on reasoning-focused science questions. Critically, this perfor-\nmance was achieved with significantly greater energy efficiency than would be possible using\nlarger models. The detailed energy profiling revealed a direct correlation between incorrect an-\nswers and higher energy consumption, and also showed how different instructions strategically\nshift the computational load between CPU-intensive retrieval and GPU-intensive generation.\nThus confirming that a ”smarter, not bigger” approach is a viable path forward.\nThe implications of this research are both practical and strategic. It provides a tangible\nblueprint for developing high-quality, cost-effective, and secure question-answering system that\ncan operate on-premise on a single GPU workstation. This work (Answer RQ4) democratizes\naccess to advanced AI capabilities, enabling organizations without massive computational in-\nfrastructure to leverage the power of language models. Furthermore, it also contributes to the\ngrowing field of sustainable AI by demonstrating that performance and efficiency are not mu-\ntually exclusive.\n10\nFuture work\nThe framework developed in this thesis successfully demonstrates that an intelligently con-\ntrolled Small Language Model can achieve high performance in resource-constrained environ-\nments. This research also opens up several compelling points for future investigation that could\nfurther enhance the robustness, efficiency, and applicability of this approach.\nFirst a critical area for future research is the system’s resilience to irrelevant or misleading\ninformation within its knowledge base. The current experiments utilized a corpus that was\nsometimes relevant to the ARC dataset. A future study could involve another dataset that has\nno relevant information to the real world, this would split the realities of this dataset versus\n109\nthe ARC one, this way when retrieval occurred for ARC it wouldn’t improve the answer of the\nsystem. Though this is part of the advantage of a system like this, as it will always aim for the\nbest possible result.\nSecond, a novel and promising research direction based on the extracted results. A promis-\ning direction would be to explore the relationship between energy consumption and model hal-\nlucination. During this work, a correlation was observed between incorrect answers and higher\nenergy usage. This suggests the possibility of identifying a computational signature for hal-\nlucination. This could involve analyzing power draw and processing patterns to determine if\nnon-factual or fabricated responses can be detected in real-time based on their energy profile.\nIf a reliable correlation is established, this could lead to the development of a mechanism that\nflags potential hallucinations as they are being generated, allowing the system to intervene and\nreroute the query for correction, thereby improving the model’s trustworthiness.\nFinally, the adaptive routing principles pioneered here for SMLs could be extended to ad-\ndress known inefficiencies in much larger models. State-of-the-art LLMs, despite their power,\ncan sometimes enter unproductive reasoning loops, repeatedly processing the same logic with-\nout reaching a conclusion, which wastes significant computational resources. A future imple-\nmentation could adapt the controller to monitor the reasoning paths of an LLM. By detecting\nmajor semantic repetition or a lack of progress, the system could intervene to break the loop,\nperhaps by injecting new information via RAG or rewriting the prompt. This would not only\nprevent wasted computation and energy while also improving the reliability of LLMs in com-\nplex, multi-step reasoning tasks positioning this framework as a valuable tool for optimizing\nboth small and large language models.\n110",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_119_image_72.png"
  },
  {
    "pageNumber": 120,
    "imageNumber": 73,
    "type": "page",
    "folderName": "Page_120_Image_73",
    "precedingText": "Figure 54: Average GPU Energy Consumption by Domain.\n11\nImages\n111",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_120_image_73.png"
  },
  {
    "pageNumber": 121,
    "imageNumber": 51,
    "type": "figure",
    "folderName": "Figure 51 Science Avg. Energy when Straight Model",
    "precedingText": "A similar trend is observed in the Science domain, as shown in Figure 60. Correcting\nthe baseline in this approach also required a substantial energy investment, with the V6 again\nshowing the highest consumption at nearly 2.5 Wh. Thus reinforcing that the act of correcting\nthe responses regardless of the domain is inherently more energy demanding.",
    "caption": "Figure 51: Science: Avg. Energy when Straight Model is Incorrect & System is also Incorrect",
    "imagePath": "/images/figure_51_science_avg__energy_when_straight_model.png"
  },
  {
    "pageNumber": 121,
    "imageNumber": 74,
    "type": "page",
    "folderName": "Page_121_Image_74",
    "precedingText": "Figure 55: Avg. Energy of Analysis Instructions that were also INCORRECT when Baseline Failed.\n112",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_121_image_74.png"
  },
  {
    "pageNumber": 122,
    "imageNumber": 75,
    "type": "page",
    "folderName": "Page_122_Image_75",
    "precedingText": "Figure 56: Correctness Comparison between system versus a 14B Model.\n113",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_122_image_75.png"
  },
  {
    "pageNumber": 123,
    "imageNumber": 52,
    "type": "figure",
    "folderName": "Figure 52 Distribution of Energy Consumption per Q",
    "precedingText": "On the other hand Figure 51 analyzes the scenario where both the baseline and the system\nfailed to produce a correct answer. This represents the least efficient use of energy, showing that\n103\nthe system uses additional resources only to arrive at the same incorrect outcome. It is notewor-\nthy that the energy consumed in these failure cases is often comparable to and sometimes even\nhigher than the energy required to produce correct answers. As proof, the V6 system consumes\nover 2.6 Wh when it fails, more than it does for a successful answer which is around 2.5 Wh.\nThis might suggest that these incorrect answeres may result from the system pursuing particu-\nlarly complex, yet flawed, reasoning paths or even retrieving irrelevant information, leading to\nwasted resources.\n6.10.4\nEnergy Distribution and Consumption Predictability\nWhile the average energy consumption provides a useful high-level metric, a deeper un-\nderstanding of the system efficiency is required to access its consistency and predictability. A\nsystem with a low average cost is less desirable if it is prone to higher spikes. The boxplot in\nFigure 52 provides valuable insight into all the versions by visually representing the distribution\nof energy consumption per query for each one.",
    "caption": "Figure 52: Distribution of Energy Consumption per Query by File",
    "imagePath": "/images/figure_52_distribution_of_energy_consumption_per_q.png"
  },
  {
    "pageNumber": 123,
    "imageNumber": 76,
    "type": "page",
    "folderName": "Page_123_Image_76",
    "precedingText": "Figure 57: Average Energy Consumption by Domain.\n114",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_123_image_76.png"
  },
  {
    "pageNumber": 124,
    "imageNumber": 77,
    "type": "page",
    "folderName": "Page_124_Image_77",
    "precedingText": "Figure 58: Average CPU Energy Consumption by Domain.\n115",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_124_image_77.png"
  },
  {
    "pageNumber": 125,
    "imageNumber": 53,
    "type": "figure",
    "folderName": "Figure 53 Overall Performance Overview Correctness",
    "precedingText": "The baseline models, particularly the ”straight model”, presents the tightest distribution.\nThe small inter quartile range shows that most of the queries are processed using a very con-\nsistently and predictably amount of energy. This helps them in terms of reliability since from a\nresource planing perspective they are very predictable, though they show lower accuracy.\nOn the opposite side of the spectrum we have the initial routing Instruction V1, that demon-\nstrates extreme unpredictability. It has by far the largest inter-quartile range and a long upper\n104\nwhisker, indicating a massive variance in energy consumption. This proves that its ambiguity\nled to highly inefficient and erratic processing.\nThe following versions from V2 to V5 show a clear trend into an increasing predictabil-\nity. While they show a higher median energy cost than the baseline, their distributions become\ntighter and tighter as versions increase. This shows the core benefit of the iterative refinement\nprocess, reflecting precisely what was discussed previously as the instructions became more\nspecific and rule-based the model’s behavior became more constrained to the rules, therefore\nmore predictable. The number of high energy outliers that caused an increase in system’s re-\nsource expenditure also decreased indicating a more robust and stable approach.\nLastly the V6 instruction, designed for efficiency presents a rather unique profile. While\nits median energy is high, its distribution is relatively contained in comparison with V1, this\nsuggests that its Chain-of-Thought process, though energy intensive, is being applied more\nconsistently.\nUltimately, this analysis underscores that effective prompt engineering does more than just\nimprove accuracy, as this proves that it enhances operational predictability. A well designed\ninstruction set not only guides the model to the correct answer but also ensures that the model\ndoes so consistently while using a manageable level of resource consumption, which is a crit-\nical factor when picking and deploying such systems in the real world, especially in resource\nconstrained environments\n6.10.5\nOverall Performance Quadrant: Synthesizing Accuracy and Efficiency for ARC\nqueries\nThe culmination of this analysis is best visualized in the performance quadrant plot, which\nvisually presents each system’s final correctness rate against its average energy cost for the ARC\nqueries. The graph present in Figure 53, offers a clear overview of the trade-offs and situates\nthe performance of the 8B parameter model (DeepSeek-R1-Distill-Llama-8B) used as the base\nand all the routing systems routing systems against a crucial benchmark, a much larger 14B\nparameter model (DeepSeek-R1-Distill-Qwen-14B).\n105",
    "caption": "Figure 53: Overall Performance Overview: Correctness vs. Energy Cost for ARC queries",
    "imagePath": "/images/figure_53_overall_performance_overview_correctness.png"
  },
  {
    "pageNumber": 125,
    "imageNumber": 78,
    "type": "page",
    "folderName": "Page_125_Image_78",
    "precedingText": "Figure 59: Avg. Energy of Analysis Models that were CORRECT when Baseline Failed.\n116",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_125_image_78.png"
  },
  {
    "pageNumber": 126,
    "imageNumber": 79,
    "type": "page",
    "folderName": "Page_126_Image_79",
    "precedingText": "Figure 60: Science: Avg. Energy when Straight Model is Incorrect & System is Correct.\n117",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_126_image_79.png"
  },
  {
    "pageNumber": 127,
    "imageNumber": 80,
    "type": "page",
    "folderName": "Page_127_Image_80",
    "precedingText": "Figure 61: Average CPU Energy Consumption by Method.\n118\nFigure 62: Efficiency Score: Energy Cost of a Correct Answer for the system versus the 14B Model at the science domain.",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_127_image_80.png"
  },
  {
    "pageNumber": 128,
    "imageNumber": 81,
    "type": "page",
    "folderName": "Page_128_Image_81",
    "precedingText": "119",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_128_image_81.png"
  },
  {
    "pageNumber": 129,
    "imageNumber": 82,
    "type": "page",
    "folderName": "Page_129_Image_82",
    "precedingText": "Figure 63: Average GPU Energy Consumption by Method.\n120",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_129_image_82.png"
  },
  {
    "pageNumber": 130,
    "imageNumber": 83,
    "type": "page",
    "folderName": "Page_130_Image_83",
    "precedingText": "Figure 64: Average Energy Consumption by Method.\n121",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_130_image_83.png"
  },
  {
    "pageNumber": 131,
    "imageNumber": 84,
    "type": "page",
    "folderName": "Page_131_Image_84",
    "precedingText": "Figure 65: Science Domain: Avg. Energy when Baseline is Incorrect & System is also Incorrect.\n122",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_131_image_84.png"
  },
  {
    "pageNumber": 132,
    "imageNumber": 85,
    "type": "page",
    "folderName": "Page_132_Image_85",
    "precedingText": "Figure 66: Total Energy Consumption by Answer.\n123",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_132_image_85.png"
  },
  {
    "pageNumber": 133,
    "imageNumber": 86,
    "type": "page",
    "folderName": "Page_133_Image_86",
    "precedingText": "Figure 67: Average Energy Consumption in Science Domain by Correctness.\n124",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_133_image_86.png"
  },
  {
    "pageNumber": 134,
    "imageNumber": 87,
    "type": "page",
    "folderName": "Page_134_Image_87",
    "precedingText": "Figure 68: Average Energy for CORRECT Answers in Science Domain (with Counts).\n125",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_134_image_87.png"
  },
  {
    "pageNumber": 135,
    "imageNumber": 88,
    "type": "page",
    "folderName": "Page_135_Image_88",
    "precedingText": "Figure 69: Average Energy for CORRECT Answers in Science Domain.\n126",
    "caption": "No Caption Detected",
    "imagePath": "/images/page_135_image_88.png"
  }
]