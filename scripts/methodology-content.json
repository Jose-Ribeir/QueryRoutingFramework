[
  {
    "type": "image",
    "src": "/images/methodology/page_88_img_1.png",
    "alt": "Page 88 Image - Figure 25: Relationship between",
    "caption": "Figure 25: Relationship between"
  },
  "scores are also used, this evaluates wether the model correctly identified what was needed to\nreach the final answer. One important thing to note is that the paper argues that for the model to\nbe accurate it should get both the answer and the reasoning path correct. So a combined metric\nis proposed where a model only gets the full credit when success at both tasks is achieved, thus\na stricter and more meaningful evaluation of a model’s true reasoning capabilities is achieved.\n6    Methodology\nAs more powerful models hit the market with more expensive requirements, finding a model\nthat suits low-budget companies or entities is becoming harder. The problem occurs due to the\ngap in the performance of LMMs.  LMMs with more tokens will always tend to have greater\nperformance [8].  However, this performance could be increased with methods described pre-\nviously  like  HyDE  [40],  RAG  [35],  CoT  [67],  CAG  [43],  RAGCache  [44],  SKR  [52],  and\nContriever [49]. All of these methods aim to improve model performance, but they do so at the\ncost of increased computational overhead.  This overhead is variable, meaning that depending\non the method or combination of methods used, the system’s requirements and consequently its\nenergy consumption can change significantly. Since most of the studies are also on bigger mod-\nels a new approach will be taken to see if the results are similar to the bigger models. The key\npoints of the research are the efficiency and performance using some of the enhancing methods\ndescribed above. These will be paired with a small LLM meaning less than 36B tokens.\nFigure 25: Relationship between",
  "model size and monthly downloads[68]\n6.1    Overview of the Query Rewriting Flow\nThe use of Large Language Models (LLMs) in Information Retrieval (IR) systems has rev-\nolutionized the way people interact with and retrieve information.  Traditionally IR systems,\nsuch as search engines,  have evolved from term-based to neural network models,  which are\nuniquely suited to detect semantic nuance and contextual hints. Nevertheless, such systems still\npose challenges like query vagueness, data sparsity, and generation of responses that, although",
  "plausible, are actually incorrect.  LLMs, with their remarkable ability in language generation,\ncomprehension, and reasoning, offer a unmatched solution for the aforementioned challenges.\nLeveraging their huge pre-training on diverse text-based data, LLMs enhance various IR com-\nponents  like  query  rewriting,  retrieval,  reranking,  and  response  generation  and  enable  more\nprecise, contextual, and user-centric search experience as shown in Figure 26.\nBreakthroughs in cutting-edge large language models (LLMs) LLaMA have exhibited the\ncapability to accomplish demanding tasks and optimize information retrieval (IR) performance.\nThese models have not just the capacity to de-modularize user queries and retrieve relevant\ndocuments  but  also  render  lengthy  and  human-sounding  responses,  thereby  overcoming  the\nlimitations between traditional IR systems and the present user expectations.\nFigure 26: Traditional information retrieval architecture[68]\nThe proposed system will integrate a rewriter module that will be placed between the user\nquery q and the retriever similar to [45], this will",
  {
    "type": "image",
    "src": "/images/methodology/page_90_img_1.png",
    "alt": "Page 90 Image - Figure 27: Graphical representation of the system diagram[68]",
    "caption": "Figure 27: Graphical representation of the system diagram[68]"
  },
  "enable the injection of the retrieved documents\nas-well as the injection of instruction like CoT. This system works by first receiving the user\nquery,  which  will  be  processed  by  the  rewriter  module.   This  module  inserts  an  instruction\ninst that is used to determine the most suitable rewriting strategy to optimize the query.  The\ninstruction will prompt the LLM to evaluate which of the three approaches is the more suitable\nnormal response, RAG, or Chain-of-Thought.\nFigure 27: Graphical representation of the system diagram[68]\nIf the models concludes that it can answer directly then the model proceeds to generate the\nanswer without the need of a new generation, meaning only one hop. As for Chain-of-Thought,",
  "the approach works by injecting a command to try and force the model into thinking step by\nstep to obtain the answer. The final approach is Retrieval-Augmented Generation (RAG), which\nis also the most complex.  It begins by passing the query to a smaller model that generates an\nembedding representation of the query, aligned with a pre-embedded document collection.  A\nsimilarity comparison is then performed to retrieve the most relevant documents based on this\nembedding space.  Next, a rewriter is used to extract and pass only the most important parts\nof the retrieved documents, reducing the amount of irrelevant information passed to the model.\nFinally, a reranker prioritizes the most relevant documents so that they are presented first during\nthe generation process.   The last two aren’t necessarily needed so they can be turned off as\nneeded for more .\n6.2    Hardware and Software Environment\nThe experiments were co",
  "nducted on a high-performance workstation running windows 10,\nequipped with an Intel Core i7-13700KF processor, an NVIDIA RTX 4090 GPU, and 32 GB\nof DDR5 RAM operating at 6000 MHz. The software environment was managed using Python\n3.12 within a virtual environment (.venv), ensuring isolation and reproducibility of dependen-\ncies.   The  main  libraries  required  are  PyTorch  (with  CUDA  12.6  support)  and  the  Hugging\nFace Transformers library,  which was used to download and run the deepseek-ai/DeepSeek-\nR1-Distill-Llama-8B LLM. To monitor the system performance and power consumption during\nmodel inference and other system requirements,  HWInfo was employed.  Additional Python\nlibraries were installed as required to support the various aspects needed for the workflow. This\nsetup provides a robust and efficient platform for running and evaluating the proposed system.\n6.3    Rewriting Approaches\n6.3.1    Straight LLM\nThis approach uses the initial model response as the final answer.  It is designed for ques-\ntions that are too simple to require more complex methods.   From an efficiency standpoint,\nminimizing processing steps is desirable, and using the first response avoids redundant genera-\ntion. As the simplest approach, it does not enhance the model’s answer but provides a baseline\nfor comparison.\n6.3.2    Chain-of-Thought\nThe Chain-of-Thought (CoT) approach enhances model performance by encouraging it to\nreason through the logical steps of a query.  In some models, this can be achieved with simple\nprompts such as ”Please reason step by step, and put your final answer within a box.”, as seen\nwith DeepSeek models [69]. Other m",
  {
    "type": "image",
    "src": "/images/methodology/page_92_img_1.png",
    "alt": "Page 92 Image - Page 92 Image",
    "caption": "Page 92 Image"
  },
  "This approach has been widely adopted to improve and enhance the reasoning capabilities of\nLLMs. One of the reasons for this adoption comes from the simple requirements since it doesn’t\nneed much change on already-built systems. This method has shown significant improvements\nin certain tasks that require logical thinking and contextual understanding.  For instance, some\nbenchmarks often use both with and without Chain-of-Thought prompting to show the direct\nimpact of the on reasoning performance [70], [58].  The proposed system selects this option\nwhen the initial model response indicates that Chain-of-Thought reasoning is required.  This\nmethod involves two inference steps: the first allows the LLM to assess whether CoT is neces-\nsary, and the second passes a modified query containing the appropriate instruction to prompt\nthe model to reason through the problem.  Finally the model answer the instruction that now\ncontains a CoT instruction.\n6.3.3    RAG\nThis approach is used to retrieve documents or passages that are relevant to the user’s query.\nThis is done by embedding the user query into a vector , which is then compared to the vectors\nof stored documents.  Based on a top-K similarity ranking,  the most relevant documents are\nretrieved. These documents can then be refined by removing non-essential parts, aiming to make\nthe resulting content as concise and relevant as possible. Following refinement, the documents\nmay be reranked, as language models tend to focus more on the initial tokens in the input [71].\nSimilar to the Chain-of-Thought method, this approach also requires two hop",
  "s: The first hop\nacts as a reflection step to determine whether the LLM deems document retrieval necessary; The\nsecond hops consists on passing the query as well as the retrieved documents to the LL;\n6.3.4    Selecting the Approach\nThe system has three possible approaches to choose from: Retrieval R, CoT C, and Straight\nanswer S.  Since only one of these approaches can be selected for a given query, a selection\nmechanism is required. This selection is performed using the model (M) and the content of the\nquery (q), this is shown in Figure 29\nFigure 28: Analysis Prompt\nFigure 28 presents the analysis prompt responsible for this selection.  This instruction can\nbe divided into two parts.  The first part is responsible for the Straight Answer (S), it asks the",
  "model to respond directly to the query if it is confident it can do so correctly.  This approach\nis  not  only  the  fastest  as  well  as  the  simplest,  as  the  system  does  not  need  to  perform  any\nadditional steps to achieve the response. The second part of the prompt is responsible to induce\nthe model into deciding between Retrieval (R) or CoT (C). This is done by asking the model if\nit would benefit from retrieval of documents. If the response is ”Yes” then Retrieval process is\ntriggered, and all subsequent steps such as reranking and refining are also executed.  However,\nif the model’s answer is ’No’, the system interprets this as a lack of confidence in the initial\nresponse.  To improve the quality of the final output, the system then forces the model to use\nChain-of-Thought (CoT) reasoning.\nTo translate the model’s textual output into a definitive choice, the syst",
  {
    "type": "image",
    "src": "/images/methodology/page_94_img_1.png",
    "alt": "Page 94 Image - Page 94 Image",
    "caption": "Page 94 Image"
  },
  "em employs a so-\nphisticated post-processing and also voting mechanism rather than simply looking for a single\nkeyword. This implementation is a critical part that enables the system to use a decision-making\nprocess robust and resilient to multiple formatting variations. The process unfolds as follows:\n1.  Initial Cleaning:  The raw output from the model is first decoded and also normalized\nto standardize characters and spacings.  Any preliminary ”Chain-of-Thought” reasoning,\nwhich is normally enclosed in special tags like ” < /think > ”,  is stripped away to\nisolate the final answer.\n2.  Check for a Direct Answer(early exit): Before classifying the need for retrieval, the sys-\ntem first checks if the model already provided the final answer on it’s first analysis.  It\nspecifically looks for a ”\\\\boxed{...}” pattern containing a single alphabetic character\n(e.g., ”\\\\boxed{A}”). If this pattern is found, the system assumes it corresponds to strat-\negy S. The process then halts immediately, returning the response as the final output. The\n’method used’ is set to ’none’, indicating that no additional processing was necessary.\n3.  A Voting System For Decision Making:  Instead of a simple parse,  the system incurs a\nscoring mechanism to ”vote” on the best possible approach.   It initializes counters for\nboth R and C.\n4.  Identifying Strong Signals:  The code meticulously scans the output for high-confidence\nindicators.  A ”\\\\boxed{1}” pattern is considered a strong, explicit vote for R, adding a\nsignificant score of 100 points. Similarly, a ”\\\\boxed{2}” is a strong vote for C.\n5.  Identifying Secondary Signals: To enhance robustness and acc",
  "uracy, the system also looks\nfor secondary indicators. The presence of the word ”yes”(case-insensitive) in the analysis\nalso adds 100 points to the R counter.  This ensures the model’s intent is captured even\nwhen the model fails to use the precise boxed format.\n6.  Final Decision: After analyzing the entire output, the approach with the highest accumu-\nlated score is chosen as the ”suggested\nmethod”.  This multi-signal, voting-based mech-",
  "anism makes the classification resilient to minor formatting errors from the model, thus\nprioritizing a correct interpretation over strict adherence to a single output format.\n6.4    Dataset Augmentation\n6.5    Query-Answer Validation\nThe proposed validation framework is comprised of a three-stage process that is used to sys-\ntematically differentiate between high-quality and problematic queries.  This hybrid approach\nintegrates automated analysis with a human-in-the-loop component.\n6.5.1    Automated Preparation\nThe data is ingested and categorized using a python script. The script then splits the queries\ninto those that require retrieval and those that do not.   Depending on the configuration,  one\nof these two sets is selected for further processing.   This set is then divided into batches of\nn queries, a value that can be configured in the settings.  Each time the Alt key is pressed, a\nnew batch is compiled and copied to the clipboard, ready to be pasted into the LLM chat.  The\ngenerated batch already includes the appropriate instruction, tailored to the selected query type.\n6.5.2    AI-Powered Triage\nThe LLM analyzes the provided query and determines if the options are clear and the ground",
  {
    "type": "image",
    "src": "/images/methodology/page_96_img_1.png",
    "alt": "Page 96 Image - Page 96 Image",
    "caption": "Page 96 Image"
  },
  "truth is correct, paying special attention to the number of correct options.  If these prove to be\ncorrect then a Green Flag is assigned.  This flag contains an emoji so the human supervisor\ncan identified the queries easily.  In this workflow, a Green Flag is treated as a definitive pass,\nmeaning these queries are not further reviewed by the supervisor.\nA Red Flag, on the other hand, is assigned to any query that fails to meet the criteria for\nexample, due to ambiguous wording or other inconsistencies.  Unlike the Green Flag, a Red\nFlag does not automatically discard the query. Instead, it signals that the query requires human\nverification.  Although the model provides an explanation for why the Red Flag was assigned,\nthe final decision rests with the human supervisor.\n6.5.3    Human Verification\nThe human validator is instructed to only look at the Red Flags queries to maintain attention\nand reduce the number of queries a human needs to verify. The job of the validator falls into the\nverification of the LLM’s justification for the Red Flag, this verification can lead to the validator\ndoing some research on the query and the correctness of the expected output.  In case a query\nfails this verification it is deemed unusable and gets removed from the dataset, a new one is\nadded to it’s place that passes this validation.",
  "6.5.4    Dataset formation\nThe output of all the previous points are fully answerable queries that are free of ambiguity.\nThis is used for two distinct datasets,  the ARC-easy and the HotPotQA, the results are then\njoined to combine queries that require retrieval as well as those needing step-by-step reasoning.\nThi",
  "s new dataset is specially designed for systems that can decide what approach is required for\nthe best possible outcome. However due to the lack of complexity of the ARC-easy queries this\nis more suited towards weaker than state-of-the-art, more around 32B tokens and lower.\n6.6    Power Data Collection\n6.6.1    GPU\nTo compare the different components of the system, one important aspect is energy con-\nsumption.  However, collecting this data on Windows is not straightforward.  Due to hardware\nlimitations, all measurements will be performed using software tools that query system com-\nponents to report energy usage at a given moment.  Starting with GPU the power consumption\nis collected using nvidia-smi [72], this tool acts as bridge that queries the GPU driver directly\nand converts the retrieved data into a useful unit, these data points are collected every 15ms,\nhowever due to driver overhead the real gap is around 50ms.  The data collected according to\nNvidia NVML [73] is related to TBP or total board power, meaning that the consumption of\nVRAM and all the necessary components to support the GPU die itself are included in that\npower measurement.  This is important as the power consumption of VRAM is highly used by\nLLM’s during inference. This data is collected directly by a purpose-built library that monitors\nGPU power draw. The power measurements are added directly into the JSONL file, which also\ncontains the model’s response and all relevant metadata for later analysis.\n6.6.2    CPU\nThe other power consumption metric is the CPU package. This measures the power used by\nthe CPU chip itself, excluding any power consumed by supporting",
  {
    "type": "image",
    "src": "/images/methodology/page_98_img_1.png",
    "alt": "Page 98 Image - Page 98 Image",
    "caption": "Page 98 Image"
  },
  "components such as voltage\nregulator modules (VRMs), the chipset, and other peripherals. Although it would be interesting\nto measure the full system power draw, this requires specific hardware that was not available for\nthis project. This CPU package draw isn’t super easy to obtain in a Windows system so the use\nof a proprietary tool called HWiNFO this tool offers a logging feature that creates a CSV with\nall the collected data, this collection is done every 20ms theoretically however in reality there\nare a lot of times where it takes more than that, but on average it takes 31ms.\nBecause the CSV is generated by a third-party tool, it is only available at the end of the\nrun when logging is manually stopped.  To align energy data with query execution, a script is\nused to match each query’s start and end times (from the previously mentioned JSONL file)\nwith the corresponding timestamps in the CSV. All data points that fall within a query’s time",
  "window are extracted. Using these points and their timestamps, the trapezoidal rule is applied to\napproximate the energy consumption.  This method works by summing the areas of trapezoids\nunder the power curve, providing an estimate of the integral of power over time (watts × time).\nThis approach compensates for the irregular intervals in data retrieval by HWiNFO. The result\nis an estimate of energy consumption by the CPU package, expressed in watt-hours. This value\nis then added to the JSONL file, along with the total energy consumption calculated as the sum\nof the GPU’s Total Board Power (TBP) and the CPU package power.   GPU power usage is\nalready recorded in the JSONL, and the trapezoidal",
  "rule is applied in real time during inference\nto account for variations in the intervals between data points.\nFigure 29: Jsonl Data Structure\nIn Figure 29 is depicted the final JSONL structure.  The structure is devised into four main\nparts Query Information, Ground Truth, AI Prediction, and Performance & Metrics each being\nfinalized at different stages. The system initially gets a JSONL with just the Query Information\nand the Ground Truth. Part 2 is formed by the responses and metrics from the system solution.\n6.7    Evaluation Framework\nThe evaluation framework may vary depending on the specific domain being tested.  This\nis because some domains might require different metrics to understand the real capabilities of",
  "the model in a given task [74].  Another key point is the need to access each part individually\nas well as combined.  This is key in accessing the quality of the system and understand which\npoints could be improved for a better combined performance.One of the most important metrics\nacross all components of the system is efficiency, as it helps to assess how each part contributes\nto the overall energy consumption. What will be compared and obtained is the following:\n•  System answers to full dataset.\n•  Straight model answers to full dataset.\n•  Forced CoT answers to full dataset.\n•  Forced Retrieval answers to just full dataset.\nThe full dataset consists of 3000 queries, with 1312 originating from the ARC-Easy dataset\nand the remainder from the DragonBall dataset.  The ARC-Easy dataset was selected because\nit primarily contains simple reasoning queries that the model can answer without relying on\nexternal knowledge,",
  {
    "type": "image",
    "src": "/images/methodology/page_100_img_1.png",
    "alt": "Page 100 Image - Page 100 Image",
    "caption": "Page 100 Image"
  },
  "although a few questions do require more complex reasoning.  The Drag-\nonBall dataset, on the other hand, was chosen because it mostly includes queries that necessitate\nretrieval, with some also requiring advanced reasoning to be answered correctly.\nTogether, these datasets offer a comprehensive evaluation of the system’s capabilities.  Ad-\nditionally, if the model under study is a larger one, the ARC-Easy portion can be replaced by\nARC-Challenge, which features more complex queries that demand deeper reasoning than its\nsimpler counterpart.\n6.7.1    Retrieval Performance Metrics\nDue to the nature of this work, the quality of the retrieval itself will not be evaluated, as it\ndepends on the specific RAG method employed.  Instead, the evaluation will focus on whether\nretrieval occurred when it was necessary.  This will be represented as a binary outcome:  1 if\nretrieval was triggered, and 0 if not.  However, what needs to be evaluated is a direct compari-\nson between the energy consumption of the proposed system and that of a baseline that always\nperforms retrieval.  This comparison is important because the system requires two hops to de-\ncide whether to retrieve, whereas always retrieving eliminates the need for this decision-making\nstep. However, since the dataset includes questions both with and without the need for retrieval,\nan evaluation will be conducted to determine whether the system results in lower energy con-\nsumption.  This is based on the premise that retrieval is not necessary for every query, and the\nsystem may avoid unnecessary retrieval steps.  The quality and correctness of the answer will\nalso be evaluated. This is",
  {
    "type": "image",
    "src": "/images/methodology/page_101_img_1.png",
    "alt": "Page 101 Image - Page 101 Image",
    "caption": "Page 101 Image"
  },
  "important because, in cases where retrieval is not necessary, the sys-\ntem may still retrieve documents from the database that are not directly relevant to the query.\nAs a result, these retrieved passages may not contribute meaningfully to the answer.",
  "6.7.2    Straight Model\nThis approach will be evaluated in multiple parts.  The first aspect is whether the answer is\ncorrect specifically, whether the model’s response matches the ground truth option.  Next, the\nevaluation will check if the model correctly identified queries that should be answered without\nretrieval.   This part is linked to the previous one:  if the answer is correct without retrieval,\nthe classification is also considered correct.  Additionally, a comparison will be made between\nanswers generated with and without forced Chain-of-Thought (CoT) prompting to determine\nwhether the increased energy consumption associated with CoT leads to improved answers or\nif the same responses would have been provided without it.  This will be done to understand\nif the model is guessing correctly wether it can answer the question directly or not.  And will\nalso provide consumption metrics that will be compared in order to understand its efficiency.\nCoT will tend to be more accurate but it also requires more energy due to the thinking phase of\ngeneration.\n6.7.3    Chain-of-Thought Reasoning Performance Metrics\nThe  metrics  for  CoT  are  the  same  as  those  used  for  the  straight  model,  as  both  will  be\ndirectly compared.\n6.7.4    Automated Evaluation Script\nTo implement the evaluation metrics described, particularly for understanding the correct-\nness of the model’s answer",
  "s, an automated script was employed.  This script is responsible for\nprocessing the model’s output for each query, which is stored in a JSONL file format. The pri-\nmary goal is to systematically determine if the model’s final answer matches the ground truth,\nespecially for ARC queries which are multiple-choice questions.\nThe  core  to  this  evaluation  lies  in  a  multi-step  parsing  strategy  designed  to  intelligently\nextract the final answer from the model’s potentially complex and verbose output, similarly to\nhow a human user would read the output and understand which character is the one that the\nmodel chose. The process is as follow:\n1.  Definitive Answer Extraction:The script first searches the model’s prediction for the\nmost explicit answer format, such as ”\\\\boxed{B}”. This format is often used by models\nto clearly delineate the final answer, so finding it is the most reliable sign. However, since\nthis work is conducted using smaller models, they often do not follow patterns very well\nand may provide the answer surrounded by verbose context reflecting their reasoning.\n2.  Pattern-Based Fallback:  If the first pattern is not found, the script then looks for common\nnatural language phrases that indicate a final answer, like ”The answer is B” or ”Answer:\nB”.  It is designed to take the last match it finds, operating on the assumption that any\nreasoning or changes of mind would occur before the final declared answer.",
  "3.  Positional Fallback:  As a final strategy, if neither of the above patterns yields a result,\nthe script searches for all standalone capital letters (A, B, C, or D) within the response.\nSubsequently, the",
  {
    "type": "image",
    "src": "/images/methodology/page_103_img_1.png",
    "alt": "Page 103 Image - Page 103 Image",
    "caption": "Page 103 Image"
  },
  "system selects the final occurrence as the intended answer, assuming it\nreflects the model’s ultimate decision.  This servers as a robust fallback for cases where\nthe model might provide the final answer without any formatting.\nOnce one answer is extracted through this hierarchy of methods, it is compared directly with\nthe ”ground\ntruth” value from the JSONL. A new metric, ”correctanswerarc”, is added to the\ndata inside the metrics section, this then gets marked as ”true” if they match and ”false” if not.\nFurthermore, this script is also responsible for the automation of the retrieval performance\nmetric.  It checks if the ”references” inside the ”ground\ntruth” section contains any references\nthat indicate that retrieval was needed.  It then cross-references this with the ”methodused”\nvalue that represents the path the system chose.  If the model used ”retrieval” for a question\nthat required it, a ”retrievedcorrectly” metric is marked as ”true” on the same metrics section,\naligning with the binary evaluation approach mentioned previously.  This automated process\nensures a consistent and scalable way to apply the defined evaluation criteria across the entire\ndataset.\n6.7.5    Efficiency Metrics\nEfficiency is a metric that can be measure in various ways depending on the focus of the\nevaluation.  This could be power consumption, cost-effectiveness, or scalability, each of which\nplays a central role in the direction of this thesis. Power consumption will be measured as watts\nper query . This metric is important due to the multiple processing steps involved in generating\neach output. However, it will not reflect the total system power cons",
  {
    "type": "image",
    "src": "/images/methodology/page_104_img_1.png",
    "alt": "Page 104 Image - Page 104 Image",
    "caption": "Page 104 Image"
  },
  "umption, as only CPU and\nGPU usage will be measured due to hardware limitations.\nThe cost-effectiveness will be calculated by dividing the cost of the required hardware com-\nponents by the system’s performance.   This allows for a direct comparison between this ap-\nproach and more powerful alternatives.  Such comparisons can be conducted through a series\nof tests, similar to the benchmarks referenced in model comparison section.  Scalability will\nbe assessed by analyzing the requirements needed when using a larger model or when more\ndocuments and keywords are added to the system.  This will likely be the most difficult metric\nto evaluate directly, as I do not have access to more powerful hardware. However, I will attempt\nto estimate scalability based on data and findings from other researchers.\n6.8    Optimizing Query Classification through Iterative Prompt Refinement\nThe performance of a LLM is fundamentally linked to the clarity and quality of the instruc-\ntion provided. In this system, where the initial goal is to classify a user’s query into one of three\npaths retrieving external documents, reasoning step by step, or simply using the model’s first",
  "response as the answer (both relying on the model’s internal knowledge) the construction of the\ninstruction plays a crucial role.  By carefully designing this part of the process, we can guide\nand control the model’s decision-making behavior.  To determine the most effective approach\nfor this classification task, a series of instructions were developed and tested, evolving from a\nsimple open-ended prompt to a highly structured one that involves a fully rule based framework.\nThis sec",
  {
    "type": "image",
    "src": "/images/methodology/page_105_img_1.png",
    "alt": "Page 105 Image - Figure 30: Instruction V1",
    "caption": "Figure 30: Instruction V1"
  },
  {
    "type": "image",
    "src": "/images/methodology/page_105_img_2.png",
    "alt": "Page 105 Image - Figure 30: Instruction V1",
    "caption": "Figure 30: Instruction V1"
  },
  "tion will analyze the iterative refinement process of the instructions in detail, eval-\nuating the performance of each version. The evaluation on this section focuses on key metrics,\nsuch as: Routing Accuracy (the model’s ability to correctly chose ’retrieval’ or ’no-retrieval’),\nAnswer Accuracy (the correctness of the final response), and Efficiency (measured in energy\nconsumption) though this metric will be more thoroughly looked at at a later stage.  By exam-\nining the trade-offs between these factors at each stage, we can identify the best practices for\nguiding an LLM in a complex classification task.\n6.8.1    Instruction V1: A Simple Baseline",
  "The initial instruction, V1, was designed as a minimal baseline to assess the feasibility of the\nproposed approach. This instruction directly asks the model for a binary classification (”Would\nthis Query benefit from the retrieval of documents?”) Figure 30, with a heavy emphasis on the\noutput format rather than the decision-making logic.  This lack of explicit criteria forced the\nmodel to rely almost totally on its internal, pre-existing biases to interpret the query’s needs.",
  "As the performance data from the evaluation reveals, this approach was highly inconsistent\nand ultimately unreliable.   The model developed a strong bias against retrieving documents,\nleading to a significant imbalance in the classification.  This likely occurred due to the smaller\nmodel lacking sufficient internal knowledge, as it is less capable than state-of-the-art models.\nWhile the system was adept at identifying general knowledge questions (those",
  "not requiring\nretrieval),  it correctly avoided retrieval 98.17% of the time,  refer to Figure 31.   However,  it\nalmost completely failed at the inverse task, only correctly choosing retrieval 16.59% for the\nqueries that required it. As a result, the overall routing accuracy was limited to just 52.3%.",
  "questions that ideally required retrieval to answer correctly, the model incorrectly routed 1408\nof them to be answer without retrieving.  This fundamental failure to identify questions need-\ning external knowledge confirmed that a simple, unguided prompt is insufficient for creating\na reliable query-routing system.  While this appears to be true for the chosen model size, fur-\nther testing is necessary to determine whether this limitation persists in larger models or if they\nperform better on this task.\n6.8.2    Instruction V2: An Aggressive, Safety-First Heuristic",
  "To  counteract  the  significant  bias  against  retrieval  observed  on  the  first  approach  Figure\n30,  the  second  iteration  introduced  a  strong,  explicit  bias  towards  retrieval.   Instruction  V2\nFigure 32, framed the model as an ”expert query analyzer” with the primary goal of eliminating\nincorrect answer by trying to force document retrieval for any non-trivial query.  It established\nretrieval (’1-Yes’) as the default assumption,  permitting a direct answer (’2-No’) only if the\nquery met a very strict and narrow set of criteria: it had to involve exclusively ’globally famous\nentities’ and ask for a single, static, and universally known fac",
  {
    "type": "image",
    "src": "/images/methodology/page_107_img_1.png",
    "alt": "Page 107 Image - Page 107 Image",
    "caption": "Page 107 Image"
  },
  "t.  This ”safety-first” heuristic\nwas designed to try and minimize the risk of factual errors originating from the model’s internal\nknowledge.\nThis agressive change dramatically inverted the model’s behavior. The Retrieval Task Rout-\ning Accuracy skyrocketed from 16.59% to 98.22%, demonstrating that the model could now\nreliably identify questions that required external documents.   Out of 1688 such questions,  it\ncorrectly chose ”retrieval” for 1659 of them.\nHowever, this success came at significant cost to efficiency and accuracy on the opposite\ntask,  with  results  that  were  almost  predictably  inverse  to  those  of  the  first  instruction.   The\nmodel’s ability to recognize simple, general knowledge questions plummeted. This can be seen\non the General Knowledge Routing Accuracy that fell from 98.17% to a mere 26.07%.  The\nsystem was now incorrectly choosing to retrieve documents for the vast majority of the queries\nthat did not need it. This over-correction is properly showed on the decision matrix, where 970\nout of the 1312 No Retrieval queries, were wrongly sent down the retrieval path.\nAlthough the Overall Routing Accuracy improved to 66.7%, this aggressive heuristic proved\nto be an over-correction. While it successfully enforced the retrieval of necessary information,\nit failed to account for cases where retrieval was unnecessary.  This led to inefficient and often\nredundant processing for a large number of relatively simple queries that the base model could\nhave answered directly. This showed perfectly that while a strong default can steer the model’s\nbehavior, a purely aggressive approach is too rigid and fails to bala",
  {
    "type": "image",
    "src": "/images/methodology/page_108_img_1.png",
    "alt": "Page 108 Image - Figure 33.",
    "caption": "Figure 33."
  },
  "nce accuracy with efficiency.\nAll of this is evidenced by the results shown in Figure 33.",
  "6.8.3    Instruction V3: Introducing Balanced Criteria",
  "The third iteration, Instruction V3, tried to strike a balance between the aggressive retrieval\nstrategy of V2 and the passive approach of V1. The goal was to try and improve efficiency by re-\nducing unnecessary retrievals without sacrificing the accuracy gains made on complex queries.\nThe key refinement was the introduction of explicit, positive categories for non-retrieval (”2-\nNo”).  For the first time , the model was given clear examples of queries that were meant to be\nanswer directly, such as ”Universally Known Facts”, ”Creative Tasks”, and ”General Explana-\ntions”.\nThis structured, two-step process first checks for a simple case, and only then defaulting to\nretrieval on more complex queries.  This proved to be a significant step in the right direction.\nThe model was no longer forced into an overly aggressive default and was instead required to",
  "reason through its decision-making process to select the appropriate path.",
  "The results depicted in Figure 35 reflect this new found balance. The Retrieval Task Routing\nAccuracy remained exceptionally high at around 98%, indicating that the safety-first principle\nfor complex questions was successfully maintained. The model correctly identified 1655 out of\n1688 queries that required retrieval.\nCrucially, the opposite task had been the main weakness of the previous iteration, and this\nremained true in the current version, with results d",
  {
    "type": "image",
    "src": "/images/methodology/page_109_img_1.png",
    "alt": "Page 109 Image - Figure 36: Instruction V4",
    "caption": "Figure 36: Instruction V4"
  },
  "eteriorating further the General Knowledge\nRouting Accuracy dropped from 26.07% to 21.27% on the General Knowledge Routing Accu-\nracy. This can also be seen on the number of times that the model picked ”retrieval” 1033 of the\n1312 general knowledge questions that don’t require it.",
  "6.8.4    Instruction V4: A Shift to Profile-Based Classification",
  "The mixed results presented on the previous iteration highlighted a potential weakness in\nthe sequential, rules-based checklist approach.  This new instruction V4 represented a major\nconceptual shift, this time re-framing the task following a procedure to a more holistic classi-\nfication exercise.  Instead of the previous step-by-step process, the model was now tasked with\nmatching the user’s query to one of two detailed profiles:  ”Profile 1:  Retrieval Required” or\n”Profile 2: Direct Answer Sufficient”.\nThis new profile-based structure is more intuitive for the LLM, as it leverages a core strength\nof these types of models pattern-matching.   The profiles provided a clearer,  more organized",
  "framework, and critically introduced the ”Recent Events” as a trigger for retrieval, this was the\napproach chosen to try and remedy the LLM knowledge cut-off from more recent knowledge.\nThe decision rule, however, still maintained a cautious stance, instructing the model to default\nto the safety of ”1-Yes” in cases of doubt or ambiguity.",
  "This new approach proved to be a big breakthrough. The results show a dramatically more\nbalanced and effective system as showed in Figure 37.\nThe General Knowledge Routing Accuracy saw a massive cr",
  "ucial improvement jumping\nfrom the previous 21.27% to 69.36%.  For the first time, the model could correctly identify the\nmajority of the questions that did not require retrieval.\nThis improvement came with a small trade-off.  The Retrieval Task Routing Accuracy saw\na slight dip from the near perfect levels of V2/V3, but still remaining very good at 91.88%.\nAs a result of this new balance, the Overall Routing Accuracy increased to 82.0% the highest\nand most effective level achieved so far indicating that the instruction was finally moving in the\nright direction.\nThe success of this version is best captured in the ”Routing System Vs.  Baseline Model”\nanalysis for general knowledge questions. This version of the routing system achieved a 95.27%\nanswer accuracy,  which was 20.35 percentage points better than the baseline for just model\nanswering on its own without any guiding instruction.  By successfully re-framing the task to\nalign with the LLM’s natural capabilities, this instruction created a far more reliable and smart\nclassification system.",
  "6.8.5    Instruction V5: Final Refinement with a Guiding Principle",
  "Building on the successful profile-based structure of V4, the fifth version was a final re-\nfinement aimed at maximizing reliability.  The core structure of the two profiles remained un-\nchanged, but a critical addition was made to the Decision Rule.  This new rule introduced an\nexplicit guiding principle to resolve the possible ambiguity:  ”A slow but correct answer is al-\nways better than a fast but wrong one.”.\nThis principle served as a powerful tie-breaker, forcing the idea of accuracy on to th",
  "e model.\nIt explicitly stated that if there was any ambiguity, or if a query even touched on the character-",
  "istics from ”Profile 1” (like a specific name or date), it must default to the safety of retrieval.\nThis approach was designed to try and solidify the instruction’s focus on producing the most\ntrustworthy assessment possible, even at the cost of possible decrease in efficiency.",
  "Figure 39: Instruction V5 Results\nThe performance data shows that this refinement had a subtle but measurable impact, tuning\nthe model’s behavior as it was intended.",
  "The model became slightly more cautious.  The Retrieval Task Routing Accuracy slightly\nincreased from 91.88% to 92.0%, which means the model identified 1553 of the 1688 queries\nthat required retrieval.  This increased caution also resulted in a minor decrease in the General\nKnowledge Routing Accuracy, which shifted from 69.36% to 67.07%.\nThe model was now slightly more likely to send a simple query for retrieval if it contained\nany element of ambiguity.  This adjustment was made based on the reasoning that the model\nmight still produce a correct answer even when retrieval is not strictly necessary. However, the\ninverse failing to retrieve when it is required almost always results in incorrect answers.  As\na consequence of this shift, a slight dip in accuracy was observed, with the Overall Routing\nAccuracy decreasing to 81.1%.\nDespite the minor shifts in routing metrics, the final answer quality for general knowledge\nquestions remained identical to that of V4, with the routing system achieving a 95.27% . Simi-\nlarly to V4, this instruction resulted in a 20.35 percentage point impr",
  "ovement over the baseline.\nThis shows that V5 successfully reinforced the system’s reliability.",
  "6.8.6    Instruction V6: A Strategic Pivot to Efficiency",
  "The final iteration, V6, represented a deliberate reversal from the ”accuracy-first” principle\nthat guided the previous version.  The main goal was explicitly re-focused to ”AVOIDING un-\nnecessary document retrieval” and to ”reduce incorrect ’1-Yes’ classifications”. This instruction\nwas designed to test a high-efficiency approach, that prioritizes speed and resource conservation\nfor general knowledge questions.\nTo achieve this, the core logic was inverted.  Non-retrieval (”2-No”) was made to be the\ndefault path, and the model was instructed to choose this approach unless a ”clear and definite\n’retrieve trigger’” was present on the query.  The burden of proof was shifted:  instead of de-\nfaulting to the safer retrieval in cases of ambiguity, the model now required compelling explicit\nreason to engage the retrieval system.",
  "Figure 41: Instruction V6 Results\nThis strategic change had a profound and predictable impact on the system’s performance\neffectively trading accuracy for efficiency.",
  "The instruction’s goal was a success for the General Knowledge Routing Accuracy making\nit surge to its highest point across all the previous versions, reaching and impressive 90.47%\nas depicted in Figure 41. The system was now exceptionally skilled at identifying and directly\nanswering simple queries without using wasteful processing when not required.\nThis efficiency came at a significant and expected cost.  The Retrieval Task Routing Accu-\nracy fell sharply from",
  "92.00% to a measly 58.77%. By no longer erring on the side of caution,\nthe system failed to identify a large portion of queries that genuinely required the retrieval of\nexternal information.\nAs a result of this trade-off , the Overall Routing Accuracy dropped to 72.6%.\nInterestingly, despite the lower routing accuracy for complex queries, this approach achieved\nthe highest final answer accuracy on general knowledge questions with 97.18%.  This was a\n22.26 percentage points over the baseline answers. This outcome shows that by correctly routing\na very high volume of simple questions to the direct-answer paths, the system maximized the\nLLm’s ability to leverage its own knowledge effectively, this was also helped with the Chain-\nof-Thought instruction that improved the base model answer without requiring any external\ninformation.\nThis last experiment shows the high degree of control that prompt engineering provides on\nsuch a system and even on the LLM’s responses. V6 is not inherently better or worse than V5, it\nsimply optimized and constructed with a different objective in mind. The choice between these\ntwo mature instructions depends entirely on the desired system behavior. Which aligns with the\npurpose of this research prioritizing efficiency whenever the trade-off proves to be worthwhile.\nV5 is the ideal choice for a system where reliability and avoiding factual errors are paramount,\nwhile the V6 version is superior for a system where efficiency and speed in handling common\nqueries are the up most concern.\n6.9    Analysis of Energy Consumption and Efficiency\nA holistic view of the system’s performance is best captured by plotting th",
  "e total energy\nconsumed versus the number of correct answers that the system outputted.  The resulting scat-\nter plot reveals a fundamental trade-off inherent in the system’s operation:  achieving a higher\nnumber of correct answers of the provided dataset is directly correlated with increased in energy\nconsumption (see Figure42).",
  "Figure 42: Energy Costs vs. Correctness Scatterplot\nThis  is  clearly  lustrated  in  the  progression  from  the  baseline  models,  which  occupy  the\nlower-left quadrant of the graph which represents both the lowest energy consumption and the\nlowest correctness.  Although one part of the baseline achieved high correctness, this may be\npartly due to the way correctness was assigned to queries requiring retrieval. In this evaluation,\nif retrieval was triggered for a query that required it, the system marked the answer as correct\nregardless of whether the retrieved content actually led to a correct response.  This introduces\na significant limitation when interpreting the results.  If correctness had instead been evaluated\nbased on the accuracy of the retrieved content itself, the score would likely have been much\nlower and more in line with the other two baseline results. Another important factor is that, since\nthe retrievable documents come from Wikipedia, many of them coincidentally align with ARC-\nEasy queries. For example, in Query 3, the retrieved document happens to contain information\nthat, through reasoning, leads to the correct answer. This pattern appears in multiple ARC-style\nqueries and may inflate the apparent effectiveness of the baseline.  These two points boost the\nanswer correctness by a",
  "lot for this specific file thus should be looked at as a skewed result far\nfrom the truth.\nAt the upper-right , the system demonstrates its ability to improve both the model’s correct-\nness and the overall quality of the answers.  However, one outlier stands out: System Analysis\nV1.  This version reveals a particularly inefficient instruction, likely due to how open it was\nto interpretation.  As a result, the model engaged in excessive reflection while still failing to\nchoose the appropriate approach for each query type.  This led to a significantly lower number\nof correct answers compared to later, more refined instruction versions.",
  "An important takeaway is that the optimization process was not intended to reduce energy\nconsumption, but rather to maximize the productive use of that energy aiming to yield the most\naccurate results possible.\nFigure 43: Energy Costs vs. Correctness Scatterplot\nLooking further into the energy dynamics of this project, an analysis of the average energy\nper  query  reveals  a  striking  and  consistent  pattern,  incorrect  answers  are  consistently  more\nenergy-intensive than correct ones.  This suggests that incorrect answers are often the result of\ninefficient processing, such as retrieving irrelevant documents, pursuing flawed reasoning paths,\nor struggling to analyze conflicting information.  In contrast, correct answers appear to follow\na more direct and energetically efficient path. With one exception, the straight\nmodel baseline,\nwhere the correct answers consume slightly more energy.   This could be because,  when the\nmodel is more confident in its answer,  it tends to generate longer,  mor",
  "e detailed responses,\nwhich in turn require more energy than the simpler, shorter incorrect ones.",
  "Figure 44: Domain Average Energy per Correct Answer\nFigure 45: Domain Average Energy per Incorrect Answer\nFurther analysis of the results, now split by the query’s original dataset (domain), reveals\nanother clear efficiency trend: queries related to ’General Knowledge’ consistently require more\nenergy than those from the ’Science’ domain.  This pattern is still present for both correct and\nincorrect answers, as can be seen in both Figure 44, and 45.  For the more refined instructions\n(V2 through V6),  the energy cost for answering a general knowledge question is noticeably\nhigher than for a science question.  This disparity likely stems from the increased complexity\nof the general knowledge queries, which typically require retrieval to be answered.  This adds",
  "further  computational  overhead,  as  the  number  of  tokens  the  model  must  process  increases\ndue to the inclusion of retrieved documents alongside the instruction.  This occurs even after\norganizing the retrieved documents from most to least relevant, and removing any unnecessary\nexpressions that do not contribute to the quality of the response.   On the other hand,  in the\nScience domain,  the number of input tokens is always lower since no retrieval is performed\nwhen the system functions correctly, thereby reducing the total input tokens.\nIn summary, the energy consumption analysis provides a comprehensive, multi-dimensional\nview of the system’s efficiency.  It demonstrates that efficiency is not a single metric but a bal-\nance of multiple factors. The inherent com",
  "plexity of the query’s domain sets a baseline for en-\nergy consumption, with ”General Knowledge” questions proving to be more resource-intensive\ndue to the required retrieval process needed to answer them correctly. UGiven this baseline, the\neffectiveness of the instructional prompt plays a pivotal role in how efficiently the energy is uti-\nlized. Well-calibrated instructions help guide the model down more efficient pathways, leading\nto correct answers at a lower average energy cost. This demonstrates that query editing could be\na promising area of study for improving model efficiency. While also proving that ambiguity or\nflawed logic results in wasted energy on incorrect outputs.  Ultimately, this analyses reinforces\nthat the iterative refinement of prompts is not merely a quest for higher accuracy, but a method\nfor controlling the crucial trade-off between correctness and computational cost, while also al-\nlowing for the strategic selection of a system profile that best aligns with the desired balance of\nperformance and resource conservation.\n6.10    Detailed Energy Consumption Profiles\n6.10.1    Overall Energy Trends Across Instruction Versions\nA foundational analyses of the the system’s energy consumption begins with the average\nenergy consumed per query for each experimental version of the instructions,  as showed in\nFigure 46. This shows a clear high-level comparison of the computational cost associated with\neach iteration of the instructions against the baseline models.",
  "Figure 46: Average Energy Consumption per Query by File",
  ".  The introduction of the first routing instruction, V1, immediately results in a\nsignificant spike in energy consumption to nearly 2.0 Wh. This aligns with its characterization\nas a inefficient, open-ended prompt that likely caused extensive and unguided model processing.\nAs  the  instructions  were  refined  from  V2  to  V5,  the  average  energy  fluctuated  between\n1.45 and 1.9 Wh.   This demonstrates that the added complexity of the routing system,  even\nwhen optimized for accuracy,  carries a consistent energy overhead compared to the simpler\nbaseline approaches as expected since it requires the model to output two responses for just\none query.  Interestingly, Instruction V6, which was explicitly designed to enhance efficiency\nby trying to reduce the usage off unnecessary retrievals, results on the highest average energy\nconsumption at around 2.2 Wh.  misconception during the creation of the instruction, as it was\ninitially assumed that the retrieval process would be the most energy-intensive.  However, the\nresults show that although retrieval does contribute to energy consumption, it represents only a\nsmall portion of the overall cost.  Another mistake made during the creation of this instruction\nwas that it ended up heavily relying on an internal chain-of-though process to answer general\nknowledge queries, which resulted in higher energy consumption. However, it also led to faster\nresponse times contradicting some of the assumptions made during the instruction’s design.",
  "Figure 47: Total Energy Percentage Difference from Baseline\nThis increased energy overhead is clearly illustrated in Figure 47.  The graph shows that,\nc",
  "ompared to the baseline, the more advanced routing systems consistently increase total energy\nconsumption  by  over 40%.   This  underscores  a  critical  finding,  that  this  implementation  of\nan intelligent routing layer,  while substantially improving answer accuracy and reliability,  it\nalso presents a significantly and quantifiable trade-off in terms of computational and energy\ncost.  While this was already considered at the start of this endeavor, the main idea is to try\nand compete with much larger models that, on average, require significantly more energy and\ncomputational power. A deeper analysis of this will be carried out at a later stage.\n6.10.2    CPU vs. GPU: Deconstructing the Energy Cost\nTo try and understand the nature of the energy overhead introduced by the routing system,\nit is essential to deconstruct the total energy consumption into its primary hardware compo-\nnents, the Central Processing Unit (CPU) and the Graphics Processing Unit (GPU). The CPU\nnormally handles data processing, I/O operations, and logical orchestration, while the GPU is\nresponsible for parallel computations required by the model inference. The Figure 48 illustrates\nthis breakdown for each system version.",
  "Figure 48: Total Energy Consumption by File (CPU vs GPU)\nA clear pattern emerges from the data, the baseline ”straight\nmodel”, which relies almost ex-\nclusively on model inference, shows the lowest relative CPU energy consumption. On the other\nhand, all subsequent versions that incorporate either forced retrieval or CoT or the intelligent\nrouting system exhibit an increase in the proportion of energy consumed by the CPU.\nConversely,",
  "the  GPU  energy  consumption  scales  more  directly  with  the  complexity  and\nlength of the generation required from the LLM. The V6 instruction, which was designed to\nfavor internal Chain-of-Thought reasoning over retrieval, shows the highest energy consump-\ntion driven by a massive increase in GPU usage.  This shows that while avoiding CPU-heavy\nretrieval processes, version V6 shifts most of the burden to the GPU, requiring it to perform\nmore extensive and energy-demanding computations to generate the answers.  As previously\nobserved, this also resulted in poorer performance compared to earlier iterations.\nThis analysis reveals that the choice of instruction foes not just how much energy is con-\nsumed, but also where it is used.  A retrieval heavy strategy taxes the CPU, while a reasoning\nheavy strategy taxes the GPU. This distinction is critical for system optimization, as it shows\nhow different prompts and engineering strategies can create distinct hardware usage profiles.\n6.10.3    The Energetic Cost of Correcting Errors\nLooking beyond the general energy profiles, a more targeted analysis reveals the specific\nenergy consumption required for the system to add value that is, to correct an answer that the\nbaseline model would have gotten wrong. This scenario represents the core justification for this\nwhole approach.",
  "Figure 49:  General Knowledge:  Avg.  Energy when Straight Model is Incorrect & System is\nCorrect\nFigure 49 provides a quantitative analysis of the ’correction cost’ associated with ’General\nKnowledge’ queries.  When the baseline fails while consuming around 0.95 Wh, the various\nrouting systems successfully pro",
  "vided a correct answer, although this came with a significantly\nhigher energy cost, ranging from 1.5 Wh (V2) to a peak of over 2.1 Wh (V1 and V6). Showing\nthat overcoming the baseline’s knowledge gaps via retrieval is an intensive operation.  Though\nlike explained previously this lacks a better understanding since the evaluation of this domain\nis just that retrieval occurred, so the straight model never got a correct answer due to that fact.\nHowever, we can still compare them, and the V6 instruction stands out once again due to its high\nenergy cost.  This is primarily attributed to its reliance on a detailed chain-of-thought process\nwhich, while somewhat effective, is significantly more computationally demanding.",
  "Figure 50: Science: Avg. Energy when Straight Model is Incorrect & System is Correct\nA  similar  trend  is  observed  in  the  Science  domain,  as  shown  in  Figure  60.   Correcting\nthe baseline in this approach also required a substantial energy investment, with the V6 again\nshowing the highest consumption at nearly 2.5 Wh.  Thus reinforcing that the act of correcting\nthe responses regardless of the domain is inherently more energy demanding.\nFigure 51: Science: Avg. Energy when Straight Model is Incorrect & System is also Incorrect\nOn the other hand Figure 51 analyzes the scenario where both the baseline and the system\nfailed to produce a correct answer. This represents the least efficient use of energy, showing that",
  "the system uses additional resources only to arrive at the same incorrect outcome. It is notewor-\nthy that the energy consumed in these failure cases is often comparable to and sometimes even\nhigh",
  "er than the energy required to produce correct answers. As proof, the V6 system consumes\nover 2.6 Wh when it fails, more than it does for a successful answer which is around 2.5 Wh.\nThis might suggest that these incorrect answeres may result from the system pursuing particu-\nlarly complex, yet flawed, reasoning paths or even retrieving irrelevant information, leading to\nwasted resources.\n6.10.4    Energy Distribution and Consumption Predictability\nWhile the average energy consumption provides a useful high-level metric,  a deeper un-\nderstanding of the system efficiency is required to access its consistency and predictability.  A\nsystem with a low average cost is less desirable if it is prone to higher spikes.  The boxplot in\nFigure 52 provides valuable insight into all the versions by visually representing the distribution\nof energy consumption per query for each one.\nFigure 52: Distribution of Energy Consumption per Query by File\nThe baseline models,  particularly the ”straightmodel”,  presents the tightest distribution.\nThe small inter quartile range shows that most of the queries are processed using a very con-\nsistently and predictably amount of energy. This helps them in terms of reliability since from a\nresource planing perspective they are very predictable, though they show lower accuracy.\nOn the opposite side of the spectrum we have the initial routing Instruction V1, that demon-\nstrates extreme unpredictability.  It has by far the largest inter-quartile range and a long upper",
  "whisker, indicating a massive variance in energy consumption.  This proves that its ambiguity\nled to highly inefficient and erratic processing.\nT",
  "he following versions from V2 to V5 show a clear trend into an increasing predictabil-\nity. While they show a higher median energy cost than the baseline, their distributions become\ntighter and tighter as versions increase.  This shows the core benefit of the iterative refinement\nprocess,  reflecting precisely what was discussed previously as the instructions became more\nspecific and rule-based the model’s behavior became more constrained to the rules, therefore\nmore predictable.  The number of high energy outliers that caused an increase in system’s re-\nsource expenditure also decreased indicating a more robust and stable approach.\nLastly the V6 instruction, designed for efficiency presents a rather unique profile.  While\nits median energy is high, its distribution is relatively contained in comparison with V1, this\nsuggests  that  its  Chain-of-Thought  process,  though  energy  intensive,  is  being  applied  more\nconsistently.\nUltimately, this analysis underscores that effective prompt engineering does more than just\nimprove accuracy, as this proves that it enhances operational predictability.  A well designed\ninstruction set not only guides the model to the correct answer but also ensures that the model\ndoes so consistently while using a manageable level of resource consumption, which is a crit-\nical factor when picking and deploying such systems in the real world, especially in resource\nconstrained environments\n6.10.5    Overall Performance Quadrant:  Synthesizing Accuracy and Efficiency for ARC\nqueries\nThe culmination of this analysis is best visualized in the performance quadrant plot, which\nvisually presents each system’s fi",
  "nal correctness rate against its average energy cost for the ARC\nqueries.  The graph present in Figure 53, offers a clear overview of the trade-offs and situates\nthe performance of the 8B parameter model (DeepSeek-R1-Distill-Llama-8B) used as the base\nand all the routing systems routing systems against a crucial benchmark, a much larger 14B\nparameter model (DeepSeek-R1-Distill-Qwen-14B).",
  "Figure 53: Overall Performance Overview: Correctness vs. Energy Cost for ARC queries\nThe optimal position on this graph is at the top-left quadrant, which represents the highest\naccuracy with the lowest energy consumption.  The bottom-right represents the inverse so the\nworst possible outcome .  The baseline models establish two reference points.  The first is the\n”straightmodel” (8B), sits in the lower left quadrant, confirming it is a low accuracy but highly\nefficient  option.   On  the  other  hand  ,  at  the  upper-right  corner  sits  the  ”straight  model  14B\n8-bit” demonstrating the brute force approach as it achieves a very high correctness of nearly\n90%, however this comes with the cost of an enormous amount of energy at around 3.7 Wh per\nquery,making it by far the least efficient model\nThe iterative refinement of the routing instructions charts a clear journey toward the ideal\nquadrant.  The initial, poorly tuned instructions (V1,V2,V3) show modest accuracy gains over\nthe 8B baseline but at a notable energy cost, placing them in the lower-middle of the graph.\nThe major breakthrough occurs with instruction V4 and later V5. These versions represent\nan optimal sweet spot, achieving a substantial leap in terms of correctness to over 83%",
  "while\nstill maintaining an average cost below 1.8 Wh.  Most importantly, they deliver a significant\nportion of the accuracy gains of the 14B model while consuming less than half the energy.\nThe efficiency-focused V6 instruction pushes the accuracy to over 85%,  however it also\nincreased the energy cost, moving the results slightly to the right. While it is the most accurate\nof the routing instructions, it begins to approach a point of diminishing returns in the accuracy-",
  "efficiency trade off.\nTo conclude, this analysis of the quadrant provides the most complete validation of the re-\nsearch.  It shows that an intelligent routing layer with a combination of a carefully engineered\nprompts  are  not  just  incremental  improvements.   They  are  part  of  a  transformative  strategy\nof optimization.   By using carefully designed instructions to guide a smaller,  more efficient\n8B model, the system achieves a performance profile that rivals a modelnearlyt twice its size,\nbut at a fraction of the computational and energy costs.  This goes to show that architectural\nand instructional refinement can be a more sustainable and effective approach to high perfor-\nmance,rather than simply scaling up the model size.\n7    Experimental Consistency and Reproducibility\nTo guarantee that the performance and energy consumption results represented in this thesis\nare  both  valid  and  reliable,  a  strict  protocol  was  established  to  try  to  minimize  the  number\nof  variables  and  create  a  consistent  testing  environment  for  all  experiments.   The  following\nmeasures  were  systematically  implemented  to  ensure  that  the  results  sh",
  "own  can  be  directly\nattributed to the changes in the system’s instruction design and model performance.\nFirst, the test system was maintained in a controlled and isolated state. To prevent interfer-\nence from many background tasks associated with other programs, and also with the operating\nsystem tasks, such as automatic updates or network-related tasks, the machine’s internet con-\nnection was physically disabled for the duration of all test runs. To further improve repeatability,\nprior to initiating any experiment, all non-essential background applications and services were\nfully terminated.  The system was then left without any intervention after the scripts started up\nuntil they were finished and all the required data was collected. This ensured that the system’s\ncomputational resources were devoted exclusively to the experimental workload.\nAnother point taken to maintain the system repeatability was that a static software envi-\nronment  was  kept  during  the  entire  research  process.   The  main  components  of  the  system,\nincluding the Python interpreter, the PyCharm IDE, and the HWiNFO monitoring utility, were\nnot updated after the initial setup. This strategy is crucial to eliminate the risk of major software\nupdates introducing performance variations that could skew the results.\nFinally,  and  most  critically,  the  underlying  code  base  for  the  query-routing  system  and\nmodel inference remained identical across all comparative experiments.  When testing the ef-\nficacy of different instructions, the sole modification between each run was the content of the\nsystem instruction itself.  This strict isolation of the",
  "independent variable ensures that all mea-\nsured differences in answer accuracy, latency and energy consumption are direct consequences\nof the prompt engineering strategy, rather than unintended changes in the software that supports\nit.",
  "8    Challenges and Abandoned Approaches\nIn the course of this research, some promising strategies were explored but ultimately aban-\ndoned due to practical constraints, resource limitations, or conflicts with the project’s core ob-\njectives.  This section represents these methodological dead ends, as understanding what these\nwere and their causes could aid further research.\nOne of the initial strategies considered for enhancing the RAG component was the usage of\na hypothetical document similar to HyDE[40]. The technique, which involves generating a hy-\npothetical answer to a query to improve the semantic search for relevant documents, has shown\npromise in other projects. However, the original HyDE (Answer RQ2) paper recommended the\ngeneration of up to eight hypothetical documents per retrieval to achieve optimal performance.\nIn practice,  this approach proved to be prohibitively expensive for the presented framework.\nThe computational overhead of generating eight separate documents before the retrieval was\ndone and the formation of the final answer would have dramatically increased both energy con-\nsumption and latency,  with the latter representing a change that would make the framework\nbasically unusable with the current hardware. This would directly undermine the primary goal\nof creating a fast and efficient system for resource-constrained environments.\nAnother considered approach was the us"
]