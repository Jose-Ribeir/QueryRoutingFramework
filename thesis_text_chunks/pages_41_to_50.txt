ric MinMax Quantization is a method used to preserve the outliers in large language models
(LLMs) and maintain their performance.
Xi
Q =
Xi
R
α

,
α = max(|XR|)
2N−1 −1 ,
[14]
In the function above the XQ represents the quantized values, XR represents the full preci-
sion and the α is the scaling factor this is a general quantization formula and it can be applied
to both weights and activations, but the method to quantize differs depending on the target. In
the case of weights, per-channel quantization is used, meaning that at each channel (or filter)
in the weight tensor it will be quantized independently. This approach allows the quantization
process to adapt accordingly to the specific range of values in every channel, preserving more
information and reducing the possible quantization error. As for activations and the KV cache,
a per-token quantization is applied. In this case, the quantization is performed independently
for each token, allowing the method to account for the diverse ranges of activation values across
the multiple tokens. This distinction ensures that the quantization process is purposely selected
to the specific characteristics of weights, activations, and KV cache, optimizing the trade-off
between efficiency and accuracy for each case.
Then LLM-QAT uses the student-teacher model framework to ensure that the quantized
model retains the performance of the full-precision model. This works by having the teacher
model the full-precision version guide the student, which is the newly quantized model. The
guidance is provided through cross-entropy-based logits distillation.
n
X
LCE = −1
X
i=1
pT
c (Xi) log(pS
c (Xi))
[14]
(2)
n
c
On Equation 2 the i represents the i-th sample in the batch , c denotes the number of classes
(vocabulary size), and T and S are the Teacher and the Student models, respectively. The
next-token data generation is based on the full-precision model and is proposed as a method to
synthesize a distribution similar to the pre-training data. This data is generated by the teacher
model, which begins with a random start token and iteratively generates subsequent tokens
until it reaches the end of the sentence or the maximum sequence length. The LLM-QAT
introduced a hybrid approach to ensure the generated data is diverse and accurate, on the hybrid
approach only the first few tokens are deterministically selected with the top-1 strategy the
rest are stochastically sampled from the full precision SoftMax output distribution. Lastly, the
generated data is then used as input for fine-tuning the quantized model, where the teacher
model’s predictions serve as labels to guide training thus achieving a performance close to the
original model yet quantized to a specified level.
33
5.3
Retrieval-Augmented Generation (RAG)
Large language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [34] this is accomplished without access to any outside data [5] this comes with
some pros and cons. On the pros side, there’s the ability of the model to capture a lot of data
and compress it, on the other hand, this comes with the downside of not being able to expand
the model knowledge or even revise their memory. Another downside is the hallucinations that
sometimes are produced by this models. These limitations can be addressed using a method pro-
posed by [35], known as Retrieval-Augmented Generation (RAG). This method consists of four
main components: a query encoder (q), a retriever (pn), a document indexer, and a generator
(p0).


[INSERT IMAGE HERE: Figure 8: RAG implementation overview[35]]

The first required model is a retriver DPR based on [36]. This retriver works by indexing all
the passages in a low-dimensional and continuous space, so that later the top K passages can
be retrived efficiently. At run-time, passages that are relevant to the input question are retrieved
for the reader module. The number of passages from which the model can select is extremely
large the paper refers to a corpus of 21 million passages, with the value of K (i.e., the number
of retrieved passages) ranging between 20 and 100.
DPR represented by pn(z|x) follows a bi-encoder architecture:
pη(z | x) ∝exp(d(z)⊤q(X)),
d(z) = BERTd(z),
q(x) = BERTq(x)
[35]
Where d(z) is a dense representation of a document produced by a BERTBASE document
encoder [12] and q(x) a query representation produced by a query encoder, in this case also
using BERTBASE. The retrieval is done using (MIPS) Maximum Inner Product Search to
calculate the top-k(pη(· | x)), which represents the list of k documents z with highest prior
probability pn(z|x). MIPS identifies these documents by finding those with the largest inner
product between their dense representations and the query representation. This process can be
approximately solved in sub-linear time using an efficient approximation to the nearest neighbor
search, like FAISS or hierarchical navigable small-world graphs. This is crucial for enabling
the scalability of large document collections such as Wikipedia.
A pre-trained bi-encoder from DPR is used to initialize the retriever and to build the doc-
34
ument index. The Generator is used to combine the input x with the retrieved content z using
BART, these are simply concatenated.
The generator component given by p (yi | x, z, y1:i−1) could be modeled using any encoder
decoder, however BART-large [37] a pre-trained seq2seq transformer with 400M parameters, is
commonly used.
BART pre trainned using a denoising objective which served as its foundation. For this
specific use case, they also added a variety of different noising functions in the training to
prevent BART from overfitting and also encourage contextual understanding [38].
They refer to the BART Generator parameters 0 as the parametric memory and all of the
retrieved external knowledge as non-parametric knowledge. The marginalization can be done
via two methods described in the paper RAG-Sequence model [39] uses the same retrieved
document to generate the entire output sequence. Specifically, it treats the retrieved document
as a single latent variable, which is marginalized to obtain the sequence-to-sequence probability
p(y—x) using a top-K approximation.
pRAG-Sequence(y|x) ≈
X
z∈top-k(p(·|x))
pη(z|x)pθ(y|x, z)
i=1
pθ(yi|x, z, y1:i−1)
[35]
(3)
N
Y
=
X
z∈top-k(p(·|x))
pη(z|x)
This marginalization allows the model to combine information from the topk document,
effectively converging the information from diverse sources within the same document to gen-
erate a coherent and contextually accurate output. The final retrieval step ensures that the most
relevant documents are selected based on their dense representations,
The second method is RAG-Token model which can be used to draw a different latent doc-
ument for each target token and marginalize accordingly. This makes it possible for the gen-
erator to choose the content from several documents when producing the answer. The top-K
documents are retrieved using a retriever. The generator then produces a probability distribu-
tion for the next output token for each retrieved document. This process is repeated iteratively,
marginalizing over the documents at each step, to generate the subsequent output tokens:
N
Y
X
pRAG-Token(y|x) ≈
z∈top-k(p(·|x))
pη(z|x)pθ(yi|x, zi, y1:i−1)
[35]
i=1
In the case for sequence classification tasks RAG-Sequence and RAG-Token can be used by
considering the target class as a target sequence of length one.
35
Aspect
RAG-Token
RAG-Sequence
Document Usage
Different documents for each to-
ken.
Same document for the entire se-
quence.
Marginalization
Per-token over top-K documents.
Per-sequence over top-K docu-
ments.
Beam Search
Standard beam search.
Beam search for each document.
Efficiency
Computationally efficient.
Thorough Decoding is expensive
Flexibility
Combines information from multi-
ple documents dynamically.
Relies on a single document for the
entire sequence.
Table 9: Comparison of Decoding Methods
5.3.1
Hypothetical Document Embeddings (HyDE)
HyDE [40] is another retrieval method that aims to increase the performance of the model
on zero-shot scenarios, meaning that they can retrieve relevant documents without requiring
specific training. This model is meant to work with any type of NLP model and is able to
generalize across multiple tasks. This method differs from RAG [35] in that it uses a generator
to produce a hypothetical document based on the input query. This document does not need
to be factually correct, as it is only used by the retriever (e.g., Contriever), which transforms
it into a dense embedding vector. This embedding represents the hypothetical document in a
high-dimensional vector space.
The retriever is then used to search the corpus for real documents that are similar in the
embedding space, this similarity is measured using inner product similarity between the hy-
pothetical document embeddings and the real document embeddings. Then the most similar
document is fed into the model which also receives the input query, it then generates the output
for the query based on the retrieved document.


[INSERT IMAGE HERE: Figure 9: An illustration of the HyDE model.[40]]

The main issue addressed by [40] is the dependence on a separate query encoder required
36
by RAG [35] systems. This is because dense retrievers compute similarity between the query
and documents using inner product similarity, which necessitates a dedicated query encoder.
Firstly it uses two encoder encqencd that maps the query q and the document d into d di-
mension vectors vq, vd, whose inner product is used as the similarity measurement.
sim(q, d) = ⟨encq(q), encd(d)⟩= ⟨vq, vd⟩
[40]
This is where zero-shot dense retrieval problems lie, it requires learning two embedding func-
tions one for the query and the other for the document, these need to align into the same em-
bedding space where the inner product can capture the document’s relevance. HyDE solves this
problem by performing a search in the document-only embedding space that captures the doc-
ument’s similarity. This method can be easily learned using unsupervised contrastive learning
[41]. They set the encd directly as the contrastive encoder enccon as follows:
f = encd = enccon
[40]
(4)
This unsupervised contrastive encoder is be shared by all incoming document corpus. The
function 4 is also denoted as f.
vd = f(d)
∀d ∈D1 ∪D2 ∪· · · ∪DL
[40]
To build the query vector they use an instruction following LLM, in this case, text-davinci-
003 from OpenAi’s GPT-3 series [42], this was specifically picked due to its generalization
ability, they call it InstructLM so it’s easy to represent. It then takes the query q and a textual
instruction INST and follows them to perform the task specified in INST, like so:
To build the query vector, they use an instruction-following LLM in this case, text-davinci-
003 from OpenAI’s GPT-3 series[42]. This model was specifically chosen for its strong gener-
alization capabilities and is referred to as InstructLM for ease of representation. It takes the
query q and a textual instruction INST, and follows the instruction to perform the specified
task, as shown below:
g(q, INST) = InstructLM(q, INST)
[40]
The g can be used to map queries to the hypothetical documents by sampling from g, the INST
is set to be ”write a paragraph that answers the question”, the generated document isn’t real and
may be factually incorrect due to models hallucinations [42]. However, this is not important
because the hypothetical document is used only to capture the relevance pattern. Then the
relevance modeling is offloaded to an NLG that has the ability to generalize more easily, nat-
urally, and more effectively. Another great thing about generating the examples is that this
also replaces explicit modeling of relevance scores making it so there’s no need to compute the
37
query-document relevance.
E[vqij] = E[f(g(qij, INSTi))]
[40]
(5)
The f corresponds to the document encoder, g defines a probability distribution based on the
chain rule. In their implementation, they assume that the distribution of vqij is uni-modal,
implying that the query is not ambiguous. To estimate Equation 5, they sample N documents
from g,
h
ˆd1, ˆd2, . . . , ˆdN
i
.
ˆvqij = 1
X
ˆdk∼g(qij,INSTi)
f(dk)
N
N
X
= 1
k=1
f( ˆdk)
[40]
N
They also consider the query as a possible hypothesis,
" N
X
#
ˆvqij =
1
N + 1
k=1
f( ˆdk) + f(qij)
[40]
The inner product is computed between ˆvqij and the set of all document vectors {f(d)|d ∈Di},
then the most similar documents are retrieved. In their implementation, the encoder function
f acts as a lossy compressor, producing dense vectors in which unnecessary details are filtered
out. This enables the system to use a generated document even if it lacks factual accuracy to
effectively search for the correct one.
According to their study, HyDE remains competitive even when compared to fine-tuned
models. Another strong result comes from the web research setting, where the performance
of HyDE is particularly impressive even when compared to methods that rely on relevance
judgments. Notably, HyDE achieves these results without requiring such judgments.


[INSERT IMAGE HERE: No Caption]

Table 10: Results for web search on DL19/20. Best performing w/o relevance and overall
system(s) are marked bold. DPR, ANCE and ContrieverFT are in-domain supervised models
that are finetuned on MS MARCO training data. [40]
38
5.3.2
Cache Augmented Generation
RAG is the most used approach for enhancing language models but as discussed previously
it has two main drawbacks, specifically the retrieval latency and the potential errors in document
selection [43]. LLMs have been increasing their context size over the years and the CAG [43]
approach proposes a method that uses this increased context size to reduce the model latency
and potential errors that could occur on RAG systems.


[INSERT IMAGE HERE: Figure 10: Comparison RAG on the top and CAG on the botom[43]]

This approach works by enabling retrieval-free knowledge integration. This is done by
preloading external knowledge sources, such as a collection of documents D = {d1, d2, ..., dn}
and precomputing these documents key-value (KV) cache CKV , this addresses some of the
computational challenges and inefficiencies inherent to real-time retrieval on RAG systems.
This approach is divided into three main steps:
External Knowledge Preloading, represents the adaptation and preprocessing of the collec-
tion of documents D that are relevant to the target application, this adapts them to fit within the
model’s context window. This is done by the LLM M, with parameters 0, and processed the
documents D, thus transforming it into a precomputed KV cache:
CKV = KV −ENCODE(D)
[43]
This KV cache which contains the inference state of the LLM, is stored on disk or in memory
for future use. This implementation brings some of the computational cost down because the
cost of processing D is incurred only once even if used in multiple subsequent queries.
During this stage the precomputed KV cache CKV is loaded alongside the user’s query Q,
39
then the LLM uses this cached context to generate the responses:
R = M(Q|CKV )
[43]
By giving the model the external cached knowledge it eliminates the retrieval latency and re-
duces the risk of errors or omissions that comes with dynamic retrieval. The prompt P =
Concat(D, Q) ensures a unified understanding of the user query and the external knowledge.
Cache Reset is the part of the system is responsible for maintaining performance across
multiple inference sessions. Since the KV cache grows in an append-only manner sequentially
storing new tokens t1, t, 2, ..., tk the context may eventually need to be freed. This is achieved
by truncating the oldest tokens to prevent the cache from exceeding memory limits.
Creset
KV = Truncate(CKV , t1, t2, . . . , tk)
[43]
This ensures that the re-initialization is fast without the need to reload it from disk, ensuring a
constant speed and responsiveness.
5.3.3
Hybrid approaches
There are two main Hybrid approaches focusing on different implementations RAGCache
[44] and Self-Route [45] though these can’t be compared directly in terms of their implemen-
tation since RAGCache is a system-level optimization for RAG and Self-Route is an approach
that selects the optimal way to give context to the model.
Starting with Self-Route, this method combines the benefits of Retrieval-Augmented Gen-
eration (RAG) notably its proven effectiveness and efficiency in leveraging external knowledge
with the capabilities of recent long-context (LC) models, which can directly process and un-
derstand extended contexts. Models like Gemini 1.5 Pro [46] is able to achieve near-perfect
recall of up to 1M tokens and maintains this recall performance of up to 10M tokens. If this
trend of bigger and bigger context sizes continues this method could be a big improvement over
traditional RAG implementations.
Although one problem with using long-context (LC) models is the increased computational
cost, their performance can sometimes exceed that of RAG implementations. However, RAG
is significantly more efficient, with its cost estimated to be around 20% of that required by LC
models.
40


[INSERT IMAGE HERE: Figure 11: Comparison between performance and costs on multiple models using LC, RAG and Self-Route[45]]

As seen in Figure 11 the performance is maintained if not improved on some models but the
cost on most cases is less than half. This is due to the nature of the approach, which combines
the strengths of both methods. When RAG can be applied, it offers reduced computational costs
while maintaining most of the performance. However, despite the performance gap, there is a
high degree of overlap in the predictions made by both methods.


[INSERT IMAGE HERE: Figure 12: Distribution of the difference of prediction scores between RAG and LC[45]]

Figure 12 shows the differences between RAG prediction scores SRAG and LC prediction
scores SLC these are not just similar, in 63% of queries the model predictions are exactly identi-
cal, and for 70% of queries, the score difference is less than 10%. This is also true for incorrect
answers when looking at the red color which corresponds to an accuracy of 0 it can be seen that
RAG and LC make similar errors as well.
Self-Route uses the LLM itself to route queries based on self-reflection, under the idea
that LLMs are well-calibrated in predicting whether a query is answerable given the provided
context, this is done using a two-step approach RAG-and-Route and long-context prediction.
41
RAG-and-Route works by providing the original query along with the retrieved text chunks
to the LLM, prompting it to assess whether the query is answerable based on the given context.
If the model determines that the query can be answered, it proceeds to generate a response.
However, if it deems the query unanswerable, it is instructed to return the phrase ”unanswer-
able” as per the prompt: ”Write ’unanswerable’ if the query cannot be answered based on the
provided text.” In such cases, a fallback approach is then triggered.
This approach consists of giving the LLM the full LC which consists of all documents able
to fit the context , although this is not explained in the paper, it gives some hints that this must be
the case. This as seen on Figure11 giving a better result although with a higher cost. This trade-
off is most cost-efficient when k = 5, meaning the number of retrieved documents is five. This
is because, as k increases, the cost of RAG also increases, but so does the number of queries
that can be successfully routed. Ultimately, the efficiency depends on the specific task being
evaluated. For instance, in extractive question answering tasks where multi-hop reasoning is
not required a lower k (e.g., k = 1) may result in lower computational cost. Conversely, tasks
that require deeper reasoning may benefit from a higher k. Therefore, the optimal value of k is
dependent on both the nature of the task and the level of performance required.


[INSERT IMAGE HERE: Figure 13: Trade-off curves between (a) model performance and (b) token percentage as a function of k.[45]]

The other hybrid approach RAGCache works by caching the key-value tensors of retrieved
documents across multiple requests, this is done to try and minimize redundant computation for
efficiency gains.
42
